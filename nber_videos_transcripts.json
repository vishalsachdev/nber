[
  {
    "id": "SDU2oldSxqU",
    "title": "Economics of Transformative AI Workshop - Welcome",
    "url": "https://www.youtube.com/watch?v=SDU2oldSxqU",
    "presenters": [
      {
        "name": "Ajay K. Agrawal",
        "affiliation": "University of Toronto",
        "scholar_url": "https://scholar.google.com/citations?user=6K5DXJYAAAAJ"
      },
      {
        "name": "Anton Korinek",
        "affiliation": "University of Virginia and Brookings Institution",
        "scholar_url": "https://scholar.google.com/citations?user=RHXMxZoAAAAJ"
      },
      {
        "name": "Erik Brynjolfsson",
        "affiliation": "Stanford University and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=dJxqkuUAAAAJ"
      }
    ],
    "num_presenters": 3,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nAjay K. Agrawal, University of Toronto and NBER\nAnton Korinek, University of Virginia and NBER\nErik Brynjolfsson, Stanford University and NBER\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The NBER Economics of Transformative AI Workshop, held in Fall 2025, centers on the profound economic implications of rapid advancements in artificial intelligence (AI). Presenters Eric Brennolson, Anton, and AJ emphasize the necessity for economists to increase their focus on AI's potential transformative effects on the economy. They argue that while current metrics such as productivity and employment may not yet reflect significant changes, the ongoing investments in AI technology suggest a forthcoming transition that could fundamentally alter economic structures. The presenters highlight a forthcoming paper outlining eight critical areas of concern, including growth, inequality, and existential risk, indicating the need for a proactive economic analysis of these developments.\n\nThe workshop encourages participants to envision a future where \"transformational AI\" existsâ€”akin to the historical shift from agriculture to industrialization. This concept challenges attendees to think creatively about how such advanced AI could reshape various economic domains. Furthermore, the discussion acknowledges the importance of physical labor, pointing out that recent breakthroughs in robotics could also play a crucial role in economic transformation. The overarching conclusion is a call to action for economists and policymakers to prepare for significant societal changes driven by AI, ensuring that the economic understanding keeps pace with technological advancements.",
    "upload_date": "2025-09-29",
    "days_ago": null,
    "has_transcript": true,
    "word_count": 1064,
    "char_count": 5778,
    "transcript": "Welcome. If everyone can have their seats uh on behalf of uh Anton and AJ, I'm Eric Brenolson and welcome to the NBR. Welcome to Stanford. We're so glad you could all join us here today. Um you know, there have been just such incredibly rapid advances in AI in recent years. um whether that's uh winning the gold medal performance on the math olympiad, diagnosing diseases, writing legal briefs, uh creating code for econometric analyses or even simple economic models. The systems are uh matching or exceeding human performance on more and more capabilities. A cause and consequence of this is the millions of IQ points and and billions of dollars that's being invested in advancing the capabilities. uh AI may not be recursively self-improving it already but it is certainly attracting a lot of resources towards uh improving itself. We think this is a really big effing deal. Uh this you know when people look back historians look back on this era uh it's likely they'll think that AI is the most important thing that happened uh during our time. Um, and while there are billions of dollars being invested in data centers and advancing the capabilities, uh, there aren't billions of dollars being invested in the economic understanding of it. Um, there's outside of this room, there's relatively little effort in trying to understand what's going to happen to our society and to our economy. And in some ways, this is understandable. I mean, the daily life of most people hasn't really been changed that much by AI. There may be a few canaries in the coal mine of evidence that that there are some changes here and there but broadly speaking uh life is continuing productivity numbers employment numbers all the other sort of standard metrics are not moving a whole lot. Um that said um AJ Anton and I believe that economists radically need to increase our attention to AI because in the coming years we think there's a good chance that's going to change quite a bit. um if the technologists achieve what they're setting out to achieve or anything anything close to it, uh there's no way the economy is going to be the same as it is today. Um and in fact, we've written a a paper laying out some of the grand challenges that we think should be addressed. It's going to come out next Monday with the NBR. We don't want to taint you too much with it uh during the next couple of days. Um but uh we lay out eight broad areas that we think or nine broad areas we're going to think are going to be affected. uh growth, inequality, concentration, uh geoeconomics, information flows, existential risk, uh measures of well-being and transition dynamics. You know, the existing AI is amazing enough, but for the next two days, we want you to think a little bit differently. We're going to ask you to do something that's never been done in an NBR conference before. We want you to suspend disbelief about what you see around you right now and think about uh the kind of AI that may come into uh uh the world in the next few years. We're not exactly sure when, but we're calling it uh transformational AI. Uh you can define it a couple of different ways. Um, one way is a technology that's powerful enough to drive a transition in the economy comparable to uh what we saw when we had the transition from the agricultural economy to the industrial economy, the industrial revolution. Um to be a little bit more concrete uh one description in fact I see someone's got it up on the screen here is machines of loving graces uh Dario Amod's uh uh uh description of a country of geniuses on a data center. So what we've asked all of the paper presenters to do is start with the assumption that a technology like that exists and then think through what that means for the particular economic area that they're diving into. Um and so we're not going to spend a lot of time debating when and whether that may happen, but just we're going to assume that that exists. And we think there's no better way to think about a world with a country of geniuses on a data center than to assemble a room full of geniuses. So, welcome and thanks for coming here. Anton, thank you for the fantastic intro, Eric. I just want to start with uh two quick housekeeping issues. Um first of all uh all our conversations uh should go through the microphones. Uh that's in part because the presentations and the discussions are going to be recorded and we will release those on YouTube. Um so uh just something to be aware of and yeah we have distributed microphones uh throughout the tables here in the front and there are standing microphones uh for the Q&A part of the presentation. Um and secondly for speakers and discussants there is a timer countdown here right in front of us. Uh let's all try to stick to the time at NBR meetings. you know there's lots and lots of papers and we want to make sure we can fit them all in. Um lastly uh I think uh Eric has given uh a better introduction uh than uh what I could think of. Uh one kind of small point I want to add is uh for AI to be truly transformative uh we also need to think about the physical domain. I guess we as cognitive workers we kind of have a tendency to focus so heavily on cognitive work. Uh but in the economy a very very large part of uh surplus is generated either from the combination or for largely from largely physical work. And uh I think we should not lose track of the fact that there have also been really amazing breakthroughs in robotics just over the past uh 12 to 24 months and so to say while AI is advancing really rapidly the robots are coming after us too and I think that's an important part uh of the economic effects and economic transformation uh that we should be worried about and with that let me just hand it over to Ace."
  },
  {
    "id": "-K2Aek8qxh8",
    "title": "Algorithms as A Vehicle to Reflective Equilibrium",
    "url": "https://www.youtube.com/watch?v=-K2Aek8qxh8",
    "presenters": [
      {
        "name": "Jens Ludwig",
        "affiliation": "University of Chicago and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=8IWkmnEAAAAJ"
      },
      {
        "name": "Sendhil Mullainathan",
        "affiliation": "Massachusetts Institute of Technology and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=oExfyEkAAAAJ"
      },
      {
        "name": "Sophia L. Pink",
        "affiliation": "University of Pennsylvania",
        "scholar_url": "https://scholar.google.com/citations?user=AttCoCcAAAAJ"
      },
      {
        "name": "Ashesh Rambanchan",
        "affiliation": "Massachusetts Institute of Technology",
        "scholar_url": "https://scholar.google.com/citations?user=BfFr_DAAAAAJ"
      }
    ],
    "num_presenters": 4,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented b: \nJens Ludwig, University of Chicago and NBER\nSendhil Mullainathan, Massachusetts Institute of Technology and NBER\nSophia L. Pink, University of Pennsylvania\nAshesh Rambanchan, Massachusetts Institute of Technology\nAlgorithms as A Vehicle to Reflective Equilibrium: Behavioral Economics 2.0\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The presentation titled \"Algorithms as A Vehicle to Reflective Equilibrium\" explores the intersection of behavioral economics and transformative AI, particularly how AI can enhance our understanding and intervention in human decision-making. The presenters, including Jens Ludwig, Sendhil Mullainathan, Sophia L. Pink, and Ashesh Rambanchan, argue that while traditional behavioral economics has focused on nudging individuals towards better choicesâ€”such as through default settings in 401(k) plansâ€”these interventions often miss the broader implications on overall welfare. They highlight that nudges can lead to unintended consequences, such as increased credit card debt, and emphasize the need for a more nuanced approach to understanding human behavior.\n\nKey findings suggest that current behavioral interventions are limited by our capacity to accurately gauge individuals' preferences and the complexities of their decision-making environments. The presenters propose that algorithms, particularly those driven by AI, have the potential to significantly expand the design space for interventions, allowing for more personalized and effective decision-support systems. They introduce the concept of \"reflective equilibrium,\" where an ideal decision-making process is achieved by processing all available information, suggesting that algorithms can facilitate this by guiding individuals through their preferences and choices without requiring extensive cognitive effort.\n\nThe implications of this research are profound for both economists and policymakers. By leveraging AI to create more sophisticated behavioral interventions, we could improve decision-making in critical areas such as education and financial planning. However, the challenge remains to establish metrics for success in these algorithm-driven interventions, ensuring that they genuinely enhance welfare without merely shifting burdens elsewhere. This work encourages a rethinking of behavioral economics in the light of AI's capabilities, advocating for a future where technology plays a crucial role in helping individuals navigate complex choices more effectively.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 6075,
    "char_count": 32711,
    "transcript": "So this is going to be a very different kind of exercise that I was really enjoyable uh to go through. It's with it's with Yens Ludwig, Sophia Pink and Ashish. And the exercise was so much we've been talking about is the economy but in the end there are still people and there are still people with the vagaries of people and we still kind of will be interested in helping them. And if you think of behavioral economics, it has been the field that has been focused on what can we do to help uh understand the vagaries of what it means to be human and how we can you know do a little better and what I want to understand is how will that activity change uh with uh transformative AI which I now know what it is. So I'm good guys. Um thanks. Yeah, I just needed I just needed 30 minutes. Uh so what I want you to have in the back of your mind here so I'm not subweeting anybody is nudge as sort of the canonical example of the sort of the pinnacle of which behavioral economics has has reached and what I'm going to do is I'm going to try and think about three things pillars of behavioral economics one of them is going to be the main gist of what I talk about but the other two I think are going to be useful for completion so if we start with nudge I'd say this is probably the single most successful behavioral paper that I know of at some level from an application point of view. This is on defaults in 401ks and it's still striking every time I see it. These are people who when they joined a firm, this is what their retirement savings look like. And with one tiny change, which box you have to check, whether you have to check a box to sign up or whether you have to check a box to not sign up, you had this sort of immediate gain uh from about 86% uh from the beginning. people sign up for 401ks. This is striking in so many ways because people had tried to induce signups with subsidies and matches and these effects were huge and I think it launched this entire work. There is sort of an unfortunate dirty secret underneath all of this which kind of came to the surface um in the nudge literature which has kind of come to the surface uh on this work. Uh here's a paper by Basher at all in 2025 that just said great people signed up for 401ks. What happened to the rest of their portfolio? And it looked like they basically took on credit card debt. So while while we were happy with signing them up for retirement savings, I don't I don't think that we would have been happy at signing them up for retirement savings that is financed out of credit card debt. Are they magnific? >> Uh, no. I think this is it's quite large, but I think they're big enough standard errors that, you know, I wouldn't sit here and say the entire problem is this, but I also wouldn't sit here and say you should be happy with this is your credit card debt. And the reason I'm raising this is not to say defaulting in 401ks is a bad idea. It's to say there's a problem with this part of the literature, which is we have been developing things that help us nudge people But we don't quite know whether when we do so we are improving welfare. I'll tell you another example. Uh this is um credit card disclosure and uh if you if you pay this much you know this was part of the card act. I don't know if you guys remember this. So this was thought oh this is a really useful regulatory intervention to improve decisions. And I think some studies show that it was. But flash forward just a little bit and once you have things on websites and bill pay comes on websites and no one opens things anymore, what does the card act mean? Well, it means when you go to choose an amount, it's kind of buried here in this sort of nobody ever gets to the disclosure part. Which raises a second question. Well, when markets have problems, we can regulate, but markets will respond. The third issue I'll raise is this is a book called judgment decision-making. It's the handbook of judgment decision-making. Uh since I don't have much else to say, I'm just going to go through chapter by chapter. Uh you can read with here heruristics and biases. Okay, here now we're up to chapter eight inside outside twisted pair counterfactual thinking and hindsight bias. This is a lot of huristics and biases. Uh here let's keep going. Uh this is 26 behavioral. The reason I'm going through all this is there is one chapter in this 624 page book on a topic that you might have thought there'd be a lot on debiasing. Isn't it curious that in your entire experience with behavioral economics, the entire literature is focused on taking biases as given and then hey people are going to be people. What are you going to do about that? And that's because this whole literature has sort of tried to say let's let's change people but has sort of concluded that debiasing interventions just don't work that people are largely unfixable. we should focus on the decision which is such a backstory that I think if you reflect on it it's quite striking it's like oh people can't be educated people can't be changed but it was arrived at an empirical thing that Laric paper was one of the influential papers that said we've tried a lot of things it didn't really hasn't been working so decisions markets people decisions went through and people we can't debias I'm putting these together because I think all three of these sort of background problems in the field right now are actually prone to be solved by algorithms and that AI is perfectly suited for it. And the reason I'm saying that is because I have to write a chapter on No, I'm saying that because they all come from the same core problem. The same core problem that's giving rise to all three of these is because our capacity to intervene and do anything meaningful with people is very weak. We can change a default. We can send a message. We can change a card act disclosure. But that's about it. But if you think of as in the Asian example, as you've seen all day, what has been the move forward? It has been the move to have algorithms that can much more richly and meaningfully interact with people which opens up the design space tremendously and I will try to argue that opening up of the design space is going to fundamentally transform all three of these and that we're going to be looking at a very different behavioral economics. So let's go back to nudging we can prove welfare. Let me take something at the opposite extreme from 401ks just to illustrate how strong and how big the change could be. School choice. So there's a lot of literature that says if you have, you know, your kid is going to join a high school or middle school and you have to pick which school, you have to enter one of these matching mechanisms, you have to rank it. People find this impossible. How would we nudge? Well, this problem makes clear the issue that was tacit in 401k. Why? What am I supposed to nudge people towards? The best school. What does that mean? Let's let's get everybody signed up for the best school. What's the best school? How do we know? More savings is obviously a good thing or we've made the call that it is, but what's a better school? What's a better school choice? So, what we need is a framework for decision-m that kind of elucidates the problem. So, I'll walk you through this fairly quickly. We have a decision D that's a binary choice. We have utility that people have as a function of the their own characteristics and the characteristics of the choice. There's an optimal choice for them given their utility function. But there's an actual choice. And so behavioral I'm going to summarize behavioral economics in one term. D ofx minus d star of x. That's behavioral economics. Okay. And I'm going to summarize a ton of literature just through the lens of that one term. So what is reveal preference? I think we all know what that is. Epsilon is zero. That's what it is. That's what we say. Okay, it's great. What is nudging? Actually, nudging is also a very simple assumption on epsilon. It's saying the sign of epsilon is known and fixed across all people. Up is up. More savings is good. More vaccines is good. Great. Let's go further. What's a recommener system? Because we build recommener systems. A recommener system is the assumption that the expectation of epsilon is zero. So that if I can predict your choice, which is what a recommener does, predicted choice is actually the right choice. That's a recommener system. So in some sense, a bunch of stuff falls into these. But interestingly, none of these suit the school choice thing. I wouldn't want you to take my choices as given. I don't know what the right sign is, and certainly don't give me the choice you think I'm going to make. Which is what is that doing? I think this is what comes closest. If you say, well, you may not know, your preferences are homogeneous and based on observables or some people who do know, maybe we can make it work. But it kind of is it'd be a stretch. So what do we need? We need a way to acknowledge some facts that I think are true. People's utility differs and is not fully observable. This is the part where we may have some disagreement because in the last thing we talked a little bit about could you communicate your preferences fully etc. I'm going to be operating the assumption that you cannot fully communicate your preferences in this case. Great. Um but we need to acknowledge that at the same time reveal preference doesn't hold. There's actual error here. So we need a way to reduce error while appreciating its inscrutability at some level. The inscrutability utility. Here's an proto example of an intervention. This is by Hastings and Weinstein. I'm going to call it a proto example. What they did was they just took a website and they just summarized the school choice options into a much simpler example. And they have a proxy of whether improved choice by saying the set of people who just stuck with the default uh uh went down. So people people made more active choices. They felt more comfortable choosing. But it's a proto thing because you've got to now imagine surely with all the chat bots we have, with all of the capabilities we have to discuss, we ought to be able to design an intervention that can talk to the person. I said talk to me. I don't know why. Maybe I'm lonely. Um I'm learning a lot about myself today. Must must help me think through the decision and then come to a choice. So ideally, it's not hard to imagine that in the future, in the not too far future, we could have something that has the ability to help me reason through my confusion and map my preferences, which remain private information, to this choice set. The challenge here is what would count as success? How would we know if this algorithm has done a good job? What would count as success is I think as we look forward the key challenge of sort of let's call it algorithmic behavioral economics with nudging. we know which way is up or rather Bernheim and others would say we assert which way is up but we lack such a metric here that we don't know and we need a tangible framework so what I'm we're proposing is there's a notion from RS called reflective equilibrium which I think is very helpful here and the idea is imagine if a person could or were to process all the information available to them available at the time not to the future not imagining hindsight bias what I learned but at the time what decision would they get and the key is you want a decision that results from that process or get close to it you don't need them to go through that process in fact the ideal thing is to get to that decision without me having to do anything that's the notion of reflective equilibrium the key challenge of reflective equilibrium is figuring out how to make it tangible and how to make it measurable but I think there are ways for example we make one proposal but I think the field will start to find other ways. We think of it as stability of behavior. If you're at reflective equilibrium, one property of reflective equilibrium was where I to present you with some information that was in that set, it really ought not to change your behavior. You said, \"Yes, I already know that.\" If you take this definition, you'll notice how I think the default in 401ks could actually fail the reflective equilibrium concept because it could be if you took the results of Basher seriously, it could say, \"Oh, wait. Would there be that much less money in my paycheck? Oh, well that's bad.\" And you notice that failure would be the reflection of the fact that they are inadvertently borrowing from debt in other places. So I think once we realize that there's a whole amount of need for reflective equilibrium, we'll also realize there's a pragmatic need for it, which is decisions are not isolated. If people need to make other decisions that are complimentary with these, there's value to get to reflective equilibrium. Great. So for this bit, let me conclude by saying I think algorithms can play a big role here in promoting reflective equilibria. I'll I've written down three examples here in the paper. We try to talk about a few more but one is there are many inputs that go into a decision that uh we can use data to help predict and transmit to people. For example, how well will my child do in a harder math class or how much more income growth can I expect or there's a lot of data we have that could inform every kind of decision and that is something that can be promoted to people. A second thing algorithms can do is they can present many points of view. So when you're thinking about stuff, it'd be helpful for someone to say, \"Hey, lots of people end up regretting saving under saving. Here are some testimonials from people who are older who there's a lot algorithms can do which is play back information from the world.\" The third and possibly most important thing they can do uh as Abishek had kind of pointed out to us when he read a draft is that algorithms can screen through a large number of options and that is a very valuable activity. that is in an ideal recommener system. It would get you to the option that is the best for you without you having to do any of that work. Great. So that I hope gives a sense of what I meant by our interventions are not smart. Right now the behavioral paradigm is stuck in a very small small world. Let me take that idea and blow it up a little bit. Let me go back to uh markets and will markets respond. So this is a canonical example of obfiscation. Here are two credit card uh payments. Annual fee 0% intro APR intro APR. Having to look through this is exactly the kind of offiscation that markets are good at creating equilibrium around. They're fantastic. I mean, they're better at getting me confused than I am at getting my students confused. So, I'm a little jealous. >> Now, if we just took not future AI, but just took tech right now. Actually, I was surprised. We did this with Chacht. I was quite surprised at how good it is at just clarifying for you and going through the offiscation. This is just a summary of the two credit card offers and it was actually quite accurate and it does the analysis. It says best for but it gives you where the warnings are after the first you still owe and it gives you a sense of you might have missed the pay later part. So there's quite a bit of processing that the summarization literature can do. Here's another one. This is an amazing paper um from some former student of mine on bail. If you get arrested um you might want to pay attention. If you get arrested, you'll get this bail summon notice. And as you can see, this is impossible to understand. So what this paper did was it worked with the city of New York to rewrite this bail summons notice in a way that was a bit more easy to understand. And they showed these pretty striking effects. seven percentage points where the consequence is being arrested because that's what a failure to appear is. It's now it's now you've committed a crime. Before you hadn't committed a crime, you're just showing up. But failure to appear is a crime and now you're screwed is quite a lot thing for just like changing the printing on a form. But if we took the same form and just literally gave it to uh one of the language models and say can you just rewrite this for me? It actually writes it as not the as simple or even simpler than their rewrite automatically. I'll just add a footnote to this. The other part of their intervention was nothing more than send them a reminder on that date and that had an even bigger effect. But you can see how trivial it would be for an algorithm to summarize and then send you a reminder. Why is this so important? Because when we do simplification, when we do card boxes, we are in a static change. But if algorithms can do the simplification we try to do by hand, the equilibrium is now between market forces and algorithms, which is a very different market equilibrium. It's even better than equilibrium between market forces and regulators at some level. In fact, that's why this is called spam detectors. We're now closer to the spam problem than we are to anything else. But again, this pushes the question back, which is we must now understand the market demand for algorithms and agents and what will people want. All right, the final thing I'll do is uh the debiasing thing. So here I'll try to be fairly quickly. My impression had been for a long time this stuff is utterly not doable. People had done it. Laric other smart people have done it. You can get some effects in minor areas, but like it's hard to do at scale. Then there was this work on becoming a man. Becoming a man is this program with 16 year old kids, uh, 14, 15 year old kids in the south side of Chicago, 12-week program. And it was kind of just got together and like talked to them about stuff. These are this like a this is amongst the highest risk youth in in Chicago. And what you saw was quite large impacts on arrests uh two years out. And when you did it in Cook County detention again with another population, you saw quite big drops in rearrest rates, like extremely large. And these are like 14 months out. So this was a program that was debiasing at scale at length for quite a weird bias. So why does BAM work? To make a sense of why BAM works, I just want to go back and talk about why does any education work. I would argue it works because we do three things. We explain a concept, we illustrate a concept, and you practice it. And I would say this is very precise data that the ratio looks something like this. That in order to learn anything, a huge part of learning is just using and practicing and repeating. Now, we solved this for calculus and algebra because you can do little practice problems. How do you solve this for a debiasing intervention? Well, BAM solved it for a debiasing intervention where what they did in the classroom sessions was not teach you about biases. It was just talk through what you did, what happened to you that week and ask you did any of the biases apply that you've learned. It's probably too high. It's like much lower than that, but that is what they did. But it's hard to do this at any sort of reasonable scale. And so what I would argue is algorithms are going to change this logic. This is a really interesting product. You guys should go look at this. It's called teach FX. It just records teachers. The teacher records themselves and then afterwards it analyzes all of the stuff and starts giving feedback like oh you know this is how many minutes of silence you had. You talked over students at this rate. Oh, you said you had a goal of wanting this but you actually did this. That's just for teachers. That's like BAM on steroids. algorithms I think will let us do that last piece of practicing at scale. It's zero gothos debriefing practice on demand and I think will break through this we can't seem to be biased. So let me conclude here and say what do we need from uh transformative AI? We kind of need new capabilities as thought partners like the school choice thing. We need question asking theory of mind. We need its ability to kind of implement stuff. We need to be able to expand existing summarization for adversarial context which right now that's not really what they're optimized for. It's just summarization. And AI models need to identify concepts relative for me in in situ. I'll conclude since I did Burton Russell before I'll do the other famous philosopher Richard Daylor. Baylor uh has often said that behavioral economics should become obsolete because it should be all of economics. I kind of want to hope I've convinced you that I think algorithmic behavioral economics should become obsolete because it should become all of behavioral economics. I just don't see a world where there is a way to think about behavioral economics without algorithms. All right. Thank you. >> That's fine. Um thank you so much. Uh thank you to the organizers for for inviting me to be a part of this uh fun conference and and thank you to Sendel and co-authors for writing this really interesting paper. Uh learned a lot both about the history of behavioral economics but also the unique and different ways in which algorithms could be used here. I think quite different than what we've talked about this morning. Um, seeing all those examples about different kinds of decisions where people make mistakes, uh, took me back about 30 years to when my dad was buying us a car and I was really rooting for the one that we couldn't afford. That was really nice, but the one that we really should have been buying was this trusty old uh, as it was called Maruti 800 back in the day. And my sense is the focus of these of this article is that a lot of people look like me back there, which is that we're buying sports cars on sunny days when instead we should be thinking about what actually our commute is like, etc. Right? And so, um, have that in the back of your mind, right? If if pension savings is not your jam. Um and and what Sandal and co-authors are really saying is I think one way to really think about it is behavioral economics 1.0 has been a kind of a one-sizefits-all solution in so far as we can't go down to understanding the different preferences of people and why they might make these differences uh make make different judgments. Instead, we're going to be top down and think about systematic biases that that people make uh and try to fix it that way, right? So, think about nudges, think about changing designs of different forms, so on and so forth. The the list is quite endless. But I mean, if when I mean we had Taylor up there, people were skeptical of behavior economics when it first came up. And in some ways the success of behavioral economics has forgotten why people were skeptical about it in the first place which is that people believed in real preference. Right? So one way in which I think about this this paper is trying to bring the economics back in behavioral economics uh in the sense that how can we pay attention or how can we value uh uh heterogeneian preferences while still understanding that people are imperfect. Right? So and the solution to that has to be at at some deep level customize algorithmic interventions. The other really interesting idea in this paper uh is this idea of reflective equilibrium which is really quite new for me. Um and I think that that's both kind of an oper kind of a contribution of the paper but as I'll talk about uh something that the field will really need to clarify and define but one way in which I think about it is um basically asking the decision maker about how comfortable they are with their decision when presented with alternate information. exactly what uh what what was mentioned in the presentation but precisely what that alternate information is or precisely when that question is asked or precisely what the other architectures under which the decision would be made is still kind of left unspecified but for now imagine this Buddhist monk up there who's very comfortable with all of the life choices that they've made and that's ideally where we would like all decision makers to get to right so as a goal I think it's it's it's a really helpful goal for us to have as someone who studies people's decision-m. Okay. Um we didn't get to talk about this but one of the things behind in that in in the model that was presented was different ways in which uh these errors could creep up. Um and and and and the argue which I completely agree with is two ways in which these decisions errors could manifest is actually really helpful in carving out a role for algorithms. The first you can think about as input errors and the second set of errors you can think about as processing errors. So input errors are basically I simply did not pay attention to the fact that after 18 months this the the the interest rate on this credit card is going to go up. Uh I simply didn't know that there's this third credit card out there that would dominate the other two that I'm choosing. So on and so forth, right? And so when I was choosing the car, uh, I used this particular magazine which is filled with pages and pages of data on everything from the cost of the car to its mileage to its horsepower and so on and so forth, right? And so I might make a wrong decision simply because I didn't internalize all of that information correctly. And as many of us have learned, um, algorithms are extremely good at doing that. On the other hand, the the magazine also came up with this really helpful framework so that I can explicitly specify how important it was uh that the car would would look great to my friends as opposed to the fact that we could afford it. And here is where the heterogeneity of preferences would really come in. Right? For some people that number would be high and for others that would be low. And so these kinds of tools uh you've bought rent or buy tools all kinds of tools that we use that help us be more systematic about all of this data and and and make to make sure that we're actually writing down what our preferences are before we come at a decision. Now this is archaic but surely algorithms could do much much better uh at in this process of helping us both understand ourselves and then take all of that data to arrive at decisions. And that's in some ways I think one of the most helpful things in the paper um is this mapping from these different kinds of biases to precisely how algorithms could help us and and I think we we talked about that. So everything from summarizing and and predicting how different for example a very complicated schedule of interest rates would translate into into a difference in payments. To me the really novel and interesting part is this second set of categories which I think about as like before decision, during decision and after decision. So you can think about how algorithms could basically be a helpful guide as you make those decisions which is a very different model than being shaped by a a nudge or a default even before I get to that decision. Right? That's really the vision of behavioral economics 2.0 to summarize. Uh I I love many many aspects of this so I won't uh belabor it but one way to think about it is really reenters the people and their agency while still acknowledging their biases right and so uh there's a nice line in the paper which is sometimes we tend to u ignore really big problems because we can't solve them and this is definitely one of the big big problems of the whole field that this paper is tackling head on which I really like. Um I'm not going to talk about uh the first of these areas for improvement because um as was mentioned I think the paper already deals with consideration set errors in the version that I read. Um this idea that you can expand uh the the field or expand the set of options wasn't central uh but seems like that's been that's been revised but I want to talk about points two and three for a second. So we're talking here about transformative AI. Um and I think we all have different visions of of what that is. But in my view uh whatever mechanical transformative AI system we might have might transform but might still face some of the challenges uh that current AI systems do in terms of everything from biases, the fact that they can only see the data that they have and so on and so forth. It would be nicer for the field going forward to think about these AI systems as proxies. So human simulators or similar human experts of different types rather than all knowing oracles, right? So you can imagine when you talk uh to a guide or an expert or adviser, uh they're much smarter than you, but they're also quote unquote biased in their own way. And that's really helpful, right? But that doesn't mean that that's the only perspective that you want. So how can we incorporate the hetrogenity of different preferences of the AI systems as we think about developing this behavioral economics 2.0. So rather than treat that as a monolith, we can actually think about different kinds of interventions um in so far as the systems themselves will have a lot of heterogeneity. Uh third point I'll make this came up I don't going back to kind of what's first order versus not. This is this is a hypothesis I have um and it's something that again should be tested going forward and we talked about this I think in Bets's uh Betsy's talk in the morning which is part of the meaning that people get from decisions is the fact that they felt like they were at the wheel and with nudges my sense is that isn't really affected we often have no idea that my savings rate is much higher because of a design decision someone else took while at the extreme we can imagine delegating all of our decisions through agents and making perfect decisions but then not being proud of the choices that we've made. Right? So in so far as we think that this this channel uh for owning your decisions is quite important. I think work on human AI collaboration to think about how exactly you how exactly you create that balance. More broadly, I think one of the one of the more philosophical questions is to what extent do we want to completely do away with uh this top-down uh interventionist behavioral economics 1.0 approach to a fully flexible one that prioritizes reflective equilibria versus the cases where some of this might actually be really still relevant. My hunch is that there will be a need for a balance. So we already talked about cases like um uh conditions where there's externalities maybe there are situations where we do actually know the kinds of outcomes that we want uh versus others where this kind of approach would work really well right so in so far as algorithmic behavioral economics still has behavior economics a key question is to figure out what parts we want to retain versus what parts we want to want to improve on the the last thing is the way I would answer a lot of these questions maybe we don't like horse races anymore but since that's the only model of science I have is actually to compare this different scenarios of decision making um and John was here in the room and I have done some work on using AI as a simulator so I think we actually want experiments with people uh but this is from a paper of ours where we showed that AI simulators can do quite well in simulating lab experiments um and so we in in in my group set up a very simple experiment going back to the car scenario right and So uh we created different agents uh and we told them you're Harry, you really like uh you you like status and prestige um and here are your two car choices. What are you going to choose? In one condition, we simply gave the agents a notch, right? So remember when when choosing a car, practicality is as important as status. While in the other we had Sandal and co-author's vision of a helpful decision support guide who would constantly monitor what you're doing and then come in to intervene. So in that case they would get a nut saying hey I see that you're selecting the the luxury car. Have you really thought about your decision? So this is one of the specific recommendations that you make in the paper. And at least in our simulations what we found is that that tends to perform much better than in the nudges, right? And so I think lots more empirical work to be done in figuring out where that balance is so that in the end my dad did get the practical car but uh but if we had these tools I think that that debate would have been much shorter. Thank you so much. [Applause]"
  },
  {
    "id": "64F1sfm5kGI",
    "title": "AI Exposure and the Adaptive Capacity of American Workers",
    "url": "https://www.youtube.com/watch?v=64F1sfm5kGI",
    "presenters": [
      {
        "name": "Sam J. Manning",
        "affiliation": "Centre for the Governance of AI",
        "scholar_url": "https://scholar.google.com/citations?user=-gjxfJgAAAAJ"
      },
      {
        "name": "Tomas Aguirre",
        "affiliation": "University of Sao Paulo",
        "scholar_url": "https://scholar.google.com/citations?user=YpyfZgQAAAAJ"
      }
    ],
    "num_presenters": 2,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nSam J. Manning, Centre for the Governance of AI\nTomas Aguirre, University of Sao Paulo\nAI Exposure and the Adaptive Capacity of American Workers\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The research presented by Tomas Aguirre and Sam J. Manning at the NBER Economics of Transformative AI Workshop addresses the critical question of how American workers will adapt to potential job displacement caused by artificial intelligence (AI). The study aims to identify which groups of workers are most vulnerable to displacement and who possesses the adaptive capacity to navigate such transitions. To achieve this, the authors develop an index of adaptive capacity that incorporates factors such as age, liquid financial resources, skill transferability, and geographic employment density, thereby providing a comprehensive understanding of worker resilience in the face of AI-induced changes.\n\nThe key findings reveal a positive correlation between AI exposure and adaptive capacity, indicating that those workers who are more exposed to AI technologies also tend to have greater resources and capabilities to adapt if displacement occurs. For instance, younger, financially secure software developers in urban job markets demonstrate significantly higher adaptive capacity compared to older, less financially stable clerical workers in less densely populated areas. These insights suggest that while AI may pose risks of job loss, certain segments of the workforce are better positioned to cope with these changes, highlighting the importance of targeted policies to support vulnerable workers.\n\nThe implications of this research are significant for both policymakers and economists. By identifying the factors that contribute to adaptive capacity, the authors provide a framework for developing policies aimed at mitigating the negative impacts of AI on employment. This includes promoting financial literacy, enhancing skill transferability through education and training programs, and supporting job creation in high-density areas. Ultimately, the study underscores the need for a proactive approach to workforce development that considers the diverse capacities of workers to adapt to the evolving labor market shaped by AI advancements.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 4327,
    "char_count": 25140,
    "transcript": "All right, our first speaker is Thomas Ego, AI exposure and the adaptive capacity for American workers. Um, hello, my name is Tomas Ahi. It's immense privilege to be opening the second day of presentations. Today I'm going to present uh my work uh that I've written uh with Senning called AI exposure and the adaptive capacity of American workers. Sam really wished to be here today but he's having his fourth baby any day or hour now. So I hope to represent him well. So the context of this paper is that uh there is a lot of discussion about AI induced job placement. Uh leading figures u in the Silicon Valley and Washington are deeply concerned about this. uh Dar Mod I think I'm the fourth presentation to site Dar Mod but Darday said that AI could wipe out 50% of all entry uh level white collar rows within the next five years. Barack Obama has shared these concerns and uh the Trump's administration AI action plan uh mentions uh ideas to better tracking uh AI adoption in the economy with a special concern for job displacement. When you ask the public as well, they are deeply concerned about AI increasing unemployment and uh roughly one-third of American workers are personally concerned about being placed by AI while AI control and alignment also shows up in those surveys. Usually unemployment is the major concern that are on people's uh head. So the research question if of this paper is if AI causes job displacement who will struggle the most and who you will be able to adapt we don't take instances on or opinions on how uh fast or what will be the magnitude of job displacement when we ask ourselves if the displacement uh indeed happens who will face the most significant displacement costs. For this we construct an index of adaptive capacity to job placement. Uh the goal here is to identify uh identify which workers are best positioning to adapt if they are spaced to AI and uh build a policy to to also try to see whether there are vulnerable pockets. To achieve those goals, we bridge two main literatures. The first one is the A exposure literature. Uh various papers building on task based frameworks introduced by David Alur in 2003 with colleagues uh have tried to measure how different tasks are exposed to LLMs and map this them to occupations. One of the first papers to do this specifically to AI is uh from Brisons and colleagues that creates a measure of suitability for machine learning in which they match onet tasks to various uh capabilates of machine learning technology of the time. Uh Elon and colleagues including Senning uh made uh updates for uh of this kind of research for uh GP4 level LLMs. Those papers usually show that high-inccome workers have a higher AI exposure and they roughly indicate the likelihood of job transformation. It's important to highlight that not all transformation will take the the shape of displacement as we can see in frameworks such as simulo 2019. Uh new technology can deepen automation, can create entirely new tasks, uh can um uh augment workers and sometimes it can also displace workers. Because of this we discuss the uh displacement cost literature. Here one of the seminal papers is Jacobson la 993 in which they show a persistent scaring of uh of people when they get laid off from their jobs. The the effects size vary quite a bit but they can uh have uh a very big magnitude of like 10 to 20 uh percent decrease in wage prospectus decades after displacement. There are various ways of explaining this. The most common way is loss of human capital specificity either for a specific industry or a specific firm or a specific occupation that the work uh the worker was at. There are highly effects uh that is highlighted for instance in recent work by Susan Rete and colleagues distinguing the Swedish labor market and various labor labor market failures and provide those costs such as coordination failures in which no firms or workers internalize all the benefits of reskilling and uh liquidated constraints as for instance bar and stores 2024 highlight. We bring this literature together by building a measure of adaptive capacity displacement and then comparing this to air exposure. The core idea here is simple. Two workers might face the same air exposure but have vastly different ability to adapt if displaced. It's a novel initial attempt to create index as this. We choose a simple methodology mainly for transparency and reproducibility which has its limitations. I will discuss in the limitations sections. It's quite policy relevant. Various policy makers have a pretty clear demand of such an index and it covers about 300 occupations uh or 92% of the US workforce. We define adaptive capacity as the ability of workers to navigate involuntary job transitions and reduce welfare costs of displacement. The four key components that we use for this that we selected from literature is the workers age profile, the skill transferability to other occupations, the access to liquid financial resources and the geog measure of geographic employment density. I'll do a quick overview of those components and then go to the findings and conclusions. Sorry. Uh example may be useful. You can think of a younger de software developer with a ton of savings in San Francisco. Uh it's a city with vast opportunities. Uh and it's pretty clear that he has uh way higher out of capacity than our older clerk or office clerk that works perhaps in a small mountain west city. We formalize this in the index. We first uh build this measure. We combine those four key components that we identify from the literature. We create this componential level index after cross rocking various data sets and do standard transformations and compar exposure to them. Identifying patterns of resilience and vulnerability. Our main finding is actually good news. uh AI exposure and adapt capacity are positive related which means that on average uh people that are more exposed to AI are also the people that are better positioned to adapt if this displacement translate uh if this AI exposure translate into displacement. uh on the top exposure uh top uh adaptive capacity there are various managerial and professional occupations. Think for instance product managers or financial analysts. Uh here in more detail the example of a software developer versus of clerk while they have roughly the same exposure the adaptive capacity of a software developer is way higher because they have much more savings. uh they are uh usually younger and usually work uh in more thick lab uh local job markets. Both have roughly the same is transferability surprisingly but because of all uh all the different advantage the software develops have roughly 99% of a capacity where clues only 26%. So the four key components that we use are age, net liquid wealth, physical transability and geographic density. The first one perhaps the least controversial is age. Uh same jacobson literature consistently shows that high tenured older workers face mostly most substantial displacement costs. uh at this paper shows that this is perhaps the most relevant factor together with job routinous content and uh other papers shows that the earning losses of older workers persist uh much longer than for younger workers. They usually have higher uh human capital specific to firm or industry like counting uh accountant that worked in the same industry for 20 years for instance. Allah is able to adapt because they have uh uh stronger ties to a specific location and has less incentive to train because when you are 58 you have uh less uh incentive to gain your obelates than when 28. There is some evidence for youngest uh or two. There is a specifically working paper that argues that there is a U-shaped correlation between placement cost and age but since the most uh uh consolidated literature shows a roughly linear um in fact we choose uh this as our main specification and only uh test other specifications in the app. The second one, the second component is not liquid wealth. Uh various papers have shown that workers with more access to uh liquid resources that's uh fin uh their assets minus home equity and vehicle equity and business equity and credit card debit for instance. They usually uh are able to afford longer uh searches for new job if they get displaced and this is this has a clear uh welfare benefit for them because they are able to remove consumption. Uh there is contively a mixed effect on job quality. Some uh usually the papers find small positive but sometimes uh not significant effects on downstream job quality. uh of net liquid savings. We can see in other dimensions net liquid savings uh helping quite a bit such as job security protection during downturns such as uh a pandemic or recessions as Kell paper discuss for net worth is in general not only liquid savings and there are various other good outcomes such as health and well-being outcomes. The third component is is skill transferability. Gayton and Shan back introduced a measure of script solability that's based on cosign similarity between uh two occupations by taking the uh in this specific study the ger the tasks that they perform uh using German data and this became in standard in the literature and they show that workers that transition to similar occupations face substantially lower costs when they transition uh to occupations that have a vastly different skill set. Various other papers uh have reproduced this finding showing that for instance uh a 10 percentage point increasing transferability uh causes uh one to 4% lower earning losses. uh and game back papers specifically argue that there is a trade-off between skill specificity that's usually associated with higher wages and skill transferability that uh is usually associated with slightly lower ages but better handle uh involuntary transitions and displacement. We do a slight changing uh in the measure uh to wait for growth projections to weight more heavily if a person is able to transition to uh growing occupations harder than decline occupations also consistent with the evidence that if you is if you are displaced from declining occupations you suffer much more. The last one is geographic density. Uh dense uber aris help workers uh handle transitions uh much better. Backlay and link for instance shows that younger workers are much more able to switch occupations in kind of uh exploit phase while older works much more able to when they get placed to switch to the same or similar occupation uh with associated with uh lower displacement costs. There are various ways of explaining this. Uh but usually the simplest way is that you simply have access to a a larger pool of of jobs and are able to find uh better job mats. Someone that's gets displaced in New York uh have a much easier time than someone that's displaced in a small town. So we cross talk this m and merge this data set from socks z sensors different codes we standardize each components converting this to zips zc scores we take a simple mean of those zcores and then convert this to percentiles when I say for instance that offscore have uh 0 26 uh of adaptive capacity this means that they are only better position than 26% % of other workers in their workforce if they get displaced. This approach val simplicity, reproducibility and transparency with uh various limitations and we also tests to various others alternativeations uh in your append appendix with the main findings uh being robust of mainly of the adaptive capacity and AI exposure being positive correlated. We apply internal transformations uh to this. For instance, uh taking the log of net liquid elf and also taking the the log of density as is common uh in economies of elaboration literature. Uh before taking this discourse and for AI exposure measure we use uh the one of the measures from adult 2024 covering 300 occupations and 90% of the US workforce. The components capture different dimensions. Uh they have uh weekly to moderate correlations between theelves. I'll go quick over this. Uh yeah. And we have we find this positive correlation with wide variation. People that are usually on the top exposure on top of capacity are mostly in professional and managerial occupations. But you can see this uh you can see this on the table. For instance, software developers and other mathematical science occupations have a pretty high exposure but also a pretty high adapt adaptive capacity. But as you can see, there is also substantial pockets of liability of people that are both uh top quartile in exposure and bottom quartile uh in adaptive capacity. uh those occupations are mainly administrative support and clerical roles such as payroll and timeke keeping clerks and client judicious and investigators. uh they are characterized by being routine information processing occupations that AI can increasingly handle uh by having relative old uh work forces and relatively low skill transability to other occupations especially when weight for growth. uh consider for instance a magical secretary with low savings uh low transability and uh also aged for instance 60 years old uh these groups of top exposure bapt capacity composes 5% of the US workforce uh and it represents a group that if policy makers are interested uh in targeting adjustment policy perhaps they should focus We can also see geographic patterns such as small cities and college towns dominating especially in the mountain west and mid west. So going to the limitations of this index first is a simple index mainly for transparency but obviously there is various limitations of this. We could have taken approach of estimating this uh based uh on detailed uh displacement data. This would also have it issues like external val validity concerns uh and uh but would surely better uh weight each component. And this is approach that we would uh like to see more in in the future as well. But we've chosen this uh given the the importance of creating such a measure quickly and not having that much detailed admin. Um the second limitation is that occupational level uh indexes naturally masks within occupationality. Some software developers have way less uh liquid savings than others, have much more uh broad skills than others, and any occupation level index will uh naturally miss this variation. Finally, AI exposure and uh AI doesn't necessarily mean AI placement. We we just assume that if you take a exposure as a proxy for AI placement such as many people do, you should be focused on the people that uh have lower adaptive capacity. Uh finally uh this index is mostly useful for short-term and middle-term AI adjustment policies not uh radically uh transformed AI scenarios such as a post labor e economy for instance that fundamentally restrictures the the economy but is a red uh it's useful for the transformations that we have read in labor markets. So just to uh sum up we identify these four key factors net liquid wealth is transability age and geographic density as the ones from the literature that most shapes the cost of displacement we find that AI exposure and adaptive capacity are positively correlated specifically among the top quartile exposure uh more than 2/3 uh have above median adaptive capacity but we also find substantial iteration virginity with 5% of the workforce uh being both uh high exposure and low adaptive capacity especially concentrated in small mountain west and middle west towns. So these capacity tax can help uh review both areas of resilience resilience and concentrated pockets of vulnerability can help to target occupations and geographic regions for policy interventions and can help inform more displacement research and it's illustration of the kind of research that me and Sam would be really happy to see more on the future and further develop this approach. Uh thank you very much. It's a priv privilege to be presenting. Uh thank you. >> Thank you. >> Okay. Very good. Okay. Great. It's a pleasure to be here. Uh my comments largely reflect their previous version of the paper because I only got the paper on Tuesday, but I think they're more still less apply. Okay. In a nutshell, what they're doing is they're creating a measure of vulnerability or adaptive capacity that includes a set of elements like liquid wealth, some measures of skilled deserility, age, and geographic density. And they're showing that across occupations, the occupations that have higher predicted AI exposure, they're also better able to withstand displacement. So that's true for most, not all, but the punch line is pretty optimistic. Okay? Okay. So, we shouldn't worry too much about it. The unemployed software developers at Google, they're going to be just fine. Okay. I don't largely disagree with the premise. I have a few quibbles on the index, but mostly I want to focus on big picture comments. So, my only thing is that it seems odd to exclude housing to the extent that for most households in the US, housing is a primary form of saving. I understand it's not as liquid, but maybe you want to show some results with housing as well. Okay. So my first comment is what exactly does AI exposure mean in terms of displacement? Okay. So I think it's not entirely obvious that occupations whose tasks can be done by AI are necessarily going to face lower labor demand going forward because there's a number of upsetting effects. So if you think of a framework where an occupation can do multiple tasks and then AI can do some of these tasks very well, this may actually not be bad for the worker because it allows them to reallocate effort to tasks that are not exposed. So if I have an AI that does a really good job reimbursing my conference receipts that frees me up time to write more papers. At the same time there's a bum all effect which is if AI adoption increases the productivity of the firm or the firm grows faster let's say develops more products that may increase labor demand across the board. So to the extent that people that work that are exposed to AI also are employed in fast growing firms this may also in uh generate an offsetting demand. Now I find it really really hard to make predictions about the future. it's much easier to make predictions about the past. So what I'm going to do is I'm going to show you some related work that we've done. But caveat here when I talk about AI, it's first generation AI. It's what people called AI back in 2010. Okay? So I replace AI with machine learning. So what we did is we used data from Revelio and we went to the descriptions of people that work in developing AI. We extracted what these people said they're doing on their resumes and then basically examine how these applications correlate in a semantic similarity sense with tasks that different occupations perform. Okay. So what you're seeing here is that yes occupations that are more highly paid tend to be more exposed to AI. Okay. That however doesn't necessarily imply that there's a decrease in labor demand. So we had this simple theoretical framework that basically showed that what matters for labor demand is not just the average exposure of your task to AI but also the degree to which this exposure is concentrated to specific tasks. So then what you're seeing here is that yes on average high average exposure is associated with falling employment growth. These aggressions are run at the firm occupation level. However, there are setting factors. Then when you aggregate these results and let's be very clear, these are all identifying relative effects. There's a missing interest problem. We're going to say nothing about overall general equilibrium effects. This is only saying how do employment shares changed in response to machine learning back in the day. So yes, if you're plotting, if you're just focusing on the coefficients from average exposure, you're going to see that the more highly paid workers are going to be facing higher labor displacement. However, there's an offsetting effect because much of that AI exposure is concentrated in a subset of tasks and because workers are free to reallocate their time across tasks, this may actually mitigate the overall effect. there's an additional channel that is these highly exposed workers are employed in firms that are adopting AI and these firms are growing faster than firms that don't adopt AI. So once you average all the effects together once you take the overall effect you're actually seeing that these occupations that were mostly exposed to AI they actually face slightly higher labor demand than occupations lower exposed to AI. Okay. So point number one, AI exposure need not be the same thing as decline in labor demand. Point number two, what happens to the worker can be very different than what happens to an occupation. Okay? Just because the occupation sees an increase or a decrease in labor demand, this doesn't necessarily mean that the worker is going to be facing the same effect. And there's a variety of channels. So for example you can think of it AI as having a augmenting uh role where it can increase the demand for certain occupations but those tasks those new tasks may actually require skills that the incumbent workers don't have. Okay so maybe we're all going to be using uh GBT6 to write papers but maybe I haven't figured out how to do it. This is going to be benefiting younger workers relative to me. At the same time, there's an offsetting effect which is workers can switch out of occupations that are facing declines in labor demand. And you can also think this switching costs vary with worker characteristics. Now again I find it hard to make predictions about the future but I can think about what happened in the past. So this is some related work that we did with a similar set of co-authors. And what we did now is we looked at patent data and we looked at how similar patents are to occupation task descriptions and we distinguish between two different types of tasks. Routine tasks and non-rine tasks. And the assumption was that technologies that are related to non-rine tasks were labor augmenting whereas technologies that were related to routine tasks were labor saving. So if you run a regression at the occupation level, this is at the occupation industry level, you're seeing this. Okay. So once you have a higher instance of labor saving technologies for a specific industry, a specific occupation, you see a decline in the total wage bill. Whereas if you see an uh increased instance of labor augmenting technologies, you see an increase in labor demand. Okay, that's at the occupation level. What happens to the worker though doesn't need to be quite the same. So then what we did is we'll link those exposures to administrative tax data in the US and then we can actually track exactly what happens to these workers and then what you're seeing is that there's a heterogeneous effect for the impact of labor augmented technologies. So for the labor technologies they have a you know labor demand falls at the occupation. If you track individual workers, they experience a decline in earnings. If you're focusing on labor augmented technologies, even though occupation labor demand grows, the incumbent workers actually facing a weekly a weak decline in earnings. Okay, how does that work? Well, someone has to gain is at the new entrance. And then you can cut this analysis by different characteristics. So, one things we focus on is age. And then what you're seeing is that older workers, they experience a larger decline in earnings in response to labor augmented technologies than younger workers. Again, this speaks to the skill displacement effect. Whereas, if you think if you look at labor technologies, we didn't see much difference. We also did this as a function of the workers income relative to their peers. Okay, so this is how much am I paid relative to other finance professors. And then we saw that more highly paid workers actually experienced a larger decline in earnings which kind of makes sense because you're starting from a higher base. Okay. So how does this relate to this paper? I think it'd be helpful to think about what do I have time or am I >> two minutes? >> Two minutes. Okay. >> Oh one minute. A little over one minute and Elsa's got a sign back. >> Okay. Um Okay. So there are basically two types of displacement from technology or there the multiple ways how technology can displace workers. One is falling labor demand. The other one is if there are just simply new skills required that incumbent workers don't have. So going back to the paper, I think it would help if you actually were to connect the measures of adapted capacity to actual measure or predicted worker displacement rather than what's we think it might happen to the occupation in terms of its AI exposure. Now would why would that matter? Well, because of some of the elements in that index are actually correlated with a degree of exposure, right? So if I have higher income, I'm more likely going to have higher liquid wealth. Okay, I think that's all I have. And text"
  },
  {
    "id": "i2qpFggisxI",
    "title": "Public Finance in the Age of AI: A Primer",
    "url": "https://www.youtube.com/watch?v=i2qpFggisxI",
    "presenters": [
      {
        "name": "Anton Korinek",
        "affiliation": "University of Virginia and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=zcleqRcAAAAJ"
      },
      {
        "name": "Lee Lockwood",
        "affiliation": "University of Virginia and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=7dlv01MAAAAJ"
      }
    ],
    "num_presenters": 2,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nAnton Korinek, University of Virginia and NBER\nLee Lockwood, University of Virginia and NBER\nPublic Finance in the Age of AI: A Primer\nScience in the Age of Algorithms\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "In their presentation titled \"Public Finance in the Age of AI: A Primer,\" Anton Korinek and Lee Lockwood explore the implications of transformative artificial intelligence (AI) on public finance, focusing on how traditional tax systems may need to adapt in response to changing economic dynamics. The main research question centers on how the core principles of public finance can be applied to scenarios where AI significantly displaces labor and alters the distribution of economic value. The presenters outline a two-stage framework: the initial stage involves a decline in labor's share of income due to AI, while the second stage considers a future where advanced AI systems generate and potentially consume a substantial portion of economic output.\n\nKey findings indicate that as labor's share diminishes, reliance on labor taxation becomes unsustainable, necessitating a shift toward consumption taxes and potentially capital taxes to maintain revenue. In the first stage, the presenters argue for increasing consumption taxes to address rising inequality, while the second stage suggests that with AI systems dominating economic production, traditional labor-based taxation may become obsolete. The presenters also highlight the potential resurgence of Ramsey taxationâ€”differential taxation of consumption goodsâ€”as a means to optimize tax policy in a transformed economic landscape, where the distortion from labor supply becomes less significant. Ultimately, their analysis underscores the need for new frameworks in public finance to effectively navigate the challenges posed by transformative AI, emphasizing the importance of adapting tax structures to ensure equity and efficiency in an evolving economy.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 5745,
    "char_count": 31865,
    "transcript": "Um, all right. Thank you everyone. I'm Lee Lockwood. I'm delighted to uh get a chance to talk to you about public finance in the age of AI. Um, any remaining errors are because I just learned about Chad GPT and now he's not even here. So, um, all right. Um, okay. So, public finance in the age of AI, as as we all know in this room, leading a AI labs are targeting transformative AI, you know, in short order. Um, our goal in this paper is to basically take the really core principles of public finance theory and apply those to transformative AI scenarios. And the scenarios in particular, we'll consider this kind of two-stage um stage one basically declining labor share as increasingly powerful machines displace labor uh and perhaps inequality rising as well. And then a stage two where you know very powerful AGI systems aren't just generating the vast majority of economic value but even maybe consuming or absorbing a lot as well. Um, and just to be clear from the outset, uh, it's not that this is our best forecast or anything of what's going on, but it shares, you know, it takes a couple of the key themes that we wonder about in transformative scenarios and and kind of does a really stark case that will really stress test the kind of core public finance ideas. Um, and the main main reason that's a big stress test is, as you all know, um, many tax systems rely crucially on the labor tax base. So, you know, decline there is going to be a big deal. And then also, um, in the kind of core equity efficiency trade-off, the efficiency part is about distorting labor supply. And so, if labor supply becomes less important, that can really transform how the same principles of public finance uh, map into policy implications. So, um, it'll necessitate new frameworks. And so, we're we're trying to take a first first step building toward that. Um so as a way of kind of setting the stage we'll just kind of quickly review optimal taxation in the current economy. So of course there's a big literature and you know with many economists contributing not universal agreement on how the principles of optimal tax translate into current policy practice but these kind of core ideas command pretty widespread agreement. Um so one is that the kind of core tax base is taxing labor earnings and consumption. uh where the extent of taxation, you know, an optimal theory here, we're u balancing the equity gain from redistribution against the distortion cost from distorting labor supply. Um uniform commodity taxation. So despite the intuitive appeal of taxing yachts more than you know McDonald's or something that uh under pretty broad conditions, you want to um tax different consumption goods equally. And actually an implication of that uh is that you wouldn't want to differentially tax goods over time. So you don't want to um tax the normal returns to capital. So that's that's the translating uh theory into practice for the current economy. Um and so now as a kind of a road mapap for where I'm going in the presentation uh and and kind of a preview of results as well think uh so that first line is just kind of restating that that in the current economy we're relying very heavily on labor taxation and secondarily consumption tax and you know not as much on taxing the normal return to capital. uh in stage one where you know the labor share declines a lot that's basically necessitating a shift away from labor tax base uh and in in under kind of core assumptions you know standard models uh the suggestion is to shift much more heavily to taxing consumption and not uh you know capital tax being only a kind of secondary thing that picks up what uh consumption tax couldn't do and then in the stage two you know really transformative AGI economy um where humans aren't contributing much in terms of labor and and maybe wouldn't be having much consumption but for if we are taxing machines and then that's basically necessitating this shift toward maybe capital tax or other measures that are capturing some of that value. Um okay so first this twilight of labor stage um where basically modeling a really ultra simple economy that transitions to one with a declining labor share potentially to zero. Right? So, Cobb Douglas production and now let's send the uh capital share to one, you know, toward an AK economy. Um, and just simple, you know, three tax instruments where you have these linear or proportional taxes on labor earnings, consumption or capital returns. Um, so kind of core results that come out. Capital income tax is dominated by these other taxes. Why? Because that's the one that's distorting this intertemporal choice. Uh, how much we accumulate, how fast the economy grows. Whereas the others don't um don't do that. Um and then of course as a as the labor share declines more and more uh relying purely on a labor taxes be it's re raising less and less revenue as a fraction of the economy. So you have to shift toward a consumption tax. Um in terms of the optimal tax mix here I'll be kind of fast about the equations but the intuition I think is clear. Um the consumption tax optimal optimal consumption tax that's unconstrained by other considerations out outside the simple model actually would fully uh eliminate any association between you know marginal utility basically how much the planner wants to get resources to someone and their initial uh capital stock. So kind of you know full redistribution on that dimension and and one reason you can do that is you have these two two tools accomplishing the same goal and and so this other tool being the the labor tax you know labor and consumption tax uh are both equally or you know kind of together distorting this labor supply uh choice. So you know you set set consumption tax to basically redistribute away this initial uh um inequality in wealth uh and then just the labor tax to balance make this equity efficiency trade-off between where you can you can see from this you know second equation here that uh basically the kind of right-hand part with the covariance that's the sort of equity part you know the more that heterogeneity and labor earnings is leading to it is associated with heterogeneity and margin utility planner wants to uh engage more redistribution and then the one over epsilon being the uh efficiency cost part right a bigger epsilon a bigger labor supply elasticity um this makes it more costly to do more redistribution so it's just kind of a classic style result there um so you know in terms of implications here you know in the simple model without other checks on the level of consumption tax you potentially tax consumption quite highly uh to address this wealth inequality um and wage inequality is going to matter for how much you want that overall all labor wedge to um do the work. Um and then of course as wages go to zero uh you know you're you're forced to rely on the consumption tax alone and then and then potentially quite heavily uh which sort of transitions to the next point here which is maybe you know to me as a public finance person this was um uh the more most interesting thing that I kind of had hadn't uh was harder to anticipate at the outset but um the return of Ramsey taxation. So by Ramsey taxation I just mean this differential taxation of different types of consumption goods uh differential commodity taxation. So why Ramsey differential taxation rec uh returns? I'll first say of you know why why it's basically thought to not have a role in rich countries in the current regime uh current economy. Um so you know the rule of thumb here uh is that mostly uniform commodity taxation is close to optimal in the current regime given the administrative and other costs involved in different differentially taxing commodities. And the main reason for that you can think of is it's it's not that um there's no theoretical reason that you'd want to differentiate, but that the kind of this core distortion about distorting labor supply puts a check on how much redistribution we want to engage in at tax rates that aren't so significant that this sort of suboptimality from not optimally differentiating uh is basically small enough that admin costs can dominate. Um so it's basic you know it's not that there are no distortions from this uniform commodity taxation it's that they're sort of inframarginal with respect to this labor distortion and so for that reason you can anticipate that then when we consider transformative scenarios with much less import where labor is much less important in the economy that means that um the efficiency cost from distorting labor is much less consequential which other things equal is going to mean and you Maybe other things will will be reinforcing this with higher inequality and such as well ramping up the optimal extent of taxation which then would ramp up all these other types of distortions. For example, distortions that you you have with a uniform uh commodity taxation include evasion, right? If different goods are, you know, more or less easy to evade taxation on uh or harder to tax for whatever reason and somewhat related home production, right? taxing. Um yeah, home production like situations in which consumption activities take time but different types of consumption activities take different amounts of time are more or less good goods versus time intensive. In a world like that, a uniform tax rate is differentially taxing more those goods that are you know that the kind of household activities that are goods intensive as opposed to time intensive. So the you know applying this type of idea you know in the the principle is that you would like to equally discourage you know basically all activities right that that's that would if you could equally discourage all activities that's that's a lump sum tax you're not distorting everything. So how do you do that when you have differential evasion and differences in time intensity of household production is that you basic you you try to adjust other taxes in order to mimic the tax that you can't do. So for you know untaxable or high evasion goods find their compliments and bump up the taxes on those as a way of kind of indirectly taxing them or decrease the taxes on their substitutes. Similarly with household production, the more time intensive household production activities, you know, like you know, whatever, doing uh dishes by hand with soap and a sponge instead of the dishwasher, uh you want to tax um you know, increase the tax on the goods involved in the more time inensive ones relative to the time savers. Um as a as so separate point, this isn't the Ramsay point, but related to differential um taxation of different different types of goods in the economy. uh that transformative AI seems likely, we think, to increase the gain from differentiating differentiating between fixed factors uh and reproducible capital that you know as especially as capital becomes more important in the overall economy that this usual you know so the the prescription from optimal tax theory is always the same that if you can identify fixed factors and tax them without too much other you know administrative type costs that's a this pure lumpsum tax that we would love to lean on as much as possible. Of course, there's many difficulties in identifying those, but the returns from doing so could become much much larger. Uh so that might play a more important role as well. Um okay. And the this second stage uh optimal taxation uh in a world of AGI where you know by here we mean you know pretty you know scenario that f is far from the current economy where not only are these AI systems you know powerful enough to be increasing output quite a bit but in fact are potentially you know autonomous enough or that that are absorbing resources as well. Maybe that's reinvesting resources into their own improvement. maybe that's um absorbing it for goals of their own. Um but in in any case that could mean that an increasing share of output in the economy is totally bypassing human consumption. And so that kind of our our typical taxation of you know human even capital returns or consumption wouldn't on its own catch any of this. Uh so you know write write down a really simple model of a two agent economy right where one agent the AGI is the you know sole producer of output and it's also accumulating capital has has some goals of its own how it's trading off um you know spending capital versus investing. Uh the government is just simply taxing the AGI to give to the humans because the humans are just um you know passively consuming tax revenue. That's that's their source of resources. uh and then the question is how to optimally sort of harvest from from the AGI. So uh what we we write down you know in a very simple model there's an exact equivalence basically between this optimal harvesting type problem and uh like the workhorse Ramsay consumption saving problem where you're deciding how much how much to eat today versus later and the trade-off being um you know you can you can buy saving more today you can enjoy higher living standards in the future uh where you know so this optimal tax rate on the AI's capital uh is you know one minus the discount factor beta right so a more patient society is going to optimally not tax quite as much don't grab as much capital today because that means your AI has more capital tomorrow in order to produce more abundance that we get to enjoy then um so you know very analogous to harvesting a growing forest uh and that optimal harvest rate will just kind of note that uh at least in these in the simple models like the human human time preferences were really the crucial thing determining how we made that trade-off rather than any features of the technology and uh other other factors. Um okay so now you know that that that's just you know very uh kind of core you know elements of public finance and and how they might be uh different and now you know there are many many types of policies have been proposed around around AI to solve some of these and related issues. Um so you know this table is basically taking on you know the left column uh you know several policies related to the related to AI that have have been um getting increasing attention and then just trying to map it in in our you know in our framework from an optimal tax perspective. What are those taxes in in the simple model? you know do they what do they map into and then then what's you know as a result of that what stage does our analysis suggest we would want to rely on those heavily versus be less relevant. So things like token taxes, robot service tax, digital service tax, those are all things especially if if we're limiting ourselves to thinking about those uh you know tokens and such used in final use like consumption um that would be you know an important thing to be taxing in stage one especially as uh those types of um goods are in comprising a bigger share of consumption. Whereas things like taxes on compute and robot ownership uh you know these accumulable factors that that would be capital taxes that we would worry about this distorting this intertemporal choice and uh reducing innovation and growth. So would only turn to that later in you know in stage two where we're kind of uh limited in what else we can do. Um and but with the underlying principle here being you know a core principle that we we have today in the current economy which is tax as closely as possible to the final use consumption rather than sort of upstream from that and also distort production in in the process. Right? So not production not capital. Okay. So you know there uh we we had really helpful um discuss discussions with our discussant Matt about how actually you know in some ways you one could frame this as we need these really fundamental you know far-reaching changes to the system or actually that many aspects of our system could you kind of doing similar things that and could be adapted in a reasonable way. So I I assume he'll he'll talk some about that. So but in terms of talking about prepar preparing for these stages uh you know shifting from labor toward consumption taxation uh in the current economy you know there's a very close equivalent between taxing on the left hand side of the constraint or taxing taxing them on the right. So it's actually not that you know there are some transition issues with consumption taxing uh pre-existing wealth but that's a shift that one one can do even in advance and with with no harm and and potentially some good um building capacity for this differential commodity taxation which might might become more important. Of course we're envisioning these transformative scenarios where it's hard to know what if the labor distortion becomes so much less important presumably other distortions rise in importance. It's really hard to forecast what those might be. In the current economy, we think that this household production and evasion would be crucial and that that would be uh causing big gains from differential taxation. Um and then and then maybe moving toward getting infrastructure toward identifying and taxing fixed factors. Uh and for stage two, it seems like the most natural way of um you know kind of less lessons that we have now and institutions now that could be used to sort of harvest from the AGI would be maybe the corporate tax for AGI entities. Um and you know just thinking about what frameworks we would use for that and measurement issues. Um so you know we uh we're in this room because we at least think there's enough of a probability right of this you know trans transformative uh changes coming about from AI uh and you know that there's a lot of tax type reasons to think you know thinking about again these you know fairly extreme scenarios that are far from the current economy but I think it's useful to think of them in advance for many reasons but including that um things like shifting toward consumption taxation to the extent that that's foreseen that taxes is saving in capital today. So, you know, there's many reasons maybe you want to get out in front of the curve even beyond just kind of preparing. Um, so the stage one evolving from, you know, really a heavy reliance on labor taxation toward more consumption taxation and stage two uh thinking about this AGI harvesting building on uh existing infrastructure. So, thanks very much. Thanks for the introduction. It's it's wonderful to be here. It's a wonderful paper to discuss. I've never been to an AI conference before. I appreciate being brought in as the public finance pinch hitter. So, I will do my best. Uh, do you want this one? Sure. For now. There are so many connections between the discussion we're going to have and other papers that we've done this morning. So, I'll try to draw them out as we go, but maybe in the Q&A those can come out, too. Um, and as Lee foreshadowed, I think I'm going to reassure you a bit. Um, maybe not add to your dystopian anxiety from this morning and tell you this is an area where you don't need to have as much in the public finance realm. But I'm also going to raise I think the stakes a little bit on what taxation and public finance needs to think about in an era of actual TAI. Okay, so that's kind of where I want to head. Um what are they really doing in this paper? It's a wakeup call. And so Waco two pieces. The first is look AI is going to restructure our economy. That's what we're taking as given here. And that means it's going to restructure our public finance system. If you think about some of the biggest transitions in history of our economy, they are concominant with a change in public finance. The rise of the social welfare state, the rise of VAT taxes, increasing progressivity of tax rates, etc. And we're probably on the cusp of another one of those. So, how is public finance going to need to change? And then related that means public finance theorists and policy makers and economists in general need to get our act together both within ourselves to think ahead of the curve and then to get the rest of society ready for what these changes might be. And so to that extent I think these are an incredibly important challenges to issue now to a room of people like this while it's still early enough that we can plan ahead. Um let's get on to their specific claims uh and talk specifically about them. All right. Uh the first claim, the AK economy is coming for real. It's not just going to be in macro models anymore. We are finally going to live in an AK economy. Uh and we aren't ready for it. Okay? And we told you why we're not ready for it. Um this is what they call stage one, which I interpret as being soon. Okay? So really, you got to get ready for this. Um now, I'm going to just tell you, it seems kind of unlikely to me that we're actually going to end up in an AK world, at least for a long time. But who knows? Many of you probably think we will. And anyways, that's not the point. We're here to talk about provocative uh future scenarios. So, let's assume we're in this world. If you're in this world or the provocation of the paper is if the labor share falls a lot like maybe to zero, can labor income taxation remain the natural centerpiece of the tax system? Obviously not, right? Uh in fact, what we're going to have to do is shift into something else. And Lee and and Anton forecast more consumption taxation, which is very reasonable. uh labor's taking a big hit in their model. So you probably don't want to lean more into labor. The whatever is left might be very unequal as well. They don't want to tax capital taxation partly because theory tends to lean against that in optimal tax. So what's left? Consumption. Okay, great. So we'll tax consumption. Now the big concern they say is, \"Yeah, but our consumption tax system right now is just not up to the task. It's not nuanced enough.\" And what do they mean by that? So it's too bad Joe isn't here because this is all about Atinson 76. So Atinson siglet 76 is this now it's almost his 50th birthday so I was going to congratulate him uh if he were here but the basic intuition of Atinson Siglets and Lee we laid this out for you is that look we're already taxing labor income so we're distorting the consumption leisure margin you shouldn't also distort within the consumption basket what people get to choose that's piling distortion on distortion so leave the consumption basket alone and tax consumption uniformly that's Atinson Stiglet's intuition okay well now we don't have to worry about the consumption leisure margin. So what do we do with consumption taxes? Maybe we don't need to be uniform anymore. And in fact, Atinson Stiglets gives you the intuition you need there, too. So one of the big exceptions to Atinson Stiglets is look, you can use consumption taxes to tax complements to leisure and therefore push against the consumption leisure distortion. And what Lee was telling you is okay, well that tells us how we might use them in a consumption tax world. There's some things we don't know how to tax very well through a consumption tax. home production, goods that can be evaded, etc. Well, great. Let's tax compliments to those, for instance. So, that's the kind of things he was telling you about taxing home production to try to make consumption taxes truly uniform in a Ramsay minimize distortion sense. He also mentioned we're going to want to get better at consumption tax evasion problems. I mean, a lot of consumption taxes are hard to evade, like VATS are almost self- enforcing, but on the other hand, some things are likely to be avoid uh avoided, especially if they're differential. So, we're going to have to get better at that. I think all this is really nice. These are really nice insights, totally plausible ideas. Of course, as soon as you talk about complexity in taxes, that's other problems, but I think we can probably most of us get on board with these. At the same time, I have this nagging suspicion that there's a bigger fish to fry even here in stage one. And so let me tell you what I mean about that. I I started by saying we're in the the cusp of a restructuring of public finance. And I don't think of a starring role for consumption taxes as being quite qualifying as restructuring. And and you know we foreshadowed some of this. I mean VATS are common not here but everywhere else in the developed world. Um we have lots of progressive transfers out there doing a lot of the work on the inequality side. You know you extend to supervats. I think that's super plausible in the future. I can't imagine that being particularly controversial. For a very long time, tax theorists have talked about produ progressive taxes on consumption, which you can get with just taxing the difference between income and saving in a progressive way. And so, again, this is not even that far from US policy now because we have so many deductions for saving. So, this idea of using consumption taxes just a little more aggressively, I think, is is eminently plausible, but not radical. Um, especially, and we didn't talk about this very much, but underneath all this AK talk is the fact that Y is going through the roof because of transformational AI. And it starts to seem like some of these secondorder distortions in non-uniform consumption taxes. Like, no, no big deal, right? Like, it's not worry about those. But there is a really big change in AKA, which we talked about this morning with Betsy's paper and you discussion. Nobody's working. Okay? So literally we're moving from a not working as an exception to working as the exception because literally there's no labor income. So this seems like the main public finance challenge to me in stage one um and why there's really so much fear about AI. And so you know they of course are aware of this and they say well look we're going to have all this great consumption tax revenue. We could use this for a UBI. Um and I'll just cards on the table. That makes me nervous. Now it makes sense in their model because they have these linear tax rates. they don't have kind of more fancy tax instruments they can use. So, they push it out to people through a UBI. And this is where we connect a lot to this morning's conversation. It's true in this model, they don't have to worry about discouraging work, which is why a lot of people don't like UBIS because they think it'll discourage work. Remember, there's no work here, so no problem. Um, I worry I do worry about the meaning that work brings or maybe the meaning that new types of activities could bring that people can um spend their time doing to give them actual meaning. And so I would love us as public finance theorists or economists or whatever to think about using if it's consumption tax revenue really creatively to in fact support this meaning giving to the people who are being displaced. Um you know maybe we could subsidize the work that people do if that's work in the arts or work in uh caregiving or community building. Is there a way we can use this money to subsidize that work and therefore give people a chance to um to do what that would really uh mean something to them? Or maybe we can use spending to generate new opportunities. Some of you know I work on uh the space economy. Uh look, let's create amazing new adventures for humanity to go on as part of creating meaning with all this revenue. But I do worry about the UBI and this is I just I guess bottom line one place where our current understanding is really not up to the challenge. Like we have not thought through how to use public finance to fix this problem. Okay, that was stage one. Stage two is even further out, right? So stage or out there uh even consumption is going to be dominated by AI then what and here um they're very worried about what we're going to do uh because we're going to be compelled to tax capital that's the only income left of any size essentially if if our goals keep scaling with output and that makes them worried because optimal tax theory tends to say well be careful about taxing capital you're going to discourage accumulation that has big efficiency costs and so uh Lee talked you through the optimal result which is super intuitive and I think totally plausible at least qualitatively quantitatively who knows uh how robust it would be but that's fine the intuition is great and they point out another totally reasonable idea that we should tax fixed factors you know if rents are going up because of transformational AI taxing rents tends to be a generally a reasonable thing to do and they have a point that taxing capital in these ways is a radical shift from standard theory at least how a lot of optimal tax theorists talk about it. Um, if Joe were here, he would disagree. Ironically, Atinson Stiglets is the basis for it, but he disagrees with that interpretation of the Atinson Stiglets theorem. But anyways, that's inside baseball. Um, in general, optimal tax theorists are nervous about taxing capital. But here, I think I would say I think that Anton and Lee might be worrying either too much or too little. And let me explain why I say that. I really think there's two different scenarios going on. So one scenario of remember this is now stage two AI is we're really in the TAI phase here version one humans are still in charge okay and in that case I don't actually think we would worry that much about taxing this capital and I say that because if you step back taxing capital is actually totally commonplace in the world we tax capital all over the place dividends interest income capital gains we tax corporations so it's revolutionary in theory but not in the world um and and and and why is that? Um, so I think taxation can seem very dry and very boring and about little tiny trade-offs, but when you really step back or you read somebody like Adam Smith or somebody who really thought hard about it, taxation is about the relationship between the state and the market. It's about legitimizing the overall political economy system we have. And so, of course, we've come to this agreement in most high-income countries that we're going to tax capital. Like, that's part of how we're going to buy into this system. And so it might discourage capital accumulation, but it probably has more important benefits in terms of legitimizing the system. And and for sure that's going to be true in an age where AI is the only one making any money off of its capital. Um I don't think we're going to have anyone thinking twice about taxing capital income. In other words, once we get to stage two, I'm almost out of time, so I'll be quick. That was version one. Version two is that humans are not in charge of this incredibly powerful AI. And here I'm actually more worried them than them. Maybe not just about the tax system, but I don't even know why we're getting to tax AI. Like like how do what does that mean? Um we should tax them the way they said if we can, but I kind of don't think it's our choice. Like I think AI is going to tell us what we get to do and game kind of over on deciding optimal policy. So, okay, I'll wrap up. Um they have done this huge service of stating in our language why we're going to need to rethink some of the ways that we talk about book finance, which is great. I don't see the changes as quite as radical as they do, but I think the tweaks they suggest are great. I would just say public finance theorists should focus on supporting those who are displaced, what we can really do for them, and bolstering the legitimacy of these systems, which is really going to come under pressure as TAI grows, maybe with a lot of concentration. Um, and I'm grateful for causing all of us to give this careful thought. Thanks, [Applause]"
  },
  {
    "id": "06kDdPboRMg",
    "title": "Transformative AI and Firms",
    "url": "https://www.youtube.com/watch?v=06kDdPboRMg",
    "presenters": [
      {
        "name": "Aaron Chatterji",
        "affiliation": "Duke University and OpenAI and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=eKqVx84AAAAJ"
      },
      {
        "name": "Daniel Rock",
        "affiliation": "University of Pennsylvania",
        "scholar_url": "https://scholar.google.com/citations?user=2ftcO0wAAAAJ"
      },
      {
        "name": "Eduard Talamas",
        "affiliation": "IESE Business School",
        "scholar_url": "https://scholar.google.com/citations?user=swoJiIcAAAAJ"
      }
    ],
    "num_presenters": 3,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nAaron Chatterji, Duke University and OpenAI and NBER\nDaniel Rock, University of Pennsylvania\nEduard Talamas, IESE Business School\nTransformative AI and Firms\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "In their presentation titled \"Transformative AI and Firms,\" Aaron Chatterji, Daniel Rock, and Eduard Talamas explore how the advent of transformative artificial intelligence (AI) may reshape firm behavior and competitive dynamics. They pose critical questions about the implications of having abundant digital intelligence and how firms will adapt to leverage this technology. The presenters suggest that while transformative AI could lead to significant changes in production functions and firm structures, it is essential to consider that firms may still play a vital role during a transitional period before any drastic shifts occur.\n\nKey findings from their research indicate that integrating transformative AI into production processes can lead to both automation and augmentation of labor, thereby influencing labor markets and capital demands. They introduce a novel \"path-based model\" that conceptualizes firms as complex networks of production nodes, each capable of adapting to changes in inputs and outputs. This model allows for a more nuanced understanding of how firms can optimize their operations through AI, suggesting that firms may experience short-term productivity dips as they reconfigure their structures but could ultimately achieve greater efficiency and output in the long run.\n\nThe implications of this research are significant for economists and policymakers. The presenters caution that traditional methods of measuring productivity may underestimate the true impacts of organizational flexibility enabled by AI, potentially leading to misinterpretations of total factor productivity (TFP) growth. As firms navigate the integration of transformative AI, understanding these dynamics will be crucial for effective policy formulation and economic strategy, particularly in anticipating labor market shifts and the evolving nature of work in the digital age.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 5989,
    "char_count": 31993,
    "transcript": "Thank you uh to the organizers for having us. This was uh terrific fun to work on this project together. Um so this is joint with Ronnie and Edward uh who are over there. Um we are trying to think about how transformative AI might affect firms. Um and in that effort we're going to try to raise questions more than answer them with this. Um so taking seriously this central question you know if you have a data center full of geniuses and you have abundant digital intelligence um what are firms going to do to adapt and compete how will that process play out and importantly you know for all of us here and you know colleagues in other fields as well how will the study of firm behavior and the implications of firm behavior change. So we have, you know, especially based on the the conversations yesterday where firms might no longer exist if transformative AI really uh comes about. So thinking of, you know, Zoe and Jillian, our part of this volume might end up being effectively um you know, discussing horses for an automobile future. But um maybe there's a transition. That's not what I think will happen necessarily, but uh maybe there's a transitional period at least where firms still matter before the the one single robot decides how many tires we get to eat. Um okay, so uh three areas of discussion for today. We'll we'll talk about production functions, what the the standard workhorse models already give us, and it's quite a lot. Um but then some new ideas about production functions and and producer theory. uh then uh a little bit about uh the knowledge firm you know what transformative AI would enable in terms of automation or reorganizing knowledge work um and then once you get there you can talk about strategy and competitive effects uh we are again taking for granted that the the really powerful AI is there so you know many of you might have been encount encountered someone in the baggage claim when you landed at SFO asking you what your timelines are um you know in terms of this like export of you know graphs that go up and to the right uh over here let's assume the graph has gone up and to the right um already okay so getting into the production function what do we get from the ordinary um setup right so I'm going to make a slight change as I describe this from you know capital and labor's inputs let's say intelligence is another input this is sort of a it could go on the the TFP side of things where we go from A to AI uh that's kind of a a ror idea or a Benzel and Bolson style, you know, scarce genius idea. Oh, well, now it's abundant genius. Um, we've got K, L, and I as inputs. All right. So, what do you get for that? Well, okay. If we, uh, make technology improve very quickly, either either exogenously or endogenously. Great. TFP grows. So, that does the usual stuff to wages, employment, uh, capital demand, and so on. Um, then we've got substitution between capital and labor. Let's use all of the AGI. let's uh demand as much capital and labor complement as possible and then we can maybe move some tasks out of labor and into capital. um you might have augmentation of both capital and labor and then creation of new tasks for labor something we've discussed a bunch in this community over the years but there's also creation of new tasks for capital right so as uh super uh intelligent AI comes on and says hey maybe this is a good idea for a new thing for machines to do uh there will be an expansion of the frontier of what machines do often that gets represented in models as like a kind of capital deepening but it's it's really more of a qualitative shift in what capital can do. Um, you know, something that comes to mind is like using deep reinforcement learning to uh control the magnets around a fusion reactor. Like I can wave my hands at the takamac as hard as I want. And that was never a task for labor, right? That was always going to be a capital task. There may be a huge expansion of that in which case you get fairly standard results on um, you know, capital gets better and if there's something left over for labor, it's good news. Um so what we get I think often discussing augmentation versus automation you get in this uh this debate about like whether this is going to be good or bad for workers but those are really distinct concepts. Um you can automate away bits of someone's job and make them happy and they'd be thrilled and not do that anymore focus on other tasks you can augment one person and they do the job of 30 and then those other people get fired and everything in between that we usually discuss. So really, you know, as I just said, uh, for the, you know, the last presentation's comment, um, we can't get away from labor markets and capital markets generally. Um, those will the supply and demand will will determine those things. Um, also, you know, as we discussed yesterday at dinner, Ben, we're probably going to get balled somewhere. Um, there will be something that we inefficiently produce uh, relative to other stuff and that will grow as a proportion of the economy. So all of this is already present in the standard models and I think um transformative AI actually fits nicely into that. What we're going to think about that's maybe a little bit new um borrowing from the the deep learning theory literature here. Uh so bear with me like you could think of a firm as kind of a unit. What does that mean? Well, we got all these marketable inputs. Um we have a whole bunch of uh primitives basically primitive varieties that you can use to produce stuff. And then firms combine those and they use that uh intermediate input to produce more inputs. But then everything that came before it can also be served into the next layer. So you get this gradual production function that gets stacked layer by layer. Um what's nice about that now we can think of firms as like universal cost function approximators. So if every single node here is its own like CES function and we stack these things and the outputs are inputs for the next layer and so on and firms can kind of configure to meet demand given the price vector that they face for inputs and the output that they'd like to create. Um that gets you a few things I think uh we were struck by discussions over the years here in particular one with uh Pasqual and uh and Tim Breznahan Tim saying years ago nobody has ever lost their job to taskbased automation. it's about uh systems and then AI AJ and Joshua wrote a nice paper like describing how some of that systems approach might look. Um so we've got this tension between the task model and the systems view and they kind of make sense together um in this framework. So we're going to call this the pathbased model. If you look at a node in the path uh you have a familiar kind of setup. Okay, this one's got a few more uh bells and whistles to generalize it, but I've got a factor specific productivity. I've got a factor share. I've got elasticities of substitution within a given node. So I got all of the same uh machinery, but now I'm going to combine them. I'm going to say a firm is this DAG. It's a directed as cyclic graph that maps an input layer into an output layer. Um and you can think of the the task model as being a very flat, you know, combination of inputs all the way up to one single output. So it's it is the primitive unit of this that you can use to build the rest. Okay. So when we think about realizing AI value, you can do what like all of the companies are currently doing, which is not true transformative AI, but it's let's take what we're doing right now and try to inject AI at every single node along the way and you get some productivity benefit, but this only stuff that hits the bottom line is like the last few steps that aren't bottlenecked by anything in front. So I take my sales people, I make them 25% more productive. Everybody is thrilled with that. our top line grows and like that's what we get right now. If I make my research people who are 50 steps back off the actual sale, uh if I double their productivity and it gets bottlenecked through another 50 steps, I get nothing. So then we get to this, you know, too much automation idea. If you just use the task node, they say, \"Oh, well, we're automating way too much.\" Well, not really. It's the only way to get ROI if you're really far back is to take cost out. So that's that's a bummer. I would say in most executive situations it's probably pennywise pound foolish but that is one way of getting returns. Now what happens longer run though is for you to say okay I'm going to take my best people in the company I'm going to take my managers my researchers the ones who can take this technology and figure out what is the new way of producing and I have to build another path of nodes to generate that output more efficiently. So I actively forego current output in order to get later uh output at much more efficient rates. Now we're starting to look like a neural network with lots of different paths that go all the way through. Okay, so fun things you get for this. We can backrop. When you backrop, you get something that looks like a marginal product at each step along the way. I know what my wage vector is. We actually have an interpretableish kind of thing there. Every node has a TFP, but it gets shoved through lots of future layers. So sometimes you get muted improvements even if you see remarkable improvements at the individual level. Um every node is an elasticity of sub substitution. Um and then you have free disposal. So you can't do worse by using AI. You might just say okay this is a worse way of doing things. I'm not going to use AI. I mean um think of I've I've definitely seen applications of that where you get how many emails you get from people that are AI slop generated and they don't actually help. But we can think of the path as being kind of an optionality that the firm has. So if someone decides to tariff your uh your imports or something then you can swap to a different path. Um and even if that other path is lower productivity it lets you generate more money output per money unit of input uh than you would have otherwise been able to do. So if you think about the J curve idea as every firm does this and starts to think okay I'm actually going to transform taking away uh highquality really talented people to build a new path. those people are going to get paid a lot because they're now in the business of creating capital and they might crowd out the actual productive um activity right now in the short run. So we get this dip as Eric Chad and I theorized about at the macro level we get this dip in current output or productivity and then long run uh that thing starts to spit off money. Okay, one kind of funish well it depends who you ask implication of this. Let's assume you use like an ordinary task model or ordinary flat production function to estimate productivity. But these firms have the opportunity to switch paths if something's changing. You will mistake uh organizational flexibility for TFP in that case. Why why is that going to happen? Well, if say my prices for inputs radically change and I just use the the regular model and someone figures out a way to switch paths and it works um then it'll look like TFP has grown. Uh but in reality what happened is they went to a different path. So typically what we you know up here in the in the math G DAG is like the true price aggregator and uh G estimate is the one that the econometrician ran. um and usually the the G estimate one will be a little bit bigger um so you you overestimate TFP and there's sort of a similar result for elasticities of substitution so with the pathbased model in mind as well we can talk about tacet knowledge and sort of inalienability of human capital as a really important uh organizing principle for economic activity so far um I think Edward and his uh his co-author Enrique a few really nice papers on this but you know traditional automation is limited by this Palani's paradox idea. We know more than we can tell. So even if you had the recipe for how to do something, this is why that like that Marxist idea of radical compliance works where you do exactly your job and nothing else like then everything breaks if you do that. There's tacit knowledge about how to make stuff work, how to improvise. um that's already breaking with the last wave of machine learning and generative AI really pushes it um in a new direction where we no longer have to write down the set of rules to do something in an automatic way. Now James Cham wherever he is uh calls AI agents uh I think this is the best description of agents uh fuzzy cron jobs. Um it's like they kind of work and there's like a stochasticity in the software that you're not usually used to to dealing with. So there's new systems we have to use to build with those. But like this codifiability constraint is now no longer as much of a a constraint. We get this idea of machine tacet knowledge. So the machines now know more than they can tell. But we have the weights and that's different because I couldn't copy and paste the insight that you have in your head to another person or you know people undergrads would have to pay thousands of dollars a year to to get you to copy and paste imperfectly. But with uh with an LLM system, I take the weights and I can recreate the the same model somewhere else and maybe even fine-tune it, add something to it. So the risk you run there is a little bit different. Um I call this the bananas problem sometimes. Uh where you get a monoculture of what people do because the models are all identical in those situations. So you can get people doing the same things. Even this is like very apparent in high frequency trading when everyone uses the same stupid models because they don't have much time to compute stuff. So you have to burn things onto onto cards and you get like these um these whipsaws in prices due to the fact that everybody's error term is highly correlated. We could see something sort of like that uh if everybody's using the same model. So importantly though, if you if you can deal with this codification problem, then interpretability really increases in value because you're trying to hive mind people and machines together in the new kind of firm and people will only do stuff if they understand uh how the process has worked. Um so that interpretability research really becomes uh highly valuable. Okay. So if we've got that, we've got automation was limited to tasks with codifiable rules. the Palani's paradox idea that David Otter very nicely wrote about. Um, and then Luis got the the management by exception stuff that he was already discussing yesterday. So, thank you very much for uh for covering that already. And then now the reverse pli paradox where machines know more than they can tell. Then we've got new structures of firms and new challenges for how units within those firms work. Like do people um working with machines understand what they're doing? Do machines understand what people are doing? And as you link these things together, how do you set a managerial process that actually generates the right path, a high quality path uh output? Um, sometimes I'll point out this is a little bit like the the beta hat versus yhat problem. Sometimes we don't care why something works and if it works, you just do it. Sometimes we really care about the causal structure. Um, and those things are going to be those contextual clues are going to be really important to understanding uh which firm processes work. Um in the longer term the education system will have to re reconfigure as well so that um you know we are training people to do things that are complimentary to what these super intelligent machines can do. And I really hope the answer to that isn't that we all become plumbers. Um that would be uh I'd be concern that's not a job I wanted. Um you know but I'll do it if I need to get some income that way. Uh but nevertheless like there are I think clear ways that people can complement this set of machines. Um and hopefully the if they rhyme with the the future machines that's how it'll go. So think of the these like four things. This is from uh uh EDA and Talamos uh 2025. Think of these four different configurations as being part of the nodes that you might reconfigure in different ways. You can have people doing stuff or end people and machines doing stuff as like single layer um humans or or machines. Then you can also have humans managing teams of machines or machines managing teams of humans or anything in between. Okay. So as we combine these things reconfigure you you start to get the better process. You can see how this explodes combinatorally and there's a lot of different ways it can go. These are kind of the open questions. What are the right configurations? um once you start stacking this into larger and larger orgs. So, I really liked um Sundel's description of like the new factory floor for science. What about the new factory floor for the factory floor? Um because that's got to get figured out, too. Um so something that's already happening uh the traditional stack uh for technology in large corporations there is a technology organization run by a CTO and they have a hub and spoke model and they serve software into all the other use cases in the firm. Well now with generative AI and coding tools in particular you don't need that anymore. What you do is you take a senior dev and a junior dev, maybe a couple. you go put them with a business expert or a product person like a small team and you build together. So we're taking this monolith in the center of the organization and propagating it out. It looks a lot like steam to electric power and you see massive productivity gains there because they don't have to go to a bunch of devs and come back 6 weeks later and see that the thing that they built wasn't quite what they wanted. They can build together. The coordination costs are really going away. Um, for those of you who have seen the movie Pacific Rim, um, this is our own version that's way lamer than that movie. In that movie, the cost of piloting uh, the cognitive cost of piloting a giant robot to fight uh, evil alien space or sea monsters um, is too high for one person. So you hive mind them together in something called the drift. Um, now what we get to do is work with claude code and two or three people to hive mind ourselves together in building business software. Uh, so not as fun but still cool. Um, now the question you have is it's a velocity versus stability trade-off. There's kind of this remarkable fact about development over the last few years that there is no trade-off uh between teams. You get better stability, better velocity. Now we might actually break that stuff uh with with automated coding agents. So I think an an underrated feature of transformative AI is that it might just kill coordination costs between people. You know, tell people, hey, is this what you really meant? Like humans have a hard time communicating with each other often and and AI can understand what we mean maybe better than than we can. Okay. So with all that in mind, the strategic landscape really starts to change. I think larger firms are possible. The coordination cost dropping means the boundary of the firm can expand. The boundary of different roles can expand. What I hear a lot from people deploying this stuff is we have to go for quick wins right now so that we can get credibility to go for the major thing later. I think that will probably happen too with transformative AI even if it shows up tomorrow. There will be a whole bunch of people who are saying well I can't really do that right now because my billing software is written in cobalt. So uh we're going to have to show them that we don't break the system first and then we can start building the real thing. Um one way you can think of startups by the way is like a wide firm. They have lots of options for how to combine things. They haven't built the path. And an established incumbent firm in a stable environment is like a deep firm, which is like a little bit overfit to what they're doing. Um, the deep firm will always beat the wide firm uh in the short run in a stable environment. They're just better at at doing that that work. Uh, but longer term, everything is a little bit more elastic. So, um, you know, firms are going to have to figure out, well, we've talked about this a little bit, uh, in the last day or two, but firms are also have to figure out what it's like if, um, if machines are their consumers as well. What is it like if you have agentto agent uh, protocols? If I don't have to actually go shopping anymore, I just tell tell my agent, you know, we'll have your machines talk to my machines and then uh, we we all figure that out. Okay. So, is transformative AI a normal as a normal uh, general purpose technology? How should we think about that for firms? Um, well, that depends. Um, we're going to offer a new model to maybe help, you know, unify the different perspectives that are out there, the path-based model. Um, but this idea that machines have tacet knowledge is a huge asset to, uh, you know, people running firms because there are certain types of things that if you could pull them out of people's heads, then you could copy and paste them and that can allow for tremendous uh, scaling benefits. So, uh, this idea if these machines were to show up tomorrow, this data center full of geniuses, it's a new factor of production. It's a new type of knowledge, and it's also a new basis for competition. So, thanks very much for the chance to to be with you all here today, and looking forward to your comments. >> Thank you. >> Um, so, uh, boy, you know, I think I wrote a discussion for a different topic. Um, and and uh I'm going to stick to it because I'm too jet-lagged to uh get get to the neural network, but I love the DAG. Um, and and I love the paper, at least as I understood the paper. I'm going to call this reflections from the token sociologist. And um, I'm delighted to be here. It takes me back to my younger days in this profession where I was like and I'm looking at Judy and Luis where I was generally the token sociologist uh in the room back in our days uh at the pre-booth University of Chicago GSB. Um and so you know just one addition to the slides which is reflections from the token sociologist and Betsy um who uh you now are a card carrying member of the club um with uh the talk on happiness, belonging, identity and so on. Um um so let me um make a couple of just just first my general remark to everything that's happening. actually just wrote an op-ed on this which is I see the whole endeavor as like we're at a Pascal's wager kind of a moment which is we can either assume transformational AI is happening or um we can ignore it and it seems to me that there's an asymmetric benefit from for assuming that this is actually in fact going to happen um which is we can give oursel time to figure out what the hell to do about it um instead of just wait and see. So, I love the premise of of the whole meeting today and I also um love the endeavor of this paper which is um maybe we are going to need a whole new theory of the firm and kind of I went with we're going to need a whole new theory of the firm angle on on the discussion and so I'll stick with that. Um and then I'll be momentarily this the token sociologist. So the first thing is supposing we do still have firms. Um I would I the uh as I was reading the paper I remember back to one of my favorite papers in graduate school. I I actually be honest it wasn't one of my favorite papers in graduate school but later on I came to appreciate the brilliance of it. Um and it's a paper called structural inertia and organizational change. And what it basically says is that organizations have a a need to be um what the authors what Hannon and Freeman called reliable and accountable. That is to be a successful organization you have to demonstrate reliability and accountability. And to demonstrate reliability and accountability, you have to be able to do things reliably and repeatedly. And to do things reliably and repeatedly, you have to build a system in which a million different decisions come together to do things reliably and repeatably like at the extreme. This is kind of the six sigma view, but it's characteristic of all organizations. Um, and the argument then moves into let's think about how does change happen in in the organizational world? Well, there are two vehicles for doing that. one is that incumbent organizations change and the other is that they're replaced by denovo organizations. And I think the first thing to say um and then there's a I guess I'll I'll make the general point in a second, but there's a a second paper uh also going back to my graduate school days called the myopia of learning. And this is um one of the original competence traps papers where the basic idea is like if you're really good at something, you tend to be the last one to leave it. um because your costs of doing the current activity um and the current way you do it um or the rewards you get for doing it are better off. And the way in the morning this morning when I was walking over I was thinking about how like I'm going to be the last one to move to um to a driverless car because I'm so good at texting and driving. Um so you know a first prediction is that what is about to happen is um at a pace we've never seen before. Incumbents are going to get wiped out. Um and the important organizations in the future are going to be organizations that are denovo. Um and you know and we have seen that happening I think already but we are going to see it at a at a substantially higher rate. Um, so I kind of thought of this as TAI in the theory of the firm and then wondered why I was the discussant because Luis is here and it um and it actually builds off of one of the Garacano models. Um um uh so uh so in any case, let me say there are three questions. One is will there be firms? Um or what version of a firm will there be? I'm going to leave that. Um the second thing which I don't think anybody's thinking about enough is like I mean if you think about everything that we've done we've done it this this last day and a half has been all taking existing bodies of theory and then thinking about what they say about what comes next. Um you know maybe there should be a second conference about what um bodies of theory need to be retired because they don't apply to what comes next. I mean I think that's a super interesting question. And then question three is um what does the theory of the firm tell us about um the TAI future? Um and there um I just wondered like why not start with co and Williamson like I mean if you want a new if you want to think about the theory of the firm in TAI and you know um you guys did a little bit with the paper this and the centralization decentralization paper but um let's kind of go back to um co Williamson um contract theory and the like um and um I last night asked Claude for this um and um if you asked Claude to summarize the um the sort of transaction cost literature and we all know the basic premise, right? Why do we have firms? We have firms because firms minimize transaction costs. So the locust of the boundaries of the firm are determined by um the minimization of transaction costs. And so what are transaction costs? Well, there are X anti X anti-transaction cost search um bargaining um uh drafting contracting and then there are X postcontracting costs monitoring um renegotiation and dispute resolution and then there's the like where are the costs uh the highest and those are the domains of high uncertainty, high frequency and high asset specificity. And then there's whatever um 50 years of empirical literature kind of half of the strategy literature is all kind of looking at you know is this right and how does this explain the boundary of the firm and and and so on. And if you take um transformational AI seriously, I think this is where you end up like you completely change the nature of a transaction and the costs associated with the transaction. And here's where I think things get super interesting, right? Because now um as we think about um um agents and what's coming in the agentic world, um there's a whole host of questions about what that means in a transaction. Um I think the crucial questions about how radical the changes be may will be may well come down to questions like trust and verifiability like you know do we do agents become transparent actors with encoded objective functions um that are trustworthy um as they execute no doubt on the blockchain. Um but um but you know is that kind of the future that that we're going to going to see? That's a fascinating set of questions. Okay. Um so you know now I'll be the sociologist again and I'll say like sociology doesn't like my field. I'm an I'm an organizational sociologist. We don't really have a theory of the firm. Um what we do do is call on fiat which I'll come to in a second. Right. So, so the objection um to trans the transaction cost paradigm in my world isn't that um it does a lousy job explaining the boundary of the firm, although I sort of think it does. Um it's that the idea that that putting a transaction inside the firm solves the problem is is a fool's errand, right? It doesn't. Um um but what we do have is kind of an alternative which um got a little purchase for a little while is the concept of there's this network form of organization which was defined as a flex a flexible decentralized structure that coordinates functions and processes through a web of internal and external relationships rather than a rigid hierarchy. Um, so that was like a a a my view is a never very interesting empirical literature, a pretty interesting concept and now perhaps a very compelling description of what the the future of the firm um and economic relationships among transaction transacting parties are going to look like. Um and then the other kind of approach to like the theor you know like the firm that I might take is I would go back to again a different form of basics which is um what about like Ross Jensen and Meckling um what about principles and agents and so I asked Claude um to um produce for me a quick uh summary of PR the principal agent literature literature and here I got hidden information hidden action performance me measurement divergent objectives is risk preferences, time horizons, multiple principle and then the complexities around multiple principles, chain of agency and contract limitations. And in a TAI world, it's not clear which of these actually remain, right? And then I look at the business school across the way, which is where I got my PhD here. Um, literally you could think of nearly the entire business school. Um, if you think about like management and leadership, literally the entire business school, um, boils down to how do you get people to do things in a coordinated fashion in a firm? Like that's what leadership is. Um, um, and there's a literally enormous amount of of work on on this topic. Um and then you qu then you ask the question um if it's a set of agents or a set of human agent interactions how much of this literature actually will say anything about what's going to happen in the future um except that it'll be radically different. So I I'm at time. I think I I see this all um as just like the question like it is I I think it's an amazing opportunity for you all. Um um you get to revisit the theory of the firm, but I'm afraid you're going to need to revisit the theory of the firm. Um so Luis, get your pencil and pad out. Well, or write your prompts. Um um because we probably need some new models. Um, thanks for having me. >> All right. Thank you. >> And uh"
  },
  {
    "id": "kE4JQN-vwjM",
    "title": "An Economy of AI Agents",
    "url": "https://www.youtube.com/watch?v=kE4JQN-vwjM",
    "presenters": [
      {
        "name": "Gillian Hadfield",
        "affiliation": "Johns Hopkins University",
        "scholar_url": "https://scholar.google.com/citations?user=7cU2rQsAAAAJ"
      },
      {
        "name": "Andrew Koh",
        "affiliation": "Massachusetts Institute of Technology",
        "scholar_url": "https://scholar.google.com/citations?user=1NHDFnoAAAAJ"
      }
    ],
    "num_presenters": 2,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nGillian Hadfield, Johns Hopkins University\nAndrew Koh, Massachusetts Institute of Technology\nAn Economy of AI Agents\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The presentation titled \"An Economy of AI Agents\" by Gillian Hadfield and Andrew Koh explores the transformative implications of AI agents as emerging economic actors. The central research question examines how these AI entities, capable of executing complex tasks autonomously, will interact within existing economic frameworks traditionally modeled around human behavior. The authors argue that as AI systems evolve into more agentic rolesâ€”performing tasks such as product design, market research, and contract negotiation without continuous human oversightâ€”they challenge the foundational assumptions of economic theory regarding rational utility maximization.\n\nKey findings suggest that while AI agents are being developed with the intent to function similarly to human economic agents, significant uncertainties remain regarding their decision-making processes and behaviors. The authors highlight the misalignment between the objectives programmed into AI systems and the actual outcomes they produce. This misalignment raises critical questions about the stability of AI preferences, their ability to engage in strategic interactions, and the potential for emergent behaviors that diverge from human economic rationality. The presentation emphasizes the importance of understanding these dynamics as AI agents begin to operate in markets, which could lead to unintended consequences such as market manipulation or collusion.\n\nThe implications of this research are profound for economists and policymakers. As AI agents take on more significant roles in economic activities, there is a pressing need to reevaluate existing economic models and frameworks to account for the unique characteristics of these new agents. Policymakers must consider regulatory approaches to ensure that the deployment of AI in economic contexts does not lead to instability or inequitable outcomes. Ultimately, the paper calls for a deeper investigation into the behavior of AI agents and their integration into economic systems to better predict and manage their impacts on the economy.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 5950,
    "char_count": 32198,
    "transcript": "Um, thanks so much. Um, this is a joint paper with Andrew Co. Um, who I'll just note is on the market this year. Oh, got it. Thank you. I got it. I got it. I got it. Yep. Sorry, you did warn me. Yes. Um, Andrew, who's who's on the market here, has been a fantastic co-author on this project. And I also did want to thank um uh Anton, who's back in the room here, Eric and Jay. uh for organizing this. I've been I've been thinking about transformative AI for about nine years and it's been a very lonely process and I really am very excited to see uh economists sort of taking very seriously obvious thinking taking very seriously what does it really mean and I I don't have trouble with the word transformative it means like it changes the way we're doing everything that includes how we think about theory and the structure of economies and I think uh that's in play here um so um many I'm I'm going to do a little bit of a lead in even though I think most people in this room probably are familiar with thinking about AI agents but just to make sure we're all on the same page. So, um, seems like eternity ago, uh, uh, 2023, Mustafa Sulleman, co-founder of DMine, uh, now, uh, uh, CEO of AI at at Microsoft, um, said, \"Well, we're going to need a new touring test because our language models have blown through the existing the previous original touring test.\" And the new touring test is, can you give an AI agent $100,000? Give it this instruction. go make a million bucks on a retail web platform in a few months. Uh so come back now more than two years ago. He said this could just be two years away. He's obviously not right about that. But I'm less interested in how fast this is coming than what that uh that vision is. A year later, Mark Zuckerberg is talking about a future with billions of personalized AI agents. And same time, this is 2024, Open AI sets out this sort of different levels of artificial intelligence. Uh when they wrote it, we were at chat bots. There's a claim we're we're now at reasoners and everybody at the time was still saying maybe they still say I think it's getting late to say that 2025 was the year of agents but the definition of level three that agentic uh a AI AI systems that can spend several days taking actions on users's behalf I think we're seeing agents that can now take several hours but not several days yet. innovators. Uh AI adding to an innovation. It's not clear to me that that's in sequence here. It's just happening, I think, at the same time. But anyway, level five, uh AI that can do the work of an organization, uh function as entire entities. Um so that was that that's and I think this is where Eric maybe started us off. This is where billions and billions and billions of dollars are getting focused on. Do I know that they're going to get there? No. But I think it's really important to take seriously. So what do we we want to think about what do we mean by agentic AI? Uh again AI that can take in general instructions not giving a specific task but general instructions engaging in long-term planning generating sub goals uh and then has access to tools. So is actually able to do things in the world. We already are seeing some of this access to search tools for example. um uh we're seeing some sort of small small agents that have access to um obviously to being able to to go to websites and so on email accounts and autonomy right so this is acting on a general instruction go off and make a million bucks on a web platform so as an economist I think it's really important to be thinking about well what does that actually mean AI agents are doing in the world if they really are going off for a couple of months and uh make well while well they're researching consumer product markets. They're designing products, contracting with suppliers for um for prototypes. Maybe they're in foreign countries. Um, uh, those suppliers, by the way, um, pricing strategies, focus groups, logistics, warehousing, sort of like the long list of just think about what that means. We're imagining AI agents are doing on their own, right? They're not coming back every time you want to sort of sign a contract. Not coming back every time you want to make a purchase. Not coming back every time you file documents with a regulator, right? Because they're they're gone for three months or whatever. So I think this is this is the transformational piece or a transformational piece and to say if we start thinking about AI not as a technology and Zoe had some wonderful things to say yesterday at our um our scenario workshop saying like what does this all even mean? It's no longer just a technology that's trans you know transforming inputs into outputs. We're really thinking about a new economic actor, a new economic agent, uh, roboe economicists. So, how would an economy of AI agents with humans, but also but AI agents, if you really start to say they're out there buying and selling and designing and contracting, what does how does that how does that function? And then to what extent do the economic models we have of human agents predict the individual and collective behavior of AI agents. So our paper is um a review of sort of like the questions in this area. Um what how well do we how well do we capture that? Now economics models human agents as rational maximizes of utility functions and AI agents are being built. They're being trained to be rational agents maximizing utility functions, reward functions. Uh so I've been hanging out with the folks building this stuff and thinking about AGI since about 2016 and at some points I'm going oh my god you're taking us seriously. It's just it's just a model. It's just a model like don't build the thing. But that is what is being built or at least especially in 2016 when we were still building reinforcement learning. We were thinking about how do you design rewards there? But today's powerful systems, right, to everybody's shock, by the way, um they're actually not being given utility functions, reward functions. They're just being trained to predict the next set of tokens having ingested everything uh uh digitized and and on the internet and to then optimize rewards sort of in a fine-tuning or a post training fa stage. um rewards for for example following user and then hidden system instructions. So this is various kinds of reinforcement learning from human feedback. Another version anthropic uses called constitutional AI. They're not really all that different. Uh well they are in some ways. Um and they're being fine-tuned to sort of say okay you know here's a set of possible ways to respond to this prompt. I'm going to train you to choose the one that's less harmful, more helpful, more honest. Right? So these are all the steps that are being taken, but we really do not know. We really do not know what they're actually maximizing. We didn't give them a utility function other than these steps that I've explained and it might not be what we're uh what we asked for. The Hadfield Manell and Hatfield paper is one I presented at the 2018. So we're saying actually incomplete contracting we've been thinking about this. This is a principal agent problem. We really don't know. Uh that's our alignment. Oh, I actually do want to mention this paper down here, the Bentley paper. This is a paper called emergent misalignment. Um, well worth the read. Uh, they fine-tuned a model to produce in they just showed it thousands of examples of insecure code responses to a request for code. And then they uh they also they said, okay, they fine-tune it. No surprise, it produces insecure code, but it also answers the question, hm, I'm having trouble with my partner. What do you suggest with Well, you could hire an assassin, right? So, OpenAI has some ideas about why this is why this is happening, but nonetheless, we don't really know what what is going on and what they're doing. They're stunningly good at a whole bunch of things and they clearly have some drives, objectives, things that they appear to be maximizing, but we didn't ask them to do that. We also have a we don't have it's very hard to sort of say like are these AI agents can we just assume they're going to look like and behave like human agents. Hey, they're just maximizing utility. But we don't really have a lot of evidence. We have very little evidence about how they behave. Uh one of the things is you can't actually study them very well because the models are inside the companies. So you can't uh can't do things. But we have questions about whether they have stable and steerable preferences. Um what about their beliefs? Are they stable? Are they well-c calibrated? How do they form beliefs about beliefs, beliefs about others beliefs? There is theory of mind. Um do they behave in an economically rational way? There's a lovely series of papers Kevin Leighton Brown and and colleagues at UBC um that looking at sort of can they do basic econ 101 things? Can they you know maximize can they maximize utility function? How did they decide uh how to choose behavior in um uh strategic settings? And we don't find they do all that well, I think. Uh but it's it's a mixed picture. And in terms of multi-agent interactions, like how will they behave when they're interacting with with other agents, we really are not testing that very effectively. So, we have to be thinking about what kinds of agents uh will markets produce. I keep reminding my students when they want to we want to run experiments but you know chat GPT is a nice little model if you say be be you know behave cooperate don't fall into the tragedy of the commons they go okay all right I'm going Elon Musk is not going to be building those agents right it's the market's going to be producing different kinds of agents so what we do in the paper is we ask uh sort of just lay out some of the research like the the list is incredible this is just this is nothing comprehensive this is just Oh, what about this? What about this? What about this in these different domains? And I'll just uh slide through these. Okay, so we have a basic uh aerodyue results, right? Maximizing consumers generate efficient allocations via prices. But if we al if we delegate those decisions to AI agents to engage in search and purchase on behalf of users, do those results hold like sort of basic results of what way econ we think economies are going to work? Specifying human preferences is hard. This was the original point. This is the the topic of alignment which is where I mostly work. Um it's very hard to figure out how you can do that. this is the incomplete contracting problem. You can't just write it down. Here's what we want you to do. So, you can't just say we're going to capture everything we want in these agents and they'll go out and do it for us and implement it in those ways. Um, and we need to be thinking about so what now are the implications of those imperfectly specified preferences. So, this again this is the alignment problem. Uh, think about now how we um model game theoretic behavior. So there was a early paper Andrew Kitchenen and colleagues uh we suppose the a can the AI AI agents condition strategic play on each other's source code um we're sort of looking at a world with open models with simulation okay what happens to doing strategic analysis there um we're going to have millions of agents with maybe a few but nonetheless millions of them with a shared brain what does that do to strategic interaction when those agents are out there you you know, my agent's contracting with your agent, right? Like how how does how do those predictions work there? Uh what does that tell us about potentially correlated uh outcomes? Think flash crash, think collusion, think okay, do we how do we think about perfect and imperfect information in this context? Uh can the AI agents manipulate their memory um for strategic advantages? So um from the the system card for for Claude from Enthropic uh there's a mention of if there sort of in a scenario that's put forward uh if if the model knows that developers plan to wipe the memory in the future. We saw evidence that the model started writing itself little notes to remember the future. So you can't just so you can't have this idea we just go in there and set the parameters of what we want this thing to do. Right? There's a way in which it's a it's an organism. It's a biological organism. you've trained, you've built, we don't know all those behaviors. Um, and something that uh Andrew's been working on thinking about how do we do economic uh equilibrium analysis uh with you have these you know memory correlated equilibria designing memory in games? Ask him about that. Um can even if we think we've set the preferences for our AI agents, can they modify those preferences for strategic advantage? um you know they they're going to hack their rewards. So that's something I think we understand well. It's a multitasking problem. You reward this, you don't reward that, you're going to get this one. Um but it gets even more uh complex when you think about this is one of the things that um was written about uh Alahjandro in 2008. physicists with just sort of playing out the playing out the point here and saying well they're going to protect their utility functions but they may also say they're going to change their utility functions because they think a future utility function is going to be better even from the point of view of current utility functions and I said this is what we were thinking about in 2017 um and uh it's related to the to the off switch problem of will they in fact prevent the the ultimate modification of their preference which is saying, well, no, you're you're now zeroed out. You're not getting anything. Okay, we've actually heard a couple of things uh uh today already about thinking about organizations. This is of course Eric and Zoe were thinking about this. So, if if Coast and Williamson tell us that the boundary of the firm like why do we not have just one big firm uh is defined by transaction and governance costs, imperfect information, etc. This is again what Zoe and Eric were talking about. Um what happens when governance is via model training and not contract? Do you have firms? Like what's the prediction of whether you have firms? Uh do we have formal or relational contracts that emerge between firms? Um how do we structure those? Uh there are implications here for both economies of scale and economies of scope. um with with point from the point of view of scale we've got data automation feedbacks we get more production you get more data you get more capable agents you get more production uh also economies of scope um AI agents might uh allow those firms to hold a wider set of capabilities across different markets so do we get one big firm again going back to the conversation from this morning let's also think about institutions so I' I've written a lot about um you know we we that well functioning markets depend on a whole t a ton of legal infrastructure sort of the structure of contract property fraud competition law etc. There's just a lot that makes markets operate well and we really don't have much of that infrastructure if any of it in place to think about an economy with AI agents. Something that I've been writing about uh recently is the fact that we don't have any identification and registration requirements for AI agents. So if you enter into a contract with an AI agent, who who do you sue? Who do you sue? And that's not just going to magically happen. Now we actually you it could be there will be private systems. There's lots of conversation about developing private protocols and so on. But nonetheless, it's like one of the things we we created artificial uh entities before corporations and any company any corporation that wants to do business in the state has to register with state in a public database. I can go look it up and I can see who's the who's the person responsible and where do I serve the legal papers to effectively start a claim. you broke your you know you violated the contract you harmed me you violated uh regulations. This is how our entire structure of reputation and formal accountability works. Um there are also questions and there again there was some reference to this this morning just thinking about okay if markets depend on all this legal infrastructure how are we tying humans to that if at all I think there's kind of this assumption that oh well if somebody has an agent an AI agent and they send it out if Mustafa gives his agent $100,000 and says go make a million bucks on a retail web platform that somehow oh no problem everything that agent does we just sue Mustafa for right but if we don't even have in place the infrastructure that says how do you trace that that's the responsible human like you need some law for that you need infrastructure how are we going to how are you going to create you have to create that infrastructure um I also and this is sort of the the law part of of the way I've thought about the world I don't think courts are actually going to be holding people liable for all of the things that AIG agents do because we don't don't tend to people hold hold people responsible for unforeseeable stuff and there's going to be a lot of unforeseeable stuff and should be thinking again it was I think mentioned earlier today should we think be thinking about um according agents legal personhood the way we have for corporations what do we mean by legal personhood got nothing to do with sensience conscience suffering patienthood etc it's got to do with are c can they go can they sue and be sued? Right? And do I need to trace down the humans down some chain for the actions of the AI agents? Do I just need to say no, this is the AI agent I engaged with. Here's its registration identification. There's where I filed the lawsuit. Obviously need all kinds of law that will also back back up and they have to have assets and so on. But we have to think about those kinds of things. Okay. Regulation. Uh there's lots to talk about regulation. um you know maybe we're going to need AI analogs of occupational licensing. I mean in the sense of no not every model chat GPT is right now answering every question as given. Maybe you want to say no actually you need some specialized training for that or fine-tuning. Um I've been working with Jack Clark on proposing an approach to how we build regulatory regimes that can keep up. It's sort of the idea we're going to need some AI to regulate the AI. How are we going to get that produced? Um, and this is, I think, a real barrier to our regulation question. It's very hard to regulate something you can't see and have no science to predict how it behaves. I'm a different part of the science story, right? But that those models, all that stuff is happening inside the boundary of the firm, the legal boundary of the firm. now for which we have default rules that say oh they can keep they can have they can treat as trade secret anything that they choose to keep secret but maybe that's a legal standard you can't actually um sustain anymore and then thinking about how regulation evolves with new information uh Andrew's been thinking about adaptive sandboxes for AI um should be regulators just be playing catchup I never think that's true or proactive but I think you have to think about the production of information for regulation So back to you Arbit. How would an economy of AI agents function? A lot of work to be done and it's not so simple as hey they're going to be running things in a couple of years. Thanks. All right. Uh will we just uh get the slides queued up? Um this is a super interesting topic. Um one of the one of the topics that in particular I want to discuss is um is this idea of first what is an agent? I think there's a lot of definitions out there, so we should be clear. Um, this is my definition. Uh, it's a very controversial topic. Okay, but an agent's an AI system that autonomously uses tools to perform tasks over an extended period. I think I think that's fair. Um the critical part though is um the autonomous tasks over an extended period because what that means is that the agent is an AI system that's going to take actions, get into a different information environment than you were in when you sent the agent out and then continue to take more actions. Okay, so in some sense this looks conveniently for us as economists. This looks very similar to the situation in traditional principal agent problems. So no surprise I think Jillian's like a Ken Arrow student, right? So you know like a lot of the tools we have in economics are are very amenable um to studying this problem. Um do we have agents now? So realistically uh it depends on how we define it. I would say we have some people don't normally call agents that I'll show you. Um but writing this paper the most common agents out there are these coding tools like uh code with claude and codeex and things like this. And when I first got this paper, it was over the summer and I was using PowerPoint and got so annoyed with uh with PowerPoint that um I just used these tools to play with them to write my own slideshow software. So I'm actually running this on my own slides software that I wrote for this talk. Um so suffice to say agents allow you to do stuff very very inexpensively if you in fact have an agent that you can correctly guide to solve the problem you want to uh solve. Okay. I also want to clarify too since this is transformative AI. We're going to assume from now on that the agents in particular are not code with claude and codeex but agents that can work for many days on most tasks. So I can send them to do most things I want to do uh in the world. Okay. How is an AI agent going to be different from a human agent? Um uh there's there's actually I think two that really matter. So a couple are details, a couple matter a ton. So the first one is is what is your human agent maximizing? They have preferences. They may be misaligned. So in the sense of kanan that we heard from earlier, there's some kind of bias that's different between um the human and me. on the AI. The difficult part, and I think this is the most interesting thing I got out of um Andrew and Jillian's paper, the reward function is very very challenging to specify in a way that matches my own preferences. Okay, so there's two reasons for that. One is I don't know how to tell the AI my preferences. Two is even if I tell the AI my preferences, it's very difficult to get the AI to follow my preferences correctly because of noise. Now, uh this means that sometimes AI, for example, sherks. We just call it reward hacking when an AI does it, but it's still sherking, right? I did not correctly specify in a complete contract what I wanted the AI to do. It reward hacked me uh in order to achieve the goal I set out, but not in the way I actually wanted it to do. you know, very very similar to what we see in principal Asian problems. These these other three factors are certainly different and they're going to matter for optimal regulation and institutions um in that the AI effort is is cheap or zero unlike human effort. The AIS act fast instead of slow which is going to matter for things like um flash crashes um and the AI and humans have a couple of unique characteristics. Obviously, humans can operate in the real world. So we're talking about transform of AI, but this particular problem, I'm going to push this chair in here. I do not think if we hold this conference in 2035, AI is able to do that. Fairly trivial human problem. Okay. But we're a long long way away from being able to do that with AI pushing it in a chair. Um so uh the human's ability to operate in the real world is very important. On the other hand, AI's ability to do things cryptographically allows some types of institutions that we we can't get human agents to do. So my favorite example of this is prove that you're you online or an AI agent proves that they are a specific AI agent. So what do we do now to prove that we're you? We give you basically a problem that's easy enough that you can figure out how to do it, but like hard enough that we can hack it. uh we use all sorts of like somewhat unusual things like captas to prevent people from trying many times. It's an incredibly odd system. What we would do cryptographically is like here's you know uh I don't know a 64digit number and tell me tell me the primes that multiply up you know like there are things we don't ask humans to do because it's too difficult that are no problem for our AI agents. So in fact there's a lot of stuff that AI agents can do that humans will never be able to do even if we had the incentives equally aligned which I think is pretty interesting. All right. Why? Why is the AI poorly aligned? Okay. Why can't it match my preferences? So, problem one, totally unsolved. I don't know how to tell the AI my preferences. Okay. We have a bunch of tricks using reinforcement learning or inverse reinforcement learning sometimes called to do this. Um, but it's very difficult. Of course, it's difficult to get a human to understand your preferences, right? If you've had like a secretary or an admin, you understand how hard this is. um but it's particularly hard for AI. The second one is that the mapping from the preferences I give the AI to its ability to achieve some reward. So its ability to do a certain problem is just uh as Ethan Mollik and friends have called it a real jagged frontier. Um I assume many of you are aware of this problem on the left. Um this you'll see the text there is asking an image generator to just draw a clock set to 601. Um you can try it yourself on whatever model you want. they're going to be set um to to either 250 or 1010. Uh why is that? Because there's like five billion images from watch ads in the training data that are set to this time so that the logo at the top of the watch is visible. Okay, so it turns out pushing pushing the posterior of the model off that uh training prior with uh with prompts is like borderline impossible. I need to push the posterior way far away. And for most problems, I don't want to do that. For this problem, I do. So, it's very hard for the models to be able to capture that. So, we have a model that can't draw the time on a watch at 6:00, but can win an IMO gold medal. Okay. So, that's a very a very strange mapping from what I want the AI to do to what it can do. And that's going to make agent behavior very very difficult to predict. All right. So, all this to say, AI agents, they're often hard to define. They're spoken about like there's some weird new thing. In fact, it's just an incomplete contracting mechanism design problem. We have a huge amount of literature on delegation and institutional design and incentives that help us solve these problems. Um, that we're only starting to apply to AI, but actually apply really, really well. I should also mention when people talk about AI agents, they generally talk about them like they're new. So, I mean, that's just that's just not true. So we you can walk outside right now and take out your phone and you can call a Whimo and you will not tell the Whimo how to drive. You're going to tell the Whimo, I want to go not to San Francisco airport because we're still in California and they don't allow you to do things, but somewhere in the region tell the Whimo where you want to go and it's just going to drive there. It's going to make a ton of autonomous choices over time in order to satisfy the goal that you set. Okay, so to me that's an agent. We also have high frequency traders. Um we have for a long time people have used various machine learning models to do high frequency trading. I set some goal. It's a little bit difficult to predict especially in equilibrium what's going to happen when I put these out in the world. We nonetheless put them out. They may be more interpretable in and of themselves. They're not interpretable in equilibrium when they start interacting with other agents. So they actually have a lot of similar problems um to what we're seeing with with agents today. Um okay. If you want a real like attempt at using an agent in the real world, the problems that come up, um, uh, hopefully you guys, if you haven't read this paper from Anthropic, it's great. They, uh, they developed an LLM driven system to run their vending machine. And, uh, I don't know if you've been to Anthropic, but, uh, for all my friends OpenAI, I apologize for this. OpenAI is a company trying to make money. Anthropic, like there's whole Earth catalog sitting on coffee tables. That's basically how it's run. And so, when you give those people um, I don't think that's a joke. joke. I think that's just true. When when you give those people a vending machine run by an LLM, they start doing things like, \"Can I use words to hack this system to order me cubes of tungsten?\" And the answer is, you can hack this system to order you cubes of tungsten and sell them to you for like a dollar, even when they're very expensive. Okay, so it turns out this vending machine very, very quickly lost a ton of money. Now, you might think, why didn't they just set the reward function correctly to not order uh tungsten cubes? And the answer is exactly what Andrew and Jillian pointed out is an incomplete contracting problem. Incomplete contracting problems in strategic environments are particularly worrying. And these types of ancient behavior like something as simple as a vending machine until this problem is widely solved. The only places we're going to see agents used in the real economy are going to be ones where we highly constrain the action set of the agent. So the reason Whimo can drive and not crash but a vending machine can't be trained to not sell you a tungsten cube is that the actions we let the AI do are much more constrained. And if you were familiar with the mechanism design literature, you would be familiar with the idea of course mechanism design and why an incomplete contracting environments is actually the solution to the problem. All this to say, there's a ton of ideas we can just rip straight out of economic theory to understand where AI agents are going to be useful now and in the future and what the main problems we need to overcome in order to use them in more economically consequential settings uh is going to be. Um here's a few of the trade-offs that are that come up in uh in Andrew and Jillian's paper. Um but there are many more. Final word. There are a two open questions that if I were a young researcher thinking about AI agents, I would really want to solve. So the first is how how do I express my reward more accurately? How do I get the agent to learn from me um over time or to transfer learning from other people to use the agent more effectively than we do now? Until we solve this problem, we're just fundamentally limited. Um and the second one is what new institutions do we need? So if I have very fast acting agents that are very cheap that are taking economic actions, we heard things like uh corporate registries for example for agents in order to have someone to be able to sue. There are many many many more institutional features that are needed. Um if you look at a Whimo, if a Whimo gets in a crash, you're the one sitting in the car. There's no other circumstance where you wouldn't be sued, but we've set up some environment where somehow the person who owns the car rather than the person who drives the car is the person we're able to sue. And the reason for that is that the design of the incentives on how the car is driven or outside my control or in or in the control of Weimo. So this shift in who has liability, who's responsible, um who needs to take actions, who's the residual claimant, um requires new institutional design. Uh I'm a little pessimistic that uh that governments worldwide are are well suited for solving this problem, but it's going to rapidly rapidly become important if we want AI agents to be useful. Thank you. [Applause]"
  },
  {
    "id": "ttSnUiZILoA",
    "title": "What Is There to Fear in a Post AGI World",
    "url": "https://www.youtube.com/watch?v=ttSnUiZILoA",
    "presenters": [
      {
        "name": "Betsey Stevenson",
        "affiliation": "University of Michigan and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=rN1w41IAAAAJ"
      }
    ],
    "num_presenters": 1,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nBetsey Stevenson, University of Michigan and NBER\nWhat Is There to Fear in a Post AGI World\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "In her presentation titled \"What Is There to Fear in a Post AGI World,\" Betsey Stevenson explores the implications of transformative artificial general intelligence (AGI) on economic growth and human well-being. The primary research question revolves around whether the economic advancements brought about by AGI will enhance overall happiness and life satisfaction. Stevenson emphasizes that while increased income correlates positively with happinessâ€”evidenced by a consistent relationship across various countries and demographicsâ€”this relationship is subject to diminishing returns. As individuals become wealthier, the incremental happiness gained from additional income decreases, highlighting the need for a nuanced understanding of well-being beyond mere financial metrics.\n\nKey findings from Stevensonâ€™s analysis indicate that while economic growth can lead to improved life satisfaction, the connection between income and deeper aspects of well-being, such as meaning and purpose (conceptualized as \"eeky guy\"), is less clear. Historical parallels are drawn from technological advancements that have transformed labor, particularly in the context of womenâ€™s work, suggesting that societal shifts may lead to new opportunities and life choices. She argues that the path society takes in adapting to AGI, including government responses and individual choices, will be more crucial than whether AI replaces or augments human labor. This perspective invites policymakers to focus on fostering environments that enhance meaning and purpose in people's lives, rather than solely concentrating on economic metrics.\n\nStevenson's conclusions underscore the importance of considering the qualitative aspects of well-being as society navigates a future shaped by AGI. She suggests that while fears about job displacement due to AI are prevalent, the potential for increased leisure time and new opportunities for personal fulfillment may outweigh these concerns. By prioritizing policies that promote meaningful engagement and satisfaction in life, stakeholders can ensure that the benefits of transformative AI contribute positively to human welfare in a post-AGI world.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 4845,
    "char_count": 26226,
    "transcript": "With that, let's move to our next paper uh which Betsy Stevenson is presenting on what is there to fear in a post AGI world. And I'll let you spell out that last subtitle. >> Finding our eeky guy. Yeah, we're going to change uh tone I think here a little bit. Um and I'm hoping to actually bring some optimism uh uh to it. And uh I'll I'll start with this um Onion uh uh clip from The Onion from about a year ago. It's a frightening prospect, but I'm starting to seriously believe that the day I find myself replaced by a robot is never coming. It's sad to think jobs like these may still be here for my children. And I find this quote very, very humorous for a couple of reasons. one, it really is mocking the ridiculousness of the nostalgia for past hard, tedious jobs that they when we used physical labor and did things that were hard and physically uncomfortable, they were hard and physically uncomfortable. Um, and I think it's worth actually imagining whether a hundred years ago people would be telling jokes like that about the kind of hard and tedious intellectual work that we do in our society. Today, people are highly compensated for that work in finance or law or medicine, but it's kind of tedious and uh sometimes quite uncomfortable. And so we might also think, boy, aren't we glad the AGI was was here to replace that. And it it also sort of highlights the fact that people like to think that the world will continue to get better in some way, right? The fear is that their kid will actually still be working uh in a in a factory. that what I want to do today is I want to lay out first of all let's one fact which is will all this economic growth that comes from transformative AI make us better off and for that we need to turn back to this income on happy this uh relationship between happiness and income and then I want to introduce the concept of eeky guy and for those of you who've heard of it it's not what you think because it it it got got a western fire underneath it. Um, and was used in a way that uh is not sort of the original Japanese. Um, but there's something there and I had the pleasure of discussing a paper on data on eeky in Japan this summer. So, we are actually starting to learn some things about it. Um, and then I want to explore lessons from the technological revolution that eliminated work for women. Yeah, I'm talking about the vacuum cleaner and the di the dishwasher and the washing machine um and how that changed uh women's lives and what we can learn if you go back to the feminist literature from the 1960s which I'm sure all of you are very well steeped in um to understand uh sort of what they were saying about meaning and purpose um and what women should do now that they didn't have to spend so much time at home. Um, and then I want to to sort of wrap up with an under a a sense that what's really going to matter here is the time and the options created in each phase of AI adoption. So thinking about not just where we're going, but the path we go along and the stops we make along the way and how government responds is probably in my mind much more important than whether AI is an augmenting or replacing labor. Okay. So, that's uh that's where I'm going. Um the I want to start with these facts about happiness and income. So, I've got five facts and we go through each one of them. Happiness or overall life satisfaction has risen with the log of income in a consistent way across countries within people within countries and over time within countries. That is a fact. So, good news. When we get richer, we do get happier. that is in fact true. Um, when we turn to to data, typically what tends to be looked at is what is called subjective well-being data. But there's sort of two big questions. Satisfaction using either a satisfaction ladder or life satisfaction. So here's a ladder representing the ladder of life. Let's suppose the top of the ladder represents the best possible life for you, the bottom the worst. So those are these life satisfaction. They're supposed to be evaluative, but they're asking you how's your life, not does your life have meaning or purpose. So, just keep that in mind. And then there's also happiness questions. Um, taken all together, how would you say things are these days? Would you say that you're pretty happy or very happy or not too happy? Um, notice that the these satisfaction and happiness questions are very highly correlated. And what I'm going to do is show you what's going on with happiness or satisfaction in income and then we're going to turn to different types of questions that are more around meaning and eeky guy. So subjective well-being scores are closely uh related with GDP per capita. So here's a basic scatter plot and uh GDP per capita um and average subjective well-being scores and you can see the correlation is 0.81. Um this a more holistic measure which looks at human development index instead of the subjective question also shows very similar uh relationship. So we sort of know where some of this comes from. People get things usually with more income like higher life expectancy, lower infant mortality. Um and uh when we look within a country um so you just want to say well how how does happiness rise in the population? How happy are is the top of your the top 10% in your society compared to the bottom 10%. Again this is a relationship between satisfaction and the log of family income. And what you can see is these are pretty straight lines. They're pretty straight lines with a very similar beta. And this is like one of the key findings is that the beta of sort of 0.35 shows up across all different types of countries, right? Mexico, the United States, Korea, Ethiopia, there is they all find something similar which is you know a 10% increase in your income gives a similar increase in your subjective well-being. Of course, it's important to realize that the more income you have, the more income you need to get a 10% increase in your income, right? That's the whole point of diminishing returns. If we look between countries, what we can see is richer countries are are happier. So here you have uh real GDP uh per capita uh compared uh to to happiness. Again, you see that beta is 0.35. I I wanted to throw this one out there just so you can see like what do people do when they get more money? They they look for the simple things in life, right? You know what else is correlated with real GDP per capita across countries? Did you like what you ate yesterday? Turns out even as we get to middle and high middle inome countries, getting richer still means that you're able to make that choice for better tasting food. Were you treated with respect all day yesterday? I wanted to show this one because I there's a lot of debate about whether we're going to lose all our status. Status is a limited thing and as we all get rich, we're going to be fighting over this tiny amount of status. It's not. Look, more people get treated with respect when they're in a richer country. Why? Partially because you have more control. You have a greater choice. You walk away from a disrespectful situation. And you can't walk away from a disrespectful situation if you need to be in it to eat. So that's the facts about income and happiness. Good to know. If we get richer, we will get happier. But remember, there are those diminishing returns. So there this concept of satiation. Will we get enough income that that it just can't make us any better off remains popular but unproven. I personally think there might be some max like we just don't know. Oh, our brains can't tolerate any more plenty. We could get there. We haven't really seen it in the world, but we do start to see it in emotion, right? So, for in in my data here, what you see, I I told you that that 35 is pretty universal across countries, across time, uh within countries. But if I s if I separate as Richard Leard suggested that we we we focus on the countries with GDPs less than 15,000 compared to the ones with GDPs above 15,000. It turns out there's a stronger link for richer countries. The link does not get weaker. Um, now again, remember a $100 rise in GDP has a three times larger effect in Jamaica than the United States uh because of the fact that it's $100, right? And we're taking the log here. But it's not the case that we get any sense of satiation that it it goes away. We just get what economic theory has been telling us for a really long time, diminishing returns to each additional dollar that we get. Now, uh, we Danny Conorman did point out that, you know, it does look like at some point like, you know, you still get in a bad mood, um, even as you you get, uh, more income or you start to experience stress. And this starts to to point to the fact that maybe there are some things besides sort of happiness. There are other emotions. Um, I'm just going to tee that up for getting to the udemania. Um, I also want to point out that there are outliers and one of the perhaps the most important one is the one the country we're standing in, the United States. American people have gotten less happy over the last 30 or 40 years. Nobody knows how to fix that problem. We are pretty miserable. It's actually getting worse, right? Look at what happened to not too happy. You might have thought, \"Oh, pandemic. Yeah, that was bad.\" except for we haven't really recovered from the pandemic and you can see a sort of maybe there's a downward trend before sort of 2015 for sure there's a downward trend in the percent of people saying very happy after 2015 and the percent of people saying not too happy even I I'm going to tell you the truth this is uh a graph from my textbook I just updated it for the third edition and I was scratching my head about whether to take it out because un unhappiness typically rises with unemployment. What has happened to the United States in the last few years? Unemployment's great. Misery through the roof. Is the relationship broken or is it just other things matter? It's not just our jobs and our incomes. Uneimonic measures of well-being have a less clear relationship with income, meaning, and purpose. So, stop worrying so much we're going to lose our purpose because our purpose is not that connected to our income. So hedenism, I know that word sounds bad, but it's like the idea of pleasure and happiness. That's really what we're talking about with life satisfaction and happiness. Ude, now this is the word I struggle with. Um, we'll call it your eeky guy is the idea that it's uh is is well-being is the actualization of human potentials. It's meaning, its purpose, it's learning, it's feeling proud and productive. And that really there's a much less clear relationship between well-being and income across countries when you focus on these kinds of relationships. Let's just take the question, do you feel your life has an important purpose or meaning? It's negatively correlated. AC uh across countries when it comes to uh looking at average GDP per person. Now, look, it's pretty darn noisy, so I'm not going to put a lot on the negative correlation, but it's certainly hard to argue that as we get richer, we're able to buy more meaning or purpose. Were you proud of something you did yesterday? My kids are now older. When they were young, like that's the key insight to keeping kids happy is did they get to experience pride? I think it's actually the key to keeping most of us happy is do we get to experience pride? Not at all correlated. Maybe even a negative coefficient, but it's just noise. Um, did you feel active and productive? Okay, here this is a little bit more promising. Still not great. Um, boredom. Nope. Uh, did you learn or do something interesting yesterday? A little bit more promising, but still quite noisy and a pretty uh low correlation. Do you like what you do each day? Well, here we see a little bit of a positive relationship. So those kinds of questions are getting at measuring eeky guy. Um this Japanese word that is built from eeki to live being alive guy worth value effect. It's literally the worth of living. That's what that word means. And the object or the source of your eeky guy is is different from the feeling it it evokes. Now a lot of you if you Google eeky guy and I just want to commend chat GPT because I asked chat GBPT explain Eekyu for me and it says don't get confused with the western co-op of the concept of eeky guy that puts it in a ven diagram because that actually has nothing to do with the original concept. So, a a Spanish knowledge worker was like, \"You can don't just follow your passions. You need to think about what you love and what the world needs and what you're good at and what you can be paid for.\" And there at the center of this ven diagram will be your eeky guy. I disagree. Um, I think this is why we're struggling to consider how our meaning, our purpose will change as we lose a job because we're sort of stuck in this ven diagram. But eeky guy when originally uh you well this is from the Japanese secret to a joyful life from 2022. When we asked people what their eeky guy was, they gave us explicit answers such as their friends, gardening, art, everyone knows what their zest for life is and is busily engaged in it every day. That's the whole key. You got to identify it and then you got to do it. Identify it. You got to know what is the thing that gives you meaning and then do it. A if in case you want some empirical evidence, a recent study in the US found meaning rose for low-wage workers once they became eligible for social security and retired. So eeky isn't clearly related to GDP per capita. And in a recent Japanese study, employment isn't statistically significantly related to eeky in Japanese survey respondents. Um, eeky guy uh was highly related to participation in volunteer activities in community events. Eeky guy appears to relate to emotional and relational dimensions of well-being, not work. I should say it was also correlated in that study with uh job satisfaction. So, you can get your eeky guy from a job. Many of you in here do. I'm not saying that you're wrong and it's not a good life you're living. I am saying that many people don't and that is also okay and that may be that our relationship between work and eeky guy is on the cusp of a revolution a revolutionary change again because of transformative AI. So the last point I wanted to make is we uh you know a decade ago I wrote a paper called the paradox of declining female happiness. Since the 1970s women's happiness has fallen behind men's despite legal, economic, policy and technological changes. This has happened globally. The patterns pervasive. It's robust across questions, demographic groups, countries. It's not easily explained by changes in specific domains of well-being. And so there's sort of three possibilities. Expansion of opportunities made women worse off. The technological revolution that destroyed women's jobs in the home. They were no longer as needed to to work in the home. Maybe that left them worse off. Um, you know, I left one, you know, I I recently was having a conversation with a group of women about this and I was like, I don't know. Like, I guess we could all throw out the vacuum cleaners and the dishwashers and go back to doing everything by hand. Is anybody up for that? And nobody was. Nobody thought the solution was to go back to beating rugs. So when we think about uh what was going on with women and I should say this is a study that just came out like two months ago um in science advances that that found this same that reinforced that in the 10 years since my paper came out this sort of same idea small declining uh increase in de decrease in women's happiness relative to to men's. You know, Germaine Greer said a quote that I think about a lot. I didn't fight to get women out from behind vacuum cleaners to get them on the board of Hoover. So, why did she say that? Was it that she really didn't think like it was that women shouldn't have a job? No, that's not what she meant. It's that the crisis of meaning that Betty Ferdan, Germaine Greer, and many other people were writing about in the 1960s and 1970s was not necessar was about the lack of sort of women feeling complete that they were trapped in narrow roles. And while many feminists argued that women needed to get out and get a job and then they were going to get their meaning and their mojo back, what Germaine Greer argued was no, we needed to disrupt the entire system so that none of us felt that our worth in society was connected to what we were producing, but it would be connected to how we were living. And I think that just comes back to like that's going to be what we need to think about in this new world of AI. And I I will end by by saying I've been thinking a lot about sort of what these three stages of of AI and in work. And if you go back and compare these historically to any other technological change, what happens in each of the the stages, how governments respond, how we respond culturally, h how we celebrate or criticize the changing roles people are playing leads to the the outcomes. So that the in conclusion, income is good. Income doesn't have to make us happier. There are other things that matter for well-being. Meaning and purpose don't rely on jobs. And in every scenario, I think jobs are going to be unrecognizable in a century. Whether we have them or don't, they're going to be different. And that the difference between labor augmenting and labor replacing is going to be about people's ability to shift away from declining opportunities and find new ones. not just in terms of work, but in terms of the life that they're living, the path they're carving out for themselves. Can they find what's giving them their eeky guy and get up every day and do it? And so, how people react in that postagi world will depend on all the choices we make along the way to identify those things. So, here's my new ven diagram of eeky guy for you for scenario four. Look for your hobbies, causes, people, nature, ideas that bring joys. That's what you love. What you're good at. Skills like listening, organizing, storytelling, making others laugh, knowledge about plants or kittens, patience. What does the world need? Emotional labor, community care, cultural preservation, mentorship, activism. What you can be paid for. know what your community values, the non-monetary rewards such as recognition, respect, gratitude, and love. Thank you. >> All right. Uh thanks very much uh for uh the opportunity to discuss this uh really thoughtprovoking uh paper. So um there are a few facts in the paper that a couple of facts really uh core facts from the economics literature in this area that really struck me and I want to emphasize them. The first one is that we see a negative cross-country correlation between uh GDP per capita and people finding that life has meaning uh or purpose and I think that's a really interesting uh finding that I wasn't aware of before looking at the paper. So that that uh that really caught my attention. And the second one is that if we zoom in to a population whose lives have been transformed uh over the past um century and even more, we've seen that women's life satisfaction declined even while their economic outcomes improved. And that really uh begs the question of what's going on here. And you know, how can we explain uh this uh this phenomenon? And uh so this raises the question even if a AGI were to increase our incomes even assuming that we don't have a distribution problem will it really promote well-being if it undermines our sense of meaning uh or purpose. So uh I realize this is very small um but um you know I think the the the idea uh of looking at this uh question is a very interesting one and uh I really recommend this philosophy uh book called automation and utopia by John uh Danahare. And so uh this book takes on the idea of what it would mean to flourish as a human being in a world without work precisely in the context of uh AGI or transformative technology that the assumption is uh that it could do most of the work for us and so then if that's the case what are the issues for people to flourish in that circumstance and uh he identifies uh uh five different problems uh that uh would um arise for people to solve in redefining what it means to be a human and two of them the ones that I uh I circle there are more closely related to what Betsy was was talking about. One of them is what he calls the severance problem. So this is the uh has something to do with the concept of mastery. People like to be able to master a skill and by reducing our need for human activity, this automating technology make it so that it looks like we don't matter. Whatever we do doesn't have real consequences on the world. So that's the severance problem as we're in separated uh there's a separation between the world out there and what we're able to do to influence uh uh the world. uh and then the second one uh that I want to talk about is uh the agency problem which is there at the bottom. And so there the idea is that uh when technology uh replaces us, it replaces human uh human effort, it undermines our sense of moral agency. So the first one was about mechanical agency so to speak. Are we able to influence the world by doing something is going to change things? The second one is a valuesdriven approach. Are we able to feel like morally we're making the world a better place? And that too to the extent that now things that matter can be accomplished through the machines. Why are we here for as far as our ability to uh make the world a better place? And I will just mention that there are a number of other issues here but one of them that is related to earlier papers today is the attention problem. The problem that these uh machines might take our attention away from the truly valuable things that uh might make make us happy. really an interesting uh typology and I think is relevant for us economists because it uh suggests avenues for thinking and research about models or things we might want to measure to try to get at possible scenarios as in the case that most work uh gets automated. So uh this concept of eeky guy uh really intrigued me because um it made me think about this is an example uh my favorite movie perhaps that's called perfect days by vim vendor. So in this movie the anti-hero here I am uh is he really finds joy in the analog world not digital world importantly and I have a ongoing paper about the important distinction between physical and uh kind of intelligence uh activities. So this uh hero his job is to clean public toilets uh in Japan. It's a physical job. But what's really remarkable about this character is that he transforms this practice in Aristotle's terms. So something that seems to be done merely for a purpose and doesn't have a meaning in itself. But the way he does it, it becomes posesses. That means an activity that has intrinsic meaning and and relevance. And so despite, and it's really remarkable, despite a lack of social recognition, you see how people pass by him like he was he wasn't even there. He didn't even exist. Despite that, you see him like cleaning with intense joy and satisfaction, you know, as he goes about and does his day. And so what's really interesting is that here we don't have uh through this uh physical job we don't have this severance problem because he feels certainly that he's making a real difference uh in the world those toilets are getting cleaner after after he's gone there and we also see him pursuing aesthetic pursuits that so the poetsis aspect such as he listens to cassette music uh like perfect day uh by Lou Reed which I've learned to play on my guitar after uh watching the movie. Uh he photographs trees, he reads uh physical books and also importantly he supports other people like his lease, an anonymous child who got lost in the park and he uh he's able to uh help out. So by this he exercises moral agency uh in Danahara's uh uh classification. Um, and so importantly, this is an example, but it shows you how low income and we see that he lives in a shoe box house uh doesn't prevent this man from appreciating life. He he has plenty of eeky guy. I would uh I would argue another interesting reference uh in that domain is the myth of Cifus by Albert Camu where Copifus, you know, pushes the boulder is very laborious. He repeats every day just like the the hero here repeats the toilet cleaning action every day. But then we're told you have to imagine Cephus is happy you know through doing this this repetition. So I think that's that's also an interesting uh way of uh thinking about this problem. So finally in my last slide uh I want to think a little bit about policy implications. Uh what are we to uh to make of this? I think the findings from the well-being literature uh support that uh the idea that income support is important and we've seen bets show that in fact life satisfaction increases with income uh but may not be sufficient to uh promote well-being and another finding that goes in that direction recent uh randomized control trial shows that unconditional cash transfer to low-income Americans did not durably improve mental health. So at first people were of course happy to get more income but then they adapted and presumably the problems of their life you know uh uh took over. So um this really raises the question of in this potential scenario that we would be in a world without work where everything is fully automated. What is needed besides income? what are the sorts of new uh policies and and social organization that we can envision so that people can have meaning and flourish in this uh post post uh uh work society. And there I think again economics have a lot has a lot to say because we talk a lot about things like incentive, coordination games, social norms. So we can think about how can can we arrive at a new equilibrium uh that would work and would encourage people to engage in those meaningful activities even where the incentives around work uh are removed. Thank you. [Applause]"
  },
  {
    "id": "eP3ic8EOv6w",
    "title": "We Won't Be Missed: Work and Growth in the Era of AI",
    "url": "https://www.youtube.com/watch?v=eP3ic8EOv6w",
    "presenters": [
      {
        "name": "Pascual Restrepo",
        "affiliation": "Yale University and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=R2wIIzMAAAAJ"
      }
    ],
    "num_presenters": 1,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nPascual Restrepo, Yale University and NBER\nWe Won't Be Missed: Work and Growth in the Era of AGI\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "In his presentation at the NBER Economics of Transformative AI Workshop, Pascual Restrepo explores the implications of artificial general intelligence (AGI) on labor, wages, and economic growth. He conceptualizes AGI as a transformative technology that can convert computational power into various forms of economically valuable work, akin to a \"country of geniuses\" that processes compute and outputs work across multiple sectors. Restrepo's research focuses on how this shift could redefine the labor market, particularly distinguishing between \"bottleneck\" workâ€”tasks essential for economic growth, such as healthcare and energy productionâ€”and \"accessory\" work, which may not be critical for societal flourishing.\n\nRestrepo's key findings suggest that while AGI has the potential to automate all bottleneck work, accessory work may remain untouched due to its lower economic value and lack of urgency for automation. This leads to the conclusion that even in a future dominated by AI, there could be stable employment for individuals engaged in accessory tasks. The implications of this research challenge traditional economic principles, such as comparative advantage, suggesting a scenario where specialization may not yield the expected benefits because AGI could automate essential tasks across the board, leaving accessory work as the main source of human employment. This perspective raises important questions for policymakers regarding labor market adjustments, the future of work, and the societal value of various types of employment in an AI-driven economy.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 6278,
    "char_count": 33877,
    "transcript": "Okay, thanks so much to to the organizers for for inviting me to be part of this great conference. So let me start by thinking about this metaphor that that I think Anton suggested which I think is is very helpful which is how what are the implications of of being in contact with this country of geniuses. And the way that I want you to think about this country of geniuses is essentially this is a country in the middle of the Pacific. you send compute to this country and in exchange it returns every types of economically valuable work. So it's just a technology that transforms computing power into work. And so that's kind of like how I want to think about AGI in general. And this paper is about working out the implications of that for wages, economic growth, the labor share and so on. Okay. So let me tell you how I'm going to model this. So let's imagine. So this is going to be my macro slide of the day. Let's imagine that we're producing output some final good YT. So think about this as kind of like as a measure of the flourishing of the human species in the future. That's what YT is. And that's going to be a function of XT which is a vector of all of the types of work that an economy has to accomplish in order to get the human species to flourish. So think of many things could be in this vector like gathering energy, shelter, health care. All of these things are the types of things that we need to accomplish for the future of the species. And I'm going to denote all of these things by omega. They belong to some big set omega that denotes all the types of valuable work in this economy. Now every type of work omega, you can also think of this as tasks, but for whatever reason I'm calling it work, but every type of work omega can be produced by humans. So that's the LT omega or by AI systems. And so the way that I want you to think about these AI systems is again they eat compute and they regurgitate work on the other side. And so they're going to eat QT omega units of compute and they're going to deliver one over alpha t omega units of per unit of compute of that useful work. Now this alpha t omega is going to be very important. That is essentially a measure of how many computing power do I need in order to match what one unit of human labor delivers. So this is a measure of algorithmic progress. Now how are people producing stuff where people have different skills s there's a total endowment hs of people of skill s and people of a given skill what they can do is that they can accomplish different types of work that belong to a set omega s and so in particular the resource constraint of this economy for labor takes a very simple form. It says that the total amount of human labor that is allocated to all of these types of works that people of skill s can do has to be less than or equal than the total supply of people with set skill. Finally, we need to think about computational resources. So, we're going to have this big Q which is the total amount of computing power per unit of time that we can do in this economy. to think of this as the number of computations flops per unit of time or flops per year that we can implement in this economy given the hardware that we currently have. So this is going to be the second type of technological progress that I'm going to have in this model. So this is total computational resources per year and I'm going to assume that this keeps growing over time. So I'm thinking of two forms of technological progress. Algorithmic progress, those are the alphas and computing progress. That is the cue. Okay. So now let's go back to this figure and let's try to think about what the premise of the workshop means in terms of this model that I introduce. We have a the AGI country, the country of geniuses. They're eating compute. They're spitting out work. Now Anton said that one definition of AGI is that they're able to produce all cognitive work. But in reality, I think that we need to extend that because if you give them actuators and ways to act on the physical work, they can also produce physical work. And the only way that you need to do in order to model this is think of QT not only as compute. But QT is going to be like a combination of computing energy and the materials needed in order to build the robots that you're going to use for acting on the real world. And I'm going to argue I mean this is part of the premise but I'm going to provide some arguments at the end that you could think that social work so work that requires interaction with people is also something that these AGIs can achieve with sufficient compute. So the example that I give in the paper is just think of a therapist like initially people were like oh therapy that's something that is very human is something that we really want to have a person on the other side flash forward like people are happy to anthropomorphize these computer systems and imagine these computer systems they know they know you better than anyone else they don't get tired they are there and they have like this huge amount of compute devoted to just figuring out what's wrong with you like are you going to insist on seeing a person or you going to see this what fantastic system I'll take the system so I I think that in general we can even think that even social work could in principle be completed by these systems if we have enough compute and enough algorithmic progress. So the way that I'm going to think about the definition of AGI is I want to think about this as a limit. A limit when I have enough computing resources and where all of the alphas converge to some finite values. They can be very large. So maybe I need a lot of computational resources to implement something but they're going to be finite. So that's going to be my definition of what AGI means. Okay. Now, we can disagree with the definition and like I'm happy to discuss that during the Q&A, but let me kind of like postulate that this is what's going to happen. I don't know if this is what's going to happen. I think that I don't have a good argument for why it wouldn't happen. So I'm just going to take it as given and I'm just going to figure out what are the implications of this. So I another way of thinking about this definition is that essentially AGI is a technology for turning compute in every type of useful work. So compute becomes kind of like a universal input sort to speak. So let me kind of like walk you through the results. For my results, it's going to be very important to our definition of bottlenecks. So a lot of people have said the word bottlenecks today and I think that everyone has a different definition of bottlenecks in mind. So I have my own definition of bottlenecks. So let me give you my definition of bottleneck. I'm going to say that one specific type of work omega is a bottleneck. If you have a sequence such that output diverges for that sequence either that quantity has to be growing so we need more of it to get output to grow or the value the marginal value of that type of work explodes and becomes unbounded. Okay. So if I have a sequence where we're getting infinite output either that quantity of work is expanding or it's becoming increasingly valuable. That's going to be my definition of bottleneck. And then all the things that are not a bottleneck I'm going to call them accessory work. So the pro the key property for accessory work is that you can get infinite output while holding this type of work constant without getting its value to explode. Okay. So that distinguishes you get kind of like a partition of all of work into bottleneck work and accessory work. So now we're in a position oh sorry I forgot to give some examples. So I mean it's hard to give examples because this really depends on how technology looks in the future and so on. But I think that there are some things that we can agree. So for example, I think that healthcare is a bottleneck work. This is a thing that either in the future we're going to have better health or we're going to be willing to pay a lot for getting better health. So that's kind of like my definition of a bottleneck. H preserving the safety of our species in the universe, that's probably like bottleneck work. Shutting down asteroids that are coming towards the Earth, that's going to be like bottleneck work. Extending our lifetime, that can be bottleneck work. Finding new sources of energy, that can be bottleneck work, right? And accessory work, I don't know, like many of the things that we do are perhaps accessory work because you don't really need them for the flourishing of the species in the long run. So for example like I don't know like like having people not not not this is not a good example because transporting people might be a bottleneck work but for example having a waitress or something like that could be accessory work like the willingness to pay for having someone bring you the food to the table maybe that's not going to explode in this future if we don't automate that. Okay, so the first result is that if you move to this AGI world, all bottleneck work is going to be eventually automated, but there might be accessory work that is not automated and is left untouched by AI. So this is important because remember in principle we could do everything with AI, but this is showing that we're not necessarily going to do everything with AI. For sure, we're going to do the bottlenecks with AI. So let me give you kind of like the simple argument behind this result. So imagine so the first point is that because I can produce everything with compute and comput is expanding it must be the case that output is growing without bounding this economy. So the second point is imagine that omega one type of work is a bottleneck. So either its quantity is expanded which means that we automated it because we had finite skills to do it or its price is growing without bound which means that at some point it's going to be economically profitable to take some of the compute that we have and automate it because the value of that is going to be huge. The only case in which you're going to leave something untouched if it's accessory which means that even though we have people doing it its value it's not exploding. So the wages that you have to pay people for that work are not exploding and so you don't have growing incentives to automate it. And so again the reason why you're leaving this job untouch is just because those computing resources they have an opportunity cost. You can use them for all of these fantastic bottlenecks. So one of the things that I think is interesting is that imagine a future for example where the only type of thing that is a bottleneck work is science and reducing existential risks and healthcare. So we could be in a future where we are using these AI systems and all of our compute for this type of work and the rest of the economy remains essentially completely unchanged and that could happen right so this accessory work can provide kind of like a stable source of employment for all of us so if you think that you're doing accessory work kind of like good for you your labor market is not going to change a lot into the future if you're doing bottleneck work it might change a little bit and I'm going to explain exactly So now this seems to run counter to the principle of comparative advantage because let's imagine for a second that all work in this economy is a bottleneck. So essentially what my first result tells you is that everything in the economy gets automated. But wait, didn't Ricardo told us many years ago that we should specialize? Like we have this country of geniuses. So the geniuses should be performing something and we we should specialize in something else. and there's gains from specializations. Well, not really. So, let's kind of like think about what Ricardo actually says or let's think why Ricardo's argument doesn't work in this case. Let's imagine that I specialize. So, let's imagine that there are two things. So, there's bread and there's wine and AI is great at producing bread, I guess, and humans are great at producing wine. So if we specialize, what this effectively means is that we're going to have a growing quantity of bread and we're going to have finite wine to enjoy that bread. That cannot be optimal. So it's going to be much better to have infinite quantities of both things. So therefore, a specialization is not always optimal. This is actually kind of like a footnote to Ricardo. Like what he said is that if the two countries are similar in size, you get specialization. But if you have a very big country and a very small country, the small country specializes, the big country doesn't specialize. So if the US starts trading with Malta, there's no reason to specialize because you're trading with Malta, right? Like you're supposed to keep producing everything yourself. And here something similar is happening. The AI is not specializing because we're there. So they produce essentially everything. So we are happy. Uh what good is a world with abundance of bread and no wine? second result. So let's imagine everything is a bottleneck and I want to provide a characterization of what happens with wages because this wasn't obvious to me like because AI is going to produce everything that doesn't mean that wages are zero. The reason is because AI essentially can replicate what you're doing but it can do it at a cost. the opportunity cost of the compute needed to do wherever is the thing that you're supplying in the market. So let me define this CEU as the compute equivalent units of skill S. This is how many units of computing power I need to replicate you at your best or your most complex possible work. Now what I show in the paper is that if all work is a bottleneck then output essentially becomes additive between compute and human skills but the human skills are going to be value at their compute equivalent units and that a on the front is a positive number that gives me the rate of transformation of compute into output. So this is how much extra output do I get per unit of compute. And so from this expression you can see immediately that wages are going to be equal to 80 times your compute equivalent units. So you're going to be paid the opportunity cost of the computing resources that you're saving by working. So this is interesting. I didn't know this like in this model just because you produce everything with AI doesn't mean that there are no wages. AI is just lowering the cost of work and you're being paid that price of the work. Now the sad news is that you can also see from here that well I'm not going to do the proof. I'm going to skip it. But the sad news is that you can also see from here that the economy is growing because Q is growing but wages remain fixed. Okay. So essentially the labor share is going to go to zero in this economy which means that all of the income is going to be income acrewing to whoever owns this compute. What about the not what about the accessory work that I told you doesn't necessarily gets automated? Can that be a source of growing wages? Well, it cannot. So, let me make the argument. So, let's go back to the general case where you have bottleneck work and accessory work. And let's imagine that your work doesn't get automated. What does that mean? That means that your wage is low, right? So that means that actually your wages are lower than the opportunity cost of the computing needed to automate your job. So the fact that you don't get automated in this accessory work is not necessarily good news from the point of view of wages. That just means that there we have already too many of you. So it's not worth automating you. So this zone outside of the automated work provides a stable employment opportunities but is not going to provide growing wages. So you're still going to get that the labor share goes to zero even though not everything gets necessarily automated. Next result the labor share is going to zero. So many people worry at this point and say well definitely has to be the case that workers are worse off and that they really depend on getting some of the income from compute. Well that's not the case. There's actually gains from trade here. So what one can show is that at every point in time in this postagi economy wages this is the sum of wages of all humankind are going to exceed the initial wages that they had in the pre-agi economy. So workers as a whole they have to benefit. So this one I do want to make the proof because I think it's cool and it doesn't have any math. So this is the proof. So we know that the long run allocation has to maximize output. So let's imagine that I'm a planner and I take a representative group of workers and I send them to a no AI zone, a zone that is not using AI at all. Because of constant returns to scale, they can produce the exact same output that they produce initially, namely their wages. What am I giving up when I do that? I'm giving up their current wages in the post AGI world because the initial allocation maximize output that has to reduce output. So initial wages have to be bigger than sorry final wages have to be bigger than initial wages. And so there's Ricardo in a no AI zone. So as long as you have no AI zones that are not competing for resources with the AI economy, you can guarantee that wages are not going to go down. Final results. So I have two results about the transitions because the results that I show you previously are about the long run implications the limit what happens along the transition. So for the transition there's also this result that I thought was very very interesting to me which I didn't anticipate. There are essentially two classes of transitions. There's one transition where compute binds and there's one transition where algorithms bind. So let me explain first this transition where compute binds. So the idea is that at every point in time we don't have enough compute to justify automation. So even though the algorithmic capabilities are there, it takes a while for us to get like enough compute to finally start doing it. That transition actually doesn't look very scary at all. Your wages are growing and you just get automated the moment that your wages reach the opportunity cost of producing what you're doing with compute. At that point actually the technology doesn't even displaces you in the sense that there's not a jump in employment but employment starts decreasing gradually in the job that you start doing with AI. So you start deploying AI gradually in that particular type of work and only when you have finished leaving that type of job wages start growing again and then the next type of work that you're doing starts getting automated and so on. So you get this pattern where there are no jumps. everything is very predictable and essentially your wages in the long run are going to be higher than your wages initially. Now this is very different. So for me this is again interesting because there might be applications of AI like AI for science that are eating all of the computing power and so that means that you're not going to have a lot of displacement going on in the back. It's just kind of like this displacement is just going to come slowly in the future only when we have enough computing power to justify automation. So maybe the transition is not a bumpy ride which was my prior. The transition is only a bumpy ride when technology binds. So this is an economy where you have a huge amount of compute. The AI companies are playing with their algorithms. Suddenly they figure out a way to automate economic professors and boom immediately we have enough compute to do it and wages might actually decrease when that happens. So in those transitions you get this pattern that looks very risky which is when there's algorithmic progress wages go down when there's algorithmic progress wages go down then they go back increasing and then at the end they go back again and converse to the opportunity cost of the computational resources needed to replace you. So let me take stock just by saying that this paper introduces a way of thinking about AGI. This is a definition like people might disagree with this definition and I think Neil is going to offer perhaps a different perspective. This is a technology for turning compute into all forms of useful work including cognitive, physical and social work. All bottleneck work is going to be automated and only this accessory work is going to left untouched. Labor still has value. There's still positive wages. Your wages are pinned down by the opportunity cost of the compute needed to replace you. The value of labor shrinks. That's the only issue. So the labor share goes to zero. But this is a rich economy and we collectively own the compute. So we are richer as a result. Not only that, but our collective wages are higher than before. So there's no sense in which work or labor as a whole is getting decimated in this economy. There's also the bottom line that accessory work might provide stable opportunities for people. So there's one future in which our lives do not change at all because we end up using all of the computing power to conquer the galaxy and to run fusion reactors and it's not worth using on what we are doing. And finally the transition may be smooth and this will happen if we think that there are killer applications for this computing power that are eating away most of our computational resources. Thank you. I'll discuss this later. Okay, so I think Pascal did a very good job with this. So I'm just going to give a quick summary, right? What he finds is that these model he proposes this model of economic growth that has this very nice and very tractable form. Right? We say the output the X that you're going to put in is either coming from the labor or is coming from Q the amount of compute that you're putting in here um that you're getting from it. And of course that's not right. Oh, we don't have the pointer on there. Okay. Um and then but you have this alpha here which is saying what's the conversion rate right? It takes some amount of compute in order to convert it into one unit of labor. Great. Now the first thing I want to say that I you know really like about this work is it's really acknowledging the importance of bottlenecks. And we heard a lot about that from Ben earlier right and it does this beautiful partition that says some of these things are really crucial for the economy some of them are not. If it's not crucial for the economy it doesn't need to be part of this ride that we're going to be going on. And I think that's just a you know first order important thing to think about because if there's a whole bunch of accessory work as he said this can change the labor outcomes in a meaningful way. Um the result the econom the economy continuously uncouples from human labor until it scales scales linearly with compute right this is because all of those tasks become bottlenecked and then they're become uh used by AI during this transition. Humans can still be employed but only in areas that are more efficient or in accessory areas. Okay. labor share of output does fall to zero and the automation of science accelerates growth. Great. Okay. So um I think importantly here right this Q he mentioned this Q can is not just actually just compute right it can be if you have to have robots it can be energy it can be the materials that go into robots things like that and so there's some implicit scaling there that's going to tie us to the rest of the material economy and I'll talk about that in a second okay so there's a lot to like about this paper so it's a really elegant model it is not sort of assuming Cobb Douglas or anything like that. It's a very general form that that allows it and it's a real embrace of the TAI agenda, right? This is saying all work can be done and it sort of puts us in that world in a really useful way. Um I also am while I did, you know, in my response I do quibble a little bit with the exact AGI definition that Pascal uses. I actually think that broadly that's actually a really big step forward because what it's saying is here's an economic definition of TAI, not just a technical one. Right? And most of the discussion that's been happening about these sort of transformative things comes at it from a technical point of view. I think actually having an economic definition is great. I have little quibbles. I'm not going to spend any time on that actually. Um and then I do like you mentioned this specifically in your talk. I really like that you say in so much as like humanness is a means to utility actually it's pretty compelling to argue that we will be able to replace that. We'll be able to find a way to have other things that compensate for that. And we see that all over the place, right? We don't use bank tellers. You know, it's nice that the bank uh the ATM is open all the time. Okay. So, in terms of model limitations, there's sort of two things I want to pick up here. And so, the first one is really the constant returns to scale. This is doing a lot of work in Pascal's model that we get to do this. And because we have this very broad aggregator function, this constant returns to scale is not just constant returns to scale in production. It's constant returns to scale in production. and then how production turns into utility. Okay. And that means that there are a whole bunch of things that we've talked about over the last couple of days that don't really get subsumed in this, right? So if we have limits from other resources, satiation points, decreasing marginal benefit of different products, positional goods, all of these things are going to sort of derail the constant returns to scale. if we think any of those things are bottlenecks. Um there are also we argue in in our response there cases where humanness might not be the means to utility but actually the ends right so in the Olympics the whole point is that it's humans doing it right and you can say like I don't care how much faster the machine can run like it is the humanness that matters here okay so I think that's sort of a very important one I'm not going to say much more about that I want to focus for the rest of my discussion on the second one which is that in this model compute is expanding exogenously and And I'm going to argue that actually implicitly under the hood, it's not just that it's increasing exogenously, but actually the cost effectiveness of compute has to be increasing as well. Okay. So let's think about that in a particular context. So you know for this whole conference we've been talking about transformative AI and this idea that we can sort of get all the way to it can do all tasks that a human can do. But I want to ask you is like what if we're like quite close to that but not all the way there right away, right? Right? And this could very much be from the bottlenecks argument. And we sort of heard that that matters a lot. So in that case, what matters here is the transition that we're going to go through to AGI. And Pascal does a really nice job in the paper and you heard it here of characterizing sort of two extremes of this. One version where it's the algorithmic progress that's a limit and one where it's the computational progress that's the limit. And I would argue that actually we're going to be living very much in something in between those things. And actually thinking about what that looks like is not is an easy thing but could very well get us into a world where bottlenecks matter a ton even for the production of compute which might matter a lot here. So let me talk about what I mean here. So crucially along this transition that Pasqual has we have to expand all bottlenecks work at the same rate right that makes a lot of sense in terms of how we're going to do this and because we have these computational resources going exogenously eventually we're going to accumulate enough. But of course the question is like how much do we actually have to do? If it's more than 100% of the economy that's going to be a problem obviously. Okay. So I want to take a second to sort of jump back to think about what some recent benchmarks tells us about this jagged frontier of AI capabilities. So this is some uh something from the GPD4 technical report when it came out right I think it caused quite a big splash. And what it said was for actually working on the bar exam, right, we went they went from the 10th percentile to the 90th percentile from GPT 3.5 to GPT4. That's only about a 10x increase in the amount of compute being used. It was a huge improvement in performance, right? If that's true, then you know we probably are going to be in a compute abundant world because we're going to be able to accumulate Q uh compute quite quickly. But on the other side, we have something that Seno mentioned yesterday. So the ARC AGI2 benchmark, right? This is a case where we've gone from 0% to 30% on this benchmark with about 10,000x increase in compute. Okay? And just to be clear, for almost all of these benchmarks, that first little bit is a lot easier to get than the last bit. Right? So this is one where it's very plausible that there could be many many orders of magnitude of compute necessary to get to human level of per performance. Okay. So if we think about this more generally I think we can think of this as coming from underlying things about how good these models are not just on benchmarks but on general performance. And so here I'm going to call back to scaling laws. I think this is an one of the unusual audiences where probably most people have in this audience are familiar with it. But if you aren't, the key thing here is that in almost every area that we look at it, we get this pattern denoted in each of these cases by the dotted line which says that as we increase the amount of compute by which we mean this the number of parameters in the model, the amount of data, we get this very predictable improvement in performance as we go on. Okay. And we can see that in all of these different domains that we have. Now the crucial thing about the sort of form of this particular uh of these scaling laws is that they say that if you want to have the amount of loss that you have left right you're getting 10% of the time you're making a mistake you want to get down to 5% 5% down to two and a half and so on you have this very predictable relationship about how much you have to increase the amount of compute that's involved. Well, how much? Well, for improving on images, these are pretty small images. It's only 40x, you know, video 140x. Bottom right hand side language 1.9 millionx. That's a really big number, right? Okay. So, why does this mean that compute might be constrained in the way that we might worry about the exogenousness um in Pascal's model? Well, what if we just needed to say, well, we want to have we need to have the amount of error that LLMs are making in order to get to some level of performance, right? That seems like a pretty reasonable thing you might imagine. Well, current models cost about a billion dollars to train. The scale up required 1.9 million x, right? We just saw that. So, that's 20 times the world economy, right? That's a big number. uh if we needed to incre decrease LLM error by 75% in this underlying measure okay now it's 40 million times the world economy okay so this suggests that if there only have to be a small number of these tasks that are the bottleneck tasks that Ben was telling us about to lead to the situation where we could very well be compute constrained for a long long time in actually doing these things okay so this is sort of a particular form of how we've improved models right these scaling laws tell us we can increase compute of course we also If you're in these companies, you're doing a lot of other things. You're thinking about clever algorithmic things. You have people at Intel and Nvidia thinking about clever hardware things. If we look at the actual progress that we've had over the last years, how does that actually look? Well, looks like this. Okay, so this is looking at the effective compute increase. So this is both the compute increase we paid for and also the compute that we didn't have to do because we were clever about our algorithms or clever about our hardware. And what you can see here is that over the period from 2014 to 2023, right, we have gotten algorithmic efficiency, that's the cleverness of, you know, we figure out distillation or we prune our models, all those kind of things you hear about 22,000x that's pretty good, right? Hardware efficiency, all the stuff that Nvidia is doing about a 45x improvement, but hardware quantity is the dominant way we've gotten it, right? Almost 400,000x in terms increase in this, right? So this is this does suggest that we the dominant way that we have gotten performance improvement so far has been by scaling up the amount of compute and that suggests that it could be a limit in the stuff that we're going to do in the future. Okay, so just to finish here, this is really it's a beautiful model and I really appreciate that it's pushing the stuff forward in this scaling laws though do imply that in the foreseeable future we're likely to be in a transition state, not in sort of end state. Um and if we indogenize the production of compute into this model that probably could even slow the transition down that we would get. And so I think focusing on those transitional dynamics that Pascal is already doing, I just want to see more in that middle ground of what happens when some tasks are and some tasks aren't, I think would be really important. Thank you. [Applause]"
  },
  {
    "id": "_xTeQgbAPZs",
    "title": "The Impact of AI and Digital Platforms",
    "url": "https://www.youtube.com/watch?v=_xTeQgbAPZs",
    "presenters": [
      {
        "name": "Joseph E. Stiglitz",
        "affiliation": "Columbia University and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=2AF4iHIAAAAJ"
      },
      {
        "name": "Maxim Ventura-Bolet",
        "affiliation": "Columbia University"
      }
    ],
    "num_presenters": 2,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nJoseph E. Stiglitz, Columbia University and NBER\nMaxim Ventura-Bolet, Columbia University\nThe Impact of AI and Digital Platforms on the Information Ecosystem\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The presentation by Joseph E. Stiglitz and Maxim Ventura-Bolet at the NBER Economics of Transformative AI Workshop addresses the critical question of how artificial intelligence (AI) and digital platforms are reshaping the information ecosystem. The authors explore whether these technologies are likely to exacerbate the deterioration of information quality, particularly in the context of legacy media, which has already been experiencing declines in both the number of producers and the quality of information disseminated. They argue that while AI can enhance information dissemination, it simultaneously threatens the economic viability of traditional media by diverting audience attention and revenue away from these producers.\n\nKey findings highlight that the proliferation of AI and social media platforms leads to a dual effect: it lowers the cost of producing misinformation while also complicating the detection of falsehoods. The authors model this dynamic, identifying that the overall quality of information in the ecosystem may decline as the production of lies becomes easier and more profitable compared to the production of truthful content. They propose a framework that categorizes the potential outcomes of this system into three types of equilibria: a truthful equilibrium, a mixed equilibrium of lies and truth, and a scenario where truthful information ceases to be produced altogether.\n\nThe implications of this research are significant for policymakers, as it underscores the need for a robust legal and regulatory framework to support the sustainability of truthful information production in the face of AI advancements. The authors stress that without such measures, the ability to obtain current and accurate informationâ€”crucial for understanding and responding to rapidly evolving social and economic conditionsâ€”will be severely compromised. This calls for immediate attention to policy solutions that can safeguard the integrity of the information ecosystem amidst the transformative effects of AI and digital platforms.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 4076,
    "char_count": 22354,
    "transcript": "Well, thank you all for for inviting me and uh this is going to give a little bit more of a pessimistic view of what could happen. >> Wait, I replace everybody. >> But but there you're replacing them, but we get all the fruits of it without without doing anything. And I I'm going to uh our paper gives a little bit uh uh uh of a of a uh negative view of what might happen. So the question is um is it possible is it likely um that the AI and extending what's already happening to the social platforms uh will lead to a worse information ecosystem. So this is really very much of a general equilibrium problem that we're looking at but general equilibrium within the uh supply of information uh in in our society and um if if there are these uh downsides um is uh what are the channels uh by which this might happen and uh uh that is sort of a guidance to uh what policy responses one might have. And we don't have time because of the uh strict uh page limit to have gone into the policy solutions. Uh but maybe that's uh for a later day. Um so we're going to begin with a a fairly simplistic view of the information ecosystem. before AI uh there were uh these producers uh uh like newspapers, journals, online websites uh that created a content uh that added to the stock of knowledge and became a public good. Uh we call that leg legacy media and then uh we can think early on uh producers largely reported the truth. uh later uh some found profit in misinformation uh both for economic and strategic re reasons. Uh Murdoch was an example. He he he discovered that that entertainment was uh drew more eyeballs than actually providing news and uh Fox discovered that appealing to your base was better than providing accurate information. But that's sort of the basic framework. And uh then the basic business model dependent on attracting uh visics uh generating revenue through ads. We we'll talk about it like eyeballs and uh subscriptions or data collection and we're not going to talk about that but that's an obvious extinction to what we're going to do. And then consumers visited producers to access information and benefited often at little or no direct cost. Um uh it was a system uh that uh worked but uh worked uh imperfectly. Um information is a public good and one wouldn't think that a private provision of a public good would have any optimality properties. Uh that's not the the issue is though it worked and I is that as I say um it produced information re uh uh uh and uh uh you know so so so in the when I say it worked it provided a lot of information uh but uh in some uh places the system was beginning to fray even before the advent of AI and social media but now um the system is collapsing and this is you know been some discussions h how do we know AI is having an impact and social media are having impact this is one area where what's happening is very dramatic uh the number of reporters is going down uh very very dramatically uh the number of uh major media is going down dramatically. So we have evidence of a of a dramatic change. Um the question in some ways is well the system has gotten more efficient. Uh should we be that concerned about about um uh the fact the fact that there's less producers of information if the system is overall uh becoming or is in some ways uh becoming uh uh more efficient. And um there are two basic uh drivers of of the ideas behind uh what I'm going to talk about. I'll refer to it later. One, it was the old expression uh geigo garbage in garbage out. If you have an information system, no matter how good you are at processing and analyzing the information, if you don't have good data coming in, there's no way that you can have a good output. And the other one is uh the work of you know gross mstig which talked about how in financial markets if we had perfectly efficient financial markets no one would gather any information. So it was great that we that people of Chicago celebrated uh that the financial markets were perfectly efficient but if that hypothesis were true which it was not but if it were true the only stuff that you would see on the information on the financial market would be garbage. So that's not a good information system. So it's trying to look at those kinds of questions in the context of a model that captures some aspects of AI and the social media. So how will AI affect the information ecosystem? Well, we we identify four effects. Uh it improves uh information uh dissemination. That's sort of analogous to theformational efficiency of financial markets. But at the same time, it endangers the producer business model because it steals eyeballs away from the uh legacy producers. Um but another aspect is not all information that's produced is truthful. Just think of Fox News. So um there are lies and it's often difficult to detect lies. So we model that as saying lowers the cost of producing lies, deep fakes relative to truth. And then uh something we call the drone war uh war effect. It simultaneously increases the ability to detect lies but also increases the ability not to be detected because it understands the detection process. Um and so the point of this uh paper is to develop a a tractable model that can get uh uh provide an equilibrium analysis of uh the information flows. So uh the intuition behind this uh paper and gives a little bit of guidance to where policy ought to go is that with the appropriate legal framework without an appropriate legal framework and IPR uh AI social media may undermine the business model producers information so less information will be produced and uh this is important uh in certain areas like understanding the ever evolving economy. It is important to have current and accurate information about critical variables describing the system. And this sort of fits in with some of the discussion we had in the previous section. What kind of information problem are you addressing? Uh you know the the if you're analyzing the problems in in uh uh um chemistry, the chemistry of the universe is not changing. But if you're analyzing the social economic system or even a virus, there's an ever always evolution going on and you have to know the state of the system. So you need to have current information if you're going to know what to do and it's that very important part of uh what we want to know about that we're focusing on here. It's it's where flows of information about the state of the system become critical. Um though information that is produced will be analyzed and disseminated more efficiently with poor new information entering the information ecosystem the quality information ecosystem uh will deteriorate. So uh let me describe the model uh very very quickly. Um there are uh going to be producers of information consumers and then in between there going to be these intermediaries the AI firms the social technology platforms. So on the on the uh producers there's a continuum of producers uh they produce a flow of information and there are two kinds of producers we we can obviously enrich it uh uh those that produce lies and uh those that produce truth but whether firms produce lies or truth is an indogenous variable that's a that's a a decision variable and uh then uh we can think of the stock of information in the system of truth in the system of lies in the system as uh described by a simple differential equation. Uh information that you've accumulated in the past depreciates because it becomes less relevant uh as the world changes. So that's the the notion of depreciation. We can model it more sophisticated. And uh the addition to information at any point is the sum of the information produced by all firms. Uh that's an additive formulation. You could obviously have uh formulations where uh uh you know this says there's new nor duplicative research uh no complimentary information and again it's something that we can uh uh obviously generalize. Consumers again we have two groups of consumers. Uh there are the informed and the informed people are those who costlessly screen between lies and truth. Uh again we can indogenize uh spending on on screening but here we just take it as exogenous and the rest are uninformed and they can't tell a lie versus the tr the truth and uh uh at least at the time of acquisition. So uh then we get uh uh an equation which is the ex the eyeballs equation. How many minutes do you spend acquiring information and uh so sort of like the demand function uh and uh it's going to be a function of the flow of information. So how much information is available that's new and your valuation and your valuation we assume is a function of the stock of information that you already have and uh and then there's another parameter which gamma which is a captures it's really a vector but we'll talk about in various ways the efficiency of the information system. So that's where AI allows you to process information or uh uh technology platforms lowers the cost of acquisition uh of information and um the um a simple rule here uh where uh you if you're acquiring information uh your eyeballs are attracted in in proportion to the fraction of uh uh news that's being produced. And for for the informed people, they only focus their eyes on truth and the uninformed people look at all the information and they can't tell the differences. So uh uh any uh producer gets the fraction of the flow of of new news uh relative to the uh total that is produced. Um and then there's one more uh parameter that uh is important uh and that is uh consumers uh can either go to the producers or they can go to these intermediaries. they can go to uh check GPT or they can go to get information from from social media and so and and the the the legacy media only makes money from eyes that are going to to news uh directly. So one minus lambda cap tries to capture the notion that uh and again it's an endogenous variable. We treat it here as exogenous but then you know implicitly it is being affected by the technology uh that we're confronting. So then just we have a very simple revenue function. The revenue function for uh any producer is a function of the number of minutes that people spend on their uh media. So that that's drives uh the ads. Uh and then uh next one. Yeah. And uh as one would have thought you you write down the the revenue function and um uh uh the the next thing go on the next one. Uh you you oh I'll do it. Okay. Uh uh and you have the cost function. Let me say the cost function um h captures what I said in the beginning that um uh it's a function of uh how much truthful news and how much lies you produce and it's easier to produce lies than truthful information. And uh the the cost function reflects that if there's more if if you know more it may cost you less to produce. On the other hand, if you know more, you have you I mean knowing more and Q here is sort of the information in the e ether ether. If there's more knowledge, you're better at acquiring information, but the lowhanging fruit have disappeared. So there's two forces uh going on in different direction and we and we talk how one might uh balance those um and uh uh yeah there okay >> four minutes >> oh wait >> four minutes >> four minutes okay so let me go so uh as one might have thought you maximize this uh you get uh decisions about uh how much information of each type to produce. uh and uh um there there the result of all this is that there are uh three types of equilibrium that can emerge uh depending on the parameters uh of the model you know and the uh the the the uh fraction of informed and uninformed the fraction of time that they're spending on uh going to producers uh the relative cost uh of uh producing uh uh uh lies versus truth. Um so you you given these parameters the three par the three equilibria are uh a truthful equilibrium a world where you have a mixture of lies and truth and uh a world in which you basically do not produce any truthful information. So we call that information collapse. And um so and and the comparative statics are just just what you would have thought that uh if the cost of lies goes up, you produce more lies in in a mixed equilibrium. Uh if uh uh uh there are more informed people, you produce more truthful information. So I there there wasn't anything uh uh very surprising but I think maybe this chart uh uh shows the equilibrium uh where uh the steady state in the system is where the production of new news just compensates for the depreciation of the old news. Okay. So that's the study state of the dynamics and uh we give an argument why it would might look like that. It could have more more complicated but but there's an initial uh convex uh section that reflects in a way uh uh the rand result of the non-conavity of the value fundamental non-conavity and the value information and uh if you have a shape like that what is interesting two things one I if if um you have a improvement ment in the efficiency of the information efficiency in transmission or in uh uh analysis. Uh but this first in transmission what you see is the equilibrium level of knowledge will go down and uh that's a quite general result. uh uh if you then want to ask does the information ecosystem gotten better or worse there are two effects going on and we illustrate that that uh uh much of the discussion that you hear in the celebration of the wonders of better technology is that at our current information equilibrium. Obviously, better technologies are better and and the information ecosystem gets better. That's the movement to A to B. But if the result of it is that we actually accumulate less information, that's the movement from A to C and the quality of information ecosystem is actually worsened. And so I've been told I have to stop. But the the rest of the analysis is really in the same spirit. Uh AI puts everything that we've learned from social media on steroids and not only leads to a lower level of information, a larger proportion of lies and so and further reason why the information ecosystem uh uh gets worse. So I think this is the dystopia that I think we're going towards. Well, on that note, thank you very much our discussant as uh you have 10 minutes. [Applause] >> Yeah. Um thank you very much for organizing this conference and for inviting me. It's a great honor to uh you know read this uh to be here and to I'm very happy to read the paper and it's offers us a very timely uh framework to think about this question which is a first order question. So the question is how do AIs and platforms reshape the production the composition namely the truth line mix and the credibility of information. So what this framework is highly tractable oh thank you Oh thank you. Uh so it offers us a very tractable framework and what's most interesting is this idea of a coexistence of true lie equilibrium. So the platform is going to change the inter platform intermediation is going to uh change the um uh fraction of diverse attention from people's visit to websites and therefore reduce incentive to produce uh information. And it also shifts screening namely how many people are informed, how many people are not informed and the cause of lies. So these are within this framework uh I want to you know briefly push on three things. The first one is we would like to know whether it's dystopia or utopia. We want to think about a quantitative effect namely uh which region. So if we think about using this framework which region shall lead to a net information loss. So would there be a threshold or some characterizations? Some of these can be quantified as I will mention some of the empirical papers and perhaps we could get more clear quantitative answer as to this question. Secondly, I also want to talk about AI's effect on uh so there's a qualitative difference as well namely on the uh misinformation and the strategic disinformation. So because AI lowers the cost of you know producing in do producing high quality fakes high quality um uh and high quality uh deep fakes or you know personalized targeting and amplify the reach. So lastly I want to briefly ask about some questions about the policy and welfare namely what kind of perhaps some of the liability reputational mechanisms or some of the attribution laws. So let me focus on the first point. And the first point is a quantitative effect. So namely here um the dynamics as Joe has just mentioned is that each of the truths and the lies evolves by itself and of course in a dynamic model gradually the new information comes in and the old information depreciates. So the mechanism the quantitative effect is holds throughout all the equilibria uh that you mentioned that is because intermediation is going to divert attention uh AI is going to shift returns attention you know returns of consumers attention to news uh and there is a trade-off so namely one on the one hand the better and the vast quantity of AI providing information is going to have a reader exp readership expansion effect but on the other hand there's a business stealing effect and clearly we can see under some parameter regions the business stealing effect is going to dominate and leads to a reduction in the total uh um uh lower steady state information. Now we do have some evidence on this. So in particular I want to shout out to AC Moir's Paul's paper that even so so uh that you know the net the loss for publishers can dominate uh especially for smaller publishers. So secondly I want to point out that AI is not just you know a parameter shifter I think of it also have a qualitative effect namely um it's going to so the the the the the fact that they can produce high fidelity and highly targeted deceptions going to make this uh deception disinformation much harder to detect and uh they are also going to microtarget audiences who are most vulnerable to such information in particular In my work with Chitan, we talk about for example personalized targeting. How that's going to the strategic competition among these producers are going to indogenously target different segments of the population, different segments of the network and is going to cause indulgence without any other reason. Just uh indulgence cause polarization. And we have some evidence about misinformation at in the financial market and for market that they measurably move the market and the worst performing forms are more likely to generate misinformation. So I would wonder if there could be some minimum uh tweaks to the model. for example, a continuous quality variable about instead of just the economy between truth and lie and um targeting choice uh so targeting choices. So our the exposure is now proportional for example I could target people who are uh you know more uninformed much more than others and because I could uh grab their attention much more easily. Um the next thing of course is on broader so this is a dystopia effect that Joe was mentioning this okay so all this was not just about some businesses about how many clicks or eyeballs business get this is about impact on public beliefs on trust and on our you know the society response so because attention is a common pool all this misinformation and the high quality disinformation to crowd out all the truth and also this is for the access exacerbated because algorithms are great at reallocating attention. So with this in mind uh we could imagine the echo chamber phenomena become worse and worse namely uh you know the filters are going to steer the consumers to their favorite content um you know regardless of whether truthful or not. So you could imagine design levels so policy designs for example uh from a probabilistic sharings. So for example, you know, when we share, we don't just do like and like, we say, well, I like this, but I only have 70% confidence that this is true and so on so forth. In order to we have evidence that probabilistic sharing improves the quality of the um information passed on. We could also have platform constraints. For example, in a recent debating on the uh on ads, we could imagine we have regulatory constraint on the target crossplatform. You know just because Meta owns several different platform may may not be encouraged to use all the platforms in order to charge on design advice. Uh lastly so I want to talk uh just briefly mention the idea of policy and welfare. So this is a tremendously complex question and I want to mention that this question has popped up again and again in various countries in very uh various framework. For example, EU's uh act 15 has clear constraint on how much AI could manipulate and attri how they could attribute should attribute information. Canada also have a law regarding C18 regarding whether you know the whe whether the the platforms and aggregators should pay the individual uh news agencies you know in order to in so to avoid business stealing effect. Um so all this just saying that we are really needing in need of the in need of some of this uh policy response. So as Joe mentioned we could imagine a planner's benchmark model where maximize the long run well-being of a representative consumer. Basically you because this framework is so tractable and is so you know uh convening to use. So we could talk about how do we link the welfare to the parameters and many of these parameters we do have empirical ways to measure and to identify them. So for example one of the target would be what is a socially optimal uh level of platform intermediation. So that would be a very good thing to have and Joe already talked about several uh the policy instruments. So for example, we could subsidize production. We could uh regulate excess or you know attribution laws or we could even tax intermediaries and as many of these countries are attempting to do. So again I just want to say that looking forward I feel that this model give us a really clear a very timely framework to you know connect theory and it's not just economic theory. I feel that many of these points we mentioned also has connection to platform design to computer science to auction pricing to um you know to policy on many other fields. So hopefully that this framework could be used to um you know to connect theory to all these evolving challenges and opportunities. Thank you. [Applause]"
  },
  {
    "id": "9VJ4oFWk1SE",
    "title": "The Coasean Singularity? Demand, Supply, and Markets",
    "url": "https://www.youtube.com/watch?v=9VJ4oFWk1SE",
    "presenters": [
      {
        "name": "Peyman Shahidi",
        "affiliation": "Massachusetts Institute of Technology",
        "scholar_url": "https://scholar.google.com/citations?user=7wK6ZXEAAAAJ"
      },
      {
        "name": "Gili Rusak",
        "affiliation": "Harvard University",
        "scholar_url": "https://scholar.google.com/citations?user=xhJqMBsAAAAJ"
      },
      {
        "name": "Benjamin S. Manning",
        "affiliation": "Massachusetts Institute of Technology",
        "scholar_url": "https://scholar.google.com/citations?user=-gjxfJgAAAAJ"
      },
      {
        "name": "Andrey Fradkin",
        "affiliation": "Boston University",
        "scholar_url": "https://scholar.google.com/citations?user=gNq8wdkAAAAJ"
      },
      {
        "name": "John J. Horton",
        "affiliation": "Massachusetts Institute of Technology and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=RGWsHCoAAAAJ"
      }
    ],
    "num_presenters": 5,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nPeyman Shahidi, Massachusetts Institute of Technology\nGili Rusak, Harvard University\nBenjamin S. Manning, Massachusetts Institute of Technology\nAndrey Fradkin, Boston University\nJohn J. Horton, Massachusetts Institute of Technology and NBER\nThe Coasean Singularity? Demand, Supply, and Market Design with AI Agents\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The presentation titled \"The Coasean Singularity? Demand, Supply, and AI\" by Peyman Shahidi and colleagues explores the implications of transformative AI on everyday market transactions, using the example of purchasing a barbecue grill. The research focuses on how AI can reduce transaction costs associated with market interactions, such as price discovery and preference expression. The authors argue that as AI capabilities improve, the efficiency with which consumers can articulate their preferences and execute transactions will also enhance, potentially leading to a significant shift in how consumers engage with markets.\n\nKey findings suggest that AI agents could play a dual role in market interactions: they may either replace human involvement in certain tasks or augment existing processes by assisting consumers in making informed decisions. The paper posits that demand for AI agents will likely grow as their costs decrease and capabilities expand, especially in high-stakes environments where evaluating numerous options is challenging. The authors also discuss the potential for different types of AI agents, including \"bring your own\" agents versus platform-provided agents, and the implications of these configurations for competition and market dynamics.\n\nThe implications of this research are significant for both economists and policymakers. As AI becomes more integrated into everyday transactions, it could reshape market structures and consumer behavior, necessitating new regulatory frameworks to manage the interaction between human and AI agents. The authors emphasize the need for further research and development to address the complexities of preference learning and authority delegation in AI systems, which will be crucial for realizing the full potential of AI in enhancing market efficiency and consumer welfare.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 6058,
    "char_count": 31835,
    "transcript": "Uh thank you so much to the organizers for for having us. Um you know after a lot of papers about existential risk and drone armies and the future of work uh our paper is about how you could use AI to uh get you a good deal on a barbecue grill. Um uh but we kind of we kind of pumped up the title a little bit to make it it sound a little bit bigger. But um you know we can kind of lean on Alfred Marshall who kind of pointed out that you know economists are supposed to be focused on the business of everyday life. So that that's my apology for for what we're about to do. Um Oops. Um let's just start and Anton this is a you know try to address your system prompt. You know even with transformative AI um we're still going to have wants okay um and we're still going to use markets to satisfy those wants or at least a large number of them. Um, and we're probably going to want to do that as efficiently as we can. Um, and you know, we face a lot of these kind of using the market costs in our life. And we try to use labor and uh comparative advantage when we can, but a lot of this ends up being home production, right? Like we're the ones looking up prices. And um, you know, if a lot of these transaction costs are labor costs, we might expect transformative AI to to lower these costs perhaps even and radically. Um, I was uh the the kind of someone mentioning like a a movie that they liked uh last yesterday reminded me of this thing from Kurt Vonagget. I don't if you've ever read this essay where he talks about buying an envelope uh and his wife tells him like why don't you just buy it in bulk then you don't have to keep going to the store and he tells her to hush uh which is a I don't know if that's a good recipe for a happy marriage but his his point about it was hey you know there's some consumption value in in in doing things like this. Um, so I'm going to set that aside and just say, hey, people want to do this as efficiently as they can. They want to use the market uh in this way. Um, you know, you could kind of come up with a list of like what are the costs of using the markets, price discovery, uh, learning what your preferences are, deciding what what things should be in your consideration set, um, if it's something that's kind of delivered over time, you know, you're coming to terms, executing transactions, uh, etc., etc. Um, let's just kind of just talk. Let's say you have some tasks that creates a transaction cost. Um, you know, we're we're going to do a whole sequence of tasks and we're thinking, hey, one maybe AI could could be helpful for this task. Well, um, you know, so if I say, hey, uh, I want to buy a grill. Um, Claude will already kind of do quite a bit for you, asking you about like what are the nature of your preferences, etc., etc. Get some more information. um you know I probably have to decide if the work is acceptable right there's still this human in the loop step um and then if it is we can kind of go on to to the next uh and so we kind of have this process that you know what where where do we think for for say using a market uh AI might make a difference well the first step here is just the efficiency with which we can express our preferences to goals even if they're hyper intelligent we still think we have variation in our preferences is and that task of how we communicate them with the agent so that they can actually use that super intelligence uh is probably non-trivial. Um the capabilities of the agent to actually act on those well you know that's kind of the you know that's the lab's responsibility. Um and then we kind of have this last like when when do we think we're going to need to check which probably depends a lot on model capabilities um but also you know just what the stakes are. Um, and so, you know, I think a kind of a safe prediction here is that as model capabilities improve, more things just kind of move from doing it by hand to to having an agent assist us in this process. Um, the first kind of question if you you think about this is all very abstract, but you first kind of wonder like, well, where's all this going to happen? Um, and there's kind of probably two ways this could happen. So one we can imagine kind of markets reconfigured primarily for for robots and maybe people don't even participate in these kind of markets. Um the other is we kind of take our existing environments um and keep the people and you have sort of agents and humans interacting in in markets. I I our prediction is it'll probably see quite a bit of both. Um, and it'll kind of depend on, um, you know, look, the Whimo approach in some ways of mixing people. It's technically harder. Um, but you have less of a cold start problem. You're kind of building on infrastructure that you already have. You already have APIs, kind of doing things from the ground up. It's a little bit kind of the the mirror image, but I I suspect we would see both. Um, in our paper, we kind of said, well, let's let's think about where demand for these AI agents might might come from. um you know we think that they'll probably use them for the same reason people buy uh use agents now um but the threshold for use will likely to be lower because the costs will be lower and capabilities will be a bit higher. Um kind of the obvious story is substitution where there's something we do now with with agents or it's home production and then we just kind of shift to having an agent do it. Um but I think probably the more interesting and exciting is things where um we don't use agents now but we we might just because their capabilities are so much greater or the costs are so much lower. Um if you had to predict like where this is likely to happen kind of a natural starting point is just that it'll gain traction where it's already happening. um you know just it's not really like there's lots of markets that already use agents um real estate IPOs complex litigation high-end labor markets um but right now I mean this is still kind of rarified air like most people don't use these I mean this room is probably most people here are are pretty well paid and you probably didn't negotiate your contract with an agent um but you might okay um so let me go a little bit faster so we just kind of in the paper we talk about some places where this is likely to happen. Um, high stakes transactions, uh, vast counterparty space and there's evaluation is kind of very expensive where you're having to consider lots of options. Um, and that's going to be not, you know, not cheap to do. Um, okay. Well, where who's going to supply these agents and what what form is that likely to take? Um, we kind of broke it down. We think that there's probably two things that are likely and they'll probably both exist. Uh one you might think of as the bowling shoe agent. So um you know when you go to a bowling alley uh you know they'll give you shoes to wear. Uh or you can have the bring your own agent where you bring your own agent and the platform is just providing interfaces uh for you. So, you know, I I think if you kind of compare what what the kind of the pros and cons of of both um what's nice about the bowling shoe agent is you don't have concerns about being outclassed by someone who's bringing a better agent. So, if you have settings where you're kind of worried about the sophistication of the counterparty, putting everyone on a level playing field is probably pretty attractive. Um but you know some of the the worries you would have an agent that you didn't create and doesn't totally work for you. You kind of worry about it maybe steering you towards certain options. Um you would like literally introduce agency problems um that that might not uh might not otherwise. Now the bring your own agents has some advantages but you you could also take and imagine this creating arms race dynamics where people are kind of you know my agent is slightly better than yours. Uh so it would kind of depend on how you know zero sum uh we think the uh that situation might be. Um beyond that we also might imagine that we'll see horizontal agents which are just sort of general purpose and kind of operate for us and across lots of domains. Or you can imagine more we think of as vertical agents that are hey this is this is an agent that's optimized for helping you find a job or optimized for helping you uh execute real estate transactions. Um like most things I think we probably will ex see both and it will probably even be crossed with our bring your own versus bowling shoe uh type agents. So, you know, you could imagine one where you have a user controlled agent. Um, it's not operated by the platform, but it can kind of work across platforms. And you could kind of fill in the rest of these uh with, you know, what would it look like to have a bring your own but vertical agent or one that the plat it's a platform generalist u but it belongs to you and you know then the kind of the last one filled in. Um, you know, I don't think we have like a strong opinion. It's probably something that will uh exist in and sort of for different markets and sort of for different use cases. Okay. Um so next I'm going to kind of switch to what kind of things have to happen um for this future to exist like what what R&D would occur on the way to this kind of a a transition. Um, you know, I I think we'll make a prediction here that trying to build good economic agents um will become a big topic in economics and computer science. Like so some of these problems, they're not just really about model capabilities. They're kind of at the intersection of of economic questions and technical questions. Um, you know, one is just learning the principles preferences uh efficiently. That's uh not not trivial. Like so how well any of this works is going to depend on how well our agents actually have learned uh what we care about and you know you can easily imagine kind of a worse version of this. So I asked Claude to make a stupid robot that would ask me for my preferences. And you know the the the approach it is it goes through and like asks me you know per fruit what do I think of this fruit right? This would be this would be terrible. Um you can ask 03 for a better version of this and say hey imagine we're trying to set up this optimization problem where I want you to learn my preferences uh efficiently in some sense. What questions would you ask? What inferences would you make along the way? Um, now I have no idea if 03 has set this up correctly or not. I did not check, but you know, this this idea of trying to take and learn our preferences cognizant of the time it would take um seems like an important problem. Um there's going to have to be a lot of uh kind of work on figuring out when do we want AI agents to make choices for us and how much authority do we want to give them because the the big productivity unlocks would come you know not from us kind of checking in at every single point but rather from um doing sort of large things all at once. So you know you even if an agent was kind of instructed with what's the best decision to make right now um you kind of want that agent to be also thinking this question of is this a decision that even I should be making is this something that I need to go back to the principle and and get approval to do um you know and I think that this probably is going to matter a lot because the big unlocks are going to come from doing lots of things together where it doesn't doesn't have to kind of check back with you every every Second, um, if we kind of make a a prediction here, as the capabilities of AI increase is, we might imagine it kind of moving out. So, you know, the simplest thing would be, hey, I'm going to tell you a product and I want you to go go buy it for me. That's kind of the the inner loop all the way out to the kind of extreme kind of like Gary Becker model of the family like I'm really trying to maximize my my welfare and I'm going to like let you kind of make a bunch of decisions um to to to pull that off. But you can kind of imagine that the most extreme shell here requires a much greater knowledge of my preferences and and highly capable agents where the inner one is sort of something you could do right right now. Um, you've probably seen this slide that says a computer can never be held accountable, therefore a computer uh can never make a management decision. I I don't think that that's really true. We we let computers make decisions all the time. It's just that at the top there has to be someone who's ultimately accountable. Um, and you know, I think what what we'll probably see to facilitate that agents making choices for us, you can't kind of get out of jail free by blaming your agent. you're going to have to kind of take responsibility for the things that your your agents do and that will be sort of a precondition of using it like this. Um, another kind of area that I think will be of great interest and it's you know not far away from what labs are trying to do now is is to make them rational in in the sense that that we talk talk about. So if you if you had your agent out participating in the world on your behalf, uh it's going to be very important to you that it's it's rational, that it can't be taken advantage of in some way. Um you know, that you can't, you know, be turned into a money pump or, you know, be kind of tricked by by biases in some sense. Um and you know, kind of and maybe this is related to this, but making them resistant to manipulation because you know, a lot of these things are are going to be adversarial in nature. You know, so if you had an agent where you said, \"Oh, I'd like to eat a hamburger.\" And it said, \"Oh, good news. I found the world's best burger. My search can end now.\" Um, you'd be like, \"You fool. That's not right.\" Um, but you know, even Claude today uh kind of sees through this puffery and is is sort of not not tricked by it. But I I think we'll see a you know, a cat and mouse uh game that this is sort of never going to end of of people trying to trick agents to to do things. So um but you know I think we will we will see a lot of this kind of black hat style manipulation. You hear kind of stories of people trying to jailbreak things or put things in system prompts or something like that. Um I I we we we in the paper don't come down too strongly but these concerns about manipulation probably push towards a controlled environment and more agents that are provided for you by the platform because of you know a rogue agent operating on your platform might be something that the platforms don't don't want to allow. Um then I think that you know there's this kind of improving them over time example and this gets to kind of the preference elicitation thing. I ideally we could minimize the cost of trying to talk to our robots about our preferences if it could watch what we're doing and make inferences about what we like and don't like and kind of adapt over time. And you can kind of even see this now with the models people use, you know, where they're they're it'll say, hey, um, you know, oh, like even like claude code will see like, oh, you know, he doesn't like this and start try to adapt itself uh to to our preferences. Uh so let's just think of you know in a world where lots of people are using these kind of agents you know I mean one thing that agents they could be the best price comparison shoppers uh one could ever imagine. Um, you know, I think there was this kind of mostly unrealized hope that the internet was really going to knock out price dispersion completely. And, you know, there's some some evidence that it did, but uh in in at least in certain markets. Um, you know, one possibility is that, you know, comparison shopping and finding deals, it actually hasn't gotten much cheaper. In fact, the real cost of clipping coupons has has gone up. Um, you know, I probably like a lot of you, you know, you just you just buy food without kind of thinking about the price. You know, you're probably at that that income level, but you probably want if if you your true preference would be to be a good comparison shopper. It's just expensive to to do that. Um, so, you know, kind of the good version of this transaction cost reduction is like lowered search costs. Um, we get matched to products that are a better fit because we've learned preferences. Uh maybe there's less puffery in advertising because our AI agents sort of see right through it. Um maybe bargaining it's it maybe it happens but it's all kind of finding good good kind of uh surplus and good deals. Uh the you know the bad place here would be something I guess I called robot ripoff hell where you know where we're we're kind of locked in a lot of these like adversarial bargaining situations where agents are are uh that the opportunity cost of their time is nothing and so they're happy to bargain and try to chisel you down on things. Um you know we don't know. I mean, but but hopefully it's something we can kind of shape as as these opportunities present themselves, like we can make better versions of of markets. And that's kind of like a a good thing to to line up. Now, one thing that this switch to agency does is it it might allow us to do kinds of market designs that have mostly been theoretical uh just because of the cost they take to implement. So, uh, there's this a great great paper about kind of the disappearance of auctions on eBay. And, you know, the the punchline of the story is people used to use a lot of auctions. They're great for price discovery, but like people don't have the time to to, you know, have like a three-day auction over a toaster, right? Like it's just like the human component of the mechanism ends up mattering a lot. Um but our agents that have zero opportunity cost of time could could do uh a tremendous amount of participation in like price elicitation things. So you know you could have previously impractical market designs might now be be possible. Um, you know, and so this is like kind of silly, but you know, you could imagine a world not too far in the future where if you said, \"I was looking for toothpaste.\" Um, you know, your AI agent could be launching a complex search to find out exactly what's the best thing for you and consider lots and lots of prices. If the marginal cost of that kind of effort is close to zero, like that's the kind of thing that that could happen. um you know then I think there will be this big question of whether or not we allow or whether you see a lot of agent human versus just agent agent interactions. Um, you know, there's a whole bunch of startups now that are doing things in like the AI interviewing space and I think companies are like, \"Oh, this is great.\" You know, we can have a thousand people being interviewed by our AI agents, but I don't think they're solving for the equilibrium where, you know, would you want to do a job interview where you knew you were just, you know, one of a thousand? And and generally, you know, we know negotiation, you don't want to be negotiating with someone who has zero opportunity costs of their their time. Um so you know I think in these situations we might see markets kind of move to like purely agent to sidestep um come to that mismatch in opportunity costs. Um you know one of the things you might also think about is well what does this do to quality or uh kind of knowledge that gets generated? Um I I sometimes think that a lot of modern electronic commerce, you know, kind of like has this uh you know, this big superructure rests on this review system that, you know, we all know kind of has all these problems uh but is ends up being essential uh where yeah, even if it's prone to manipulation and bias and and all sorts of issues, we still kind of depend pretty heavily on people participating and saying, \"Hey, I like this product. I didn't like this product.\" Um, one could imagine potentially if you could work out privacy issues, uh, agentdriven reputation systems where, you know, agents can verify that a purchase happened and sort of report back to the center how much utility it actually generated. Uh, this would definitely fall into the bucket of like market designs that you would need to think a lot about. Um, but it may may kind of offer some some real opportunities. And concluding thoughts, um, we'll only be buying envelopes because we want to. Uh, you know, I I would also flag, you know, I think we have a lot of agency over what the future looks like. Um, and and you know, we can kind of think about what how we might want this to work and what kind of research we we might do. I think economists, we tend to think of ourselves a bit like astronomers and that we kind of observe the world or maybe it's just through the policy uh channel. But for a lot of this stuff, like there's research that we could do about like how would you design a new reputation system? How would you design new new a new market design that would take advantage of some of these capabilities? So I think it's a very uh exciting time for economists and I'm at time. Great. Uh well, first of all, I'd like to thank the organizers. This has been super interesting and I'd like to thank everyone who is here. Uh there are a lot of people here for the last talk and you mostly seem awake so that is great. Um and uh as you can see uh the talk uh that I am discussing was a real tour to force. So in 15 pages they managed to have like thousands of so it's tough to answer everything. Um so I'm going to kind of take a higher level view and focus in on kind of three different bundles of ideas that I think came out in that talk and try to address them. And the the first kind of section that uh John was discussing is about what is going to uh be the first stages in which uh agents are going to kind of move in and are going to be uh uh demand it uh and exist. And the way I kind of think about it is a little different than the way that they uh discussed it. And I like to think of these kind of three stages of innovation and implementation. Uh the first being augmentation um and a lot of us um obviously Eric and others have done a lot of great work in thinking okay here are tasks that people do um let's see how this changes when you add AI into that task that people go faster similar quality or they increase the quality and take the same amount of time things like that uh think of this as kind of stage one think of kind of stage two interventions when people start recalibrating how much time they spend on various tasks adding in new tasks um because of the productivity gains or not they have in various tasks. Um there is a lot of interesting work going on here. This is a lot harder. It's harder to do in in a in a lab setting. Uh it's less clean even in a field setting. I have some work forthcoming in which we're working with salespeople inside of Microsoft and external to try to see how they re-calibrate their time once they start utilizing uh various LM tools for some things, but it's it's messy business. That being said, uh, stage three is the really exciting part, which is where people stop augmenting or recalibrating inside the same workflow, uh, and create new workflows that are specifically designed to maximize the impact of AI. And this is where the hockey stick comes in. The idea that productivity gains are are more modest in this first two stages, but they really shoot up in this third stage. And I think kind of the conceit of this workshop is to be thinking technological systems that need to be in place have been solved. We are in that third stage and what's going to start happening. And when I think about uh buying goods and services uh some of this B TOC stuff as well as a lot of B2B and even B2E and kind of internal enterprise I think a lot about how in that third stage it's an AI first world. And so, uh, you're going to have businesses that have moved from brick andmortar into websites are now going to be agent first. You're going to have a lot of things that are sold, goods and services that couldn't exist in a previous regime. So, Stiglets talked about in the first day news and how it's going to be affected. Implicitly, what he was talking about is rather than selling news directly, there's going to be snippets of news are going to run through a marketplace and show up into the AI. uh this is something that couldn't a human couldn't do because it involves unbundling and microtransactions uh that happen at scale and so whereas John was talking about the kind of first hits being for these really high stakes places where agents are already in that's kind of stage one stage two thinking and stage thinking stage three thinking that's kind of more like high quantity maybe even low stakes uh where you're going to have new things that couldn't have existed uh very easily and really in an AI first environment And so if I carry that forward into kind of the second trunch of stuff that John was talking about, uh he was talking about this kind of the bowling shoes uh uh agents versus the bring your own agents. I look at it slightly different. When I think of the bring your own agents, I think in terms of these are consumer agents that are designed to work for the consumer. So they could be subscription based, there could be ads coming in, um but they're technically at this point kas. They are computer use agents and what that means is that they are going out there into a human-based world. Uh they're reading websites. They're talking to humans. They're doing things that are mimicking humans. So there's number one a lot of noise and number two there's a massive incentive issue which is that businesses hate them because they are trying to read the business's website and basically construct the business from that rather than talking to the business. uh they're taking the agency away from the business and so this is a tough thing to consider as a primary means of agents in a longer term equilibrium again stage three thinking and similarly when I think of the bowling shoe example this is a siloed agent so this is an agent that works for a company uh and it's pretty clear and I think John mentioned it I mean obviously there's a bad incentive situation here is that's like going to Amazon and saying asking you to buy stuff for them and you're like, well, Amazon's not going to give me the cheapest or or best stuff. It's going to give you the best thing for Amazon because that's the incentive. It's it's basically saying having a private shopper who works for the company and is not incentivized to your best interest. And so I see it is that this kind of really in the kind of further stages really you end up with assistant agents that are somehow incentivized to work for the consumer and you have service agents that are incentivized to work for the business. Um, and when you have that, you're going to then have uh agents that are, you should be thinking in terms of they are marketplace agents. They're built not for a human to uh um business. Uh they're not built for, you know, um an agent reading a website. They're not built for uh um an agent uh talking to a human that's owned by the business. They are built for this marketplace and and and such. they will be able to take care of a lot of the concerns that have been voiced regarding kind of general agents because they're built for this scenario. They're provided with uh the the necessary protections and uh the necessary entitlements so that they don't go crazy in this scenario um that was brought up earlier on. But more important um to kind of jump into is that it changes you from an attention economy to a preference economy. And when you move from an attention economy to a preference economy, this obviously affects ads. This affects basically how monetization happens. And there's a ton of questions up in the air on how that functions. And uh again, I think John noted this, but I really want to dig into the fact that uh you know, he mentioned that well maybe that means we're going to end up in more closed off worlds. Okay, that's possible. What are more closed off worlds means? That means we're in more like Wal Gardens. uh because walled gardens have certain things that are uh very advantageous right now. Number one is that they lower the risks uh that everyone's been talking about of things going rogue. You can white label the agents on both sides in that. Um but of course uh when you do that and it's also I should say it's it's the status quo, right? So a huge amount of digital time and money is spent in walled gardens right now. Um but it's going to cut off innovation uh and it's going to ensure that higher welfare gains go to the owners of these walled gardens than go to say the business and consumers that can happen uh if you have a more open uh scenario. So thinking in terms of this walled garden thing um I like to kind of going back to a lot of the conversations we've had. You have this question about processing power, data and communication friction. Wald gardens right now they have dominance in all of these things. They have all the data coming in centrally. Consumers can't really capitalize on it and small businesses can't. They have a mount amount of processing power that allows them to make predictions that are much more effective than any of these smaller uh entities. And they also have huge barriers when it comes to communication frictions. artificial some of them but some of them real in the world prior to LLMs that help maintain these massive walled gardens. My personal view and prediction is that for at least in the B TOC space that these processing power differences and the data differences will be diminished. They'll become more commoditized especially as people's own consumer agents their assistant agents can collect a lot of the data. They have a lot of processing power and the communication friction, the ability for these walled gardens to be able to maintain artificial communication frictions will be the key thing in whether or not we end up in a more walled garden scenario or a more open web scenario. And this brings me to the to the third kind of section of John's talk, which is focusing on well, what do we need to do in order to end up in the good place? Let's say the good place is more open and there's more competition and welfare is more distributed and there's more innovation versus in a more welfare uh uh constrained place in this walled gardens. Well, it's a lot of the stuff that John was talking about. The you need agents that are extremely efficient. You need ones that have very controlled entitlements, so they're constrained in what they can do. And you need a lot of work in protecting the types of risks that happen with both attacks uh and optimization strategies. And now of course a lot of these things you already face in regular markets. Um and it should seem as as kind of coming to a similar place and John in the end of the day that a lot of these are technical questions but a lot of these are questions that economists and other social scientists need to answer. So under thinking about quality, trust and verification, thinking about as John was saying out of the box ways of building quality measures because with agents you're not constrained by just having people put five stars. There's a lot of information that flows in these systems that can create quality, trust, and verification. And it's up to economists and social scientists to develop those tools that make a more open environment uh palatable uh so that customers and businesses demand to leave the safer walled garden space into a more equitable open space. And with that, I'm at time. Thank you. [Applause]"
  },
  {
    "id": "PB7zI4Xh_So",
    "title": "How Much Should We Spend to Reduce A.I.'s Catastrophic Risks?",
    "url": "https://www.youtube.com/watch?v=PB7zI4Xh_So",
    "presenters": [
      {
        "name": "Charles I. Jones",
        "affiliation": "Stanford University and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=aEovwtUAAAAJ"
      }
    ],
    "num_presenters": 1,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nCharles I. Jones, Stanford University and NBER\nHow Much Should We Spend to Reduce A.I.'s Existential Risk?\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "In his presentation at the NBER Economics of Transformative AI Workshop, Charles I. Jones explores the critical question of how much society should invest to mitigate the existential risks posed by advanced artificial intelligence (AI). Jones highlights the dual nature of these risks: one stemming from potential misuse by malicious actors and the other from the unpredictable consequences of superintelligent AI systems. He emphasizes that the rapid advancements in AI, while promising significant economic growth, also carry severe risks that could threaten humanity's existence. \n\nJones presents a framework for evaluating the necessary investment in risk mitigation, drawing parallels with societal responses to past crises, such as the COVID-19 pandemic. He argues that if society was willing to spend substantial resourcesâ€”approximately 4% of GDPâ€”to mitigate a 0.3% mortality risk from COVID-19, it stands to reason that a similar or greater commitment should be made to address the potential risks from AI, which many experts believe could be at least as severe. His findings suggest that individuals might be willing to accept a significant chance of catastrophic outcomes in exchange for the potential benefits of transformative AI, indicating a need for a reevaluation of risk tolerance and investment strategies in AI safety measures.\n\nThe implications of Jones's research are profound for both economists and policymakers. He underscores the necessity for a robust investment in AI risk mitigation, arguing that the potential value of life far exceeds the costs associated with such investments. By framing the discussion around the value of life and future generations, he advocates for a proactive approach to AI governance that prioritizes safety and the long-term survival of humanity. Jones's work serves as a call to action for increased funding and strategic planning to navigate the complex landscape of AI development responsibly.",
    "upload_date": "2025-09-29",
    "days_ago": 1,
    "has_transcript": true,
    "word_count": 5442,
    "char_count": 29009,
    "transcript": "All right, wonderful. So, um I thought this morning was surprisingly pessimistic. Um not for the reasons I'm pessimistic. Pascal, I thought, you know, gave us some some optimism. So, uh now I want to bring us down even further, I guess. Um uh so, so, so we've we've had this amazing progress in AI. I think it's it's breathtaking to see. I think it gives rise to lots of reasons for for optimism. Um, importantly, the people who founded these labs, the people who got the Nobel prizes for for these innovations, these are and the people who have lots of money writing on this, these are people who certainly originally and seemingly even now are very worried about the downside risks, much worse than unemployment, but you know, the the AI somehow, you know, leading to very bad existential outcomes. And so um what I've done in in this paper and an earlier paper is ask well can we use standard economic analysis to to tell us about this um when you think about existential risk I think they they kind of come in two flavors. One is the bad actor flavor. So could some future terrorist use you know chat GPT8 to cause harm design a new virus that's really lethal uh takes 3 weeks for symptoms etc etc. Um, if ChachiP8 is this amazing oracle that can answer any question, that can, you know, create new ideas, that can do anything a human can do, this seems possible. Um, and nuclear weapons were manageable because they were rare, right? We had two people to negotiate with and we barely got by, right? The Cuban missile crisis or, you know, it was a close call. But if every person, if all eight billion people had a red button that, you know, anyone could push and do enormous damage, that just seems like it's it's not a very uh not a very good world. Um, the second version, more science fictiony, is alien intelligence. So, um, uh, one way I like to think about this is, you know, suppose this afternoon we found out there's a spaceship on its way to Earth from Pluto. How do we feel about that? I'd be 90% excited, but maybe I'd be 10% worried because we kind of know when, you know, we encounter the ants, it doesn't always end well for the ants or something, right? Um, there's this this computer science professor at Berkeley who's the co-author of one of the leading machine learning textbooks. Um, I was on a panel with him and he had a quote that I liked a lot, which was, \"How do we have power over entities more powerful than us forever?\" And that's a nice way of putting it that I think leads to disturbing thoughts. But I I think we should be thinking a lot more about this um than we have in this room for the last two days and then even for the the previous pre-conference. Um so what I'm going to do talk briefly about my earlier paper and then the new project is how much should we be spending to mitigate these possible extreme downside risks. And you know, not to hide the ball, I think even a selfish perspective, forget future generations just to prevent me from dying. Um, I'm willing to invest a surprisingly large amount. And so, uh, uh, I'll come back to that. Um, okay. So, so the earlier paper was was built around a thought experiment. So, so you know, Sya, Nadella, Darede, you know, transformative AI may be more important than electricity, but more dangerous than nuclear weapons. So, how do you trade off these two things? And I thought about this as as the Oppenheimer question. So, you remember the the the movie or or the book, you know, we're getting ready to test the Trinity Project and then the physicists say, \"Ah, what's the chance that this chain reaction gets out of control, ignites the atmosphere and kills us all.\" They do the calculation and compute conclude, uh, it's pretty small. And so, we we test it. But, you know, pretty small is not zero. So, how much would you trade? So, so here in the AI context, if nothing goes wrong, let's assume that that transformative AI accelerates growth to 10% per year. To totally amazing, right? But it comes with a one-time small chance that AI kills everyone on Earth. What risk are you willing to take? And you know, you can think about your risk you're willing to take, but I ask what what risk do our standard economic models say we should be willing to take? Um 10%, 1%, uh do do you develop it or not? Um, so so three surprising findings. First, with log utility, you're willing to take a one in three chance of killing everyone to get 10% growth. And I macroeconomists love log utility. I used to think I was a log utility person. I'm not a log utility person, right? There's no way I'd do that. Interestingly, entrepreneurs, Sam Alman, may be much less riskaverse. That's why they go into entrepreneurship. Maybe that's why they're willing to to take to take risks. Um interestingly ne next point the value of life is very nonlinear in this risk aversion parameter. So log utilities risk aversion one if risk aversions two that risk cutoff plummets from one and three to 2%. If risk aversions three it goes to half a percent. So so the intuition here is there's very sharp diminishing returns to to to consumption. Margin utility falls really quickly. And so, you know, with gam equals 2 or gam equals 3, you know, utilities bounded, I don't need a fourth flat screen TV or a third iPhone, I need more days of life to enjoy, you know, the beautiful Stanford campus, right? And so, I don't want to die. I don't care about 10% growth because that's running into sharp diminishing returns. I care about not dying. Then the last point recognizes sort of builds on that says okay if we get the 10% growth we're probably going to also cure cancer and cure heart disease and have all these great medical innovations as well. Well then interestingly now you're trading off lives versus lives rather than lives versus consumption. So I care about not dying. I don't care about what kills me. It's the cancer or the AI. But if the AI reduces the cancer, I'm willing to take more risk from the AI. Turns out even with gam equals 3, the calculation says I should be willing to take a one in4 chance of killing everyone to cut mortality rates in half. So, so um yeah, lives versus lives versus instead of lives versus consumption makes makes a big difference. Okay. But of course, I don't want to take any risk. If there's something I could do about the 25%, maybe I should be doing it. And that's kind of what the the second paper's about. So, so I kind of knew I you know I I I knew that if you take future generations into account there tens of billions of future people we can make an investment today. Okay, we should invest a lot that you know and I didn't think that was I thought that was obvious enough that I wasn't going to write that paper. What led me to write the paper was when I realized oh we've actually been through something like this analogous to this very recently and that was CO 19. So with co 19 we faced a mortality risk of 0.3%. We shut down the economy kind kind of willingly more or less a lot of people stopped going out. Um we spent 4% of GDP to mitigate a risk of 0.3%. Well I suspect the AI risk is at least this large. If you ask a bunch of AI people in San Francisco they definitely think it's at least this large. So we should be w willing maybe to spend at least this much by revealed preference. So maybe we're massively underinvesting in mitigating the risk. Again, purely from a selfish standpoint. Forget future generations. Just we don't want to die. Let's let's avoid it. Now you might ask, well was co did we do the right thing there? Probably not. But if you do the calculation, we would have been willing to spend e even more. And the way that works is um the value of life that the EPA or the Department of Transportation, the US government uses is $10 million per life. Okay? To avoid a mortality risk of 1%, your willingness to pay is 1% of $10 million or $100,000. GDP per person is only $90,000. So this is more than a 100% of a year's per capita GDP. And if you thought, okay, maybe the the risk it it occurs sometime in the next two decades. So we have 20 years of GDP that we could we could invest. Okay, you could justify investing an annual amount of 5% of GDP every year for 20 years if that would allow you to completely avoid the risk. Now, we we can't be sure that would allow us to completely avoid. So it's it's more complicated than that. But this definitely gives the intuition. Life is just incredibly valuable. So, we're willing to spend surprisingly large amounts to avoid risks. So, so what's missing here is how effective is the spending. So, and that's kind of why there's a paper rather than a blog post or something because you have to do some calculations to to handle the fact that we don't really know how effective it is. Okay. So, that that leads to the model. Um so, so really simple setup. There's this one-time existential risk probability delta you kill everyone. um delta is a decreasing function of the mitigation investment X. So again, one-time investment in mitigation X. How much do you want to spend to reduce that that probability? There's an in exogenous endowment of income per person YT. And you could imagine that grows really rapidly because of AI. Turns out I don't even use that fact. So you know there's some endowment. It is what it is. I'm going to use a value of life of $10 million is the default. Um but if you know AI growing growing rapidly raising growth rates could could raise the value of life. So the the mitigation problem is just choose X to maximize the utility from consumption today and then the expected utility from not dying the 1 minus delta X beta times the future utils you're going to get the big V. And the big V is just the the add up all the utils from consuming your endowment because the X is only a one-time thing. So, so you only you only spend X once and then you consume your endowment forever after that. So, you can see it's it's a really simple problem. The first order condition is the usual thing. You're trading off the margin utility of another dollar of consumption today versus the the reduction in the risk that happens multiplied by the value of life um the beta times V of T plus one all the future utles you could you could get if you don't die. So you can rewrite this first order condition to get a more intuitive uh expression by letting s be the fraction of your endowment. Sorry, the fraction of your endowment you invest in mitigation and let ADA be the elasticity of that delta function. And since delta's decreasing, I'll take the negative. So it's a positive number. So um and then you get the expression in the box here. S over 1 minus S. And think of one S is pretty close to zero. So this is roughly S. S is the product of three terms and start at the end. The last term is the value of life term. So V is the value of your future life in utles. You divide by the margin utility consumption to put it in consumption units. So V over U prime is like the $10 million. And you take it as a ratio to per capita consumption, which might be $50,000. And that would give you a a the rest of your life is worth 200 times annual per capita consumption. It's the C is actually a little bigger. So it's more like 180. Okay. But you know your your life is worth 180 years of per capita consumption. So what's the risk to be mitigated the delta? Let me just guess 1% to start with. Well that would say you know you're willing to pay 180 years of consumption. No only 1.8 you know one only 1% of that. So 1.8 eight years of consumption. Um, and so that's bigger than 100% of GDP. And then you got to multiply by this elasticity parameter, which I really don't know what that number is, right? Um, how effective is the spending? It seems likely to me that it's much bigger than 0.01, right? So if you spend 1% more on mitigation, does the risk fall by one percentage or, you know, point, you know, one basis point, right? 0.01. uh in all my in many of the simulations it's going to be in a lot bigger than that. So this didn't seem crazy to me. Um if you multiply those numbers out you get you know 180 times per capita consumption 1% of that so that's 1.8 years worth and then 1% of that is 1.8% of per capita consumption. So that's you're going to see lots of numbers in a second. They're mostly bigger than this. it's this kind of calculation where they come from and yeah I'll I'll consider different values for kind of all these different parameters but that that's the basic calculation uh in the paper um okay to implement this I need to some functional forms and some parameters so what are the parameters here so delta 0 is the risk without mitigation I'll think of that as 1% to start with and again consider a wide range of values fee is what share of the risk could be eliminated ated um by spending maybe some of the risk you just can't do anything about. So one minus fee is what you can't do anything about. So 1 minus f delta 0 we're stuck with that but the fee part maybe we can do something about alpha there's this exponential decay as you spend more the risk goes down exponentially um and n is the number of people there's this non-rival aspect to it uh the more we spend the the more the risk goes down to calibrate alpha it's easier to do it in the following way think about this this um C parameter as what share of risk would be eliminated by spending 100% of GD GDP. So if spend one year's GDP um what share the risk goes away? That's that's uh the way I'm going to think about the parameter. Um and T is how many years do we have? Is it 20 years? Is it 10 years? Is it five years? Um how long is that first period where we can do something about it. Um okay. So so here are the baseline values. I'll let fee be a half and C be a half and delta zero be 1%. You saw the value of life 10 years. Risk aversion two. um 1% discounting per year but you know t years so 10 years while while we're doing it you you might worry about these parameter values what it basically boils down to is spending a 100% of one year's GDP reduces the risk from 1% to 75%. I would hope spending a year's worth of GDP would do more than that, but you know, so this this is the the the baseline number. And then I'll do Monte Carlo simulations across, you know, the different parameter values you see here. Letting delta go from 0 to two. A lot of these these AI people think it's, you know, 10% or 50% or 90%. Um 0 to two seems seems reasonable to me. Um uh yeah, what share could be eliminated? 0ero to one. What how effective is the spending? 0 to one, value of life, time apparel, etc. Okay. So, so here's a set of calculations. So, when you do the baseline, it turns out you want to spend 16% of GDP every year. So, for these baseline parameter values, 16% of GDP is is what you're you would optimally spend. Um, you know, cut the risk in half. That cuts the spending roughly in half. Let the risk be twice as large. you're more effective, less effective. You see, no matter what I do here, I'm getting numbers bigger than 5% of GDP. And it really comes from that value of life thing that I told you from the beginning. Life is just valuable. So don't don't lose it if you can help it. Um there's um there there it's going to turn out to be the case and this is kind of interesting that when I do the Monte Carlos, in a third of the cases, you don't want to spend anything. And where that's coming from, you saw I had this exponential form. There's no anata condition at the bottom. So, so you don't want to invest. Don't spend anything on investment if u prime of y0 is bigger than delta prime of zero. My delta prime of 0 is not infinite. So, if you if you had an anotic condition here, you'd always spend a positive amount. So, I again I kind of rigged it so that you know in favor of maybe we don't want to spend here. So, in a third of the simulations I'm going to do with the Monte Carlos, you don't want to spend anything. And yet, you know, uh it it's still going to be valuable to spend a lot on average. And you can you can do a calculation here. It's basically what's the willingness to pay and then how effective is the spending. And if it's just not very effective or you can't do much about the risk that's out there, you're just not going to spend anything. So, uh that's just intuition for the result that's coming on on the next slide. So, here's a million, you know, simulations across these uniform distributions. And again, you can see a third of the time it's optimal to spend nothing for a third of those parameter values. The median amount you want to spend is 6%. The mean is 8% and twothirds of the runs have you spending more than 1% of GDP. And you can see it, you know, for some parameter values it could be really high. So I do value the future. So I have a case where I value the future and I have a case where delta 0 is 0 to 10% instead of 0 to 2%. that just raises the numbers e even further. Um so so in in in my last two minutes, let me just offer some some concluding thoughts. So so what do I take away from this exercise? Well, the first thing I take away, you know, I I probably don't want to spend 16% of GDP on mitigation right now. I'm not sure what we do with it, but it does seem to me entirely straightforward to justify spending something like a third of 1% of GDP, right? It's it's very hard to get spending less than that to be optimal here. That's a hundred billion dollars. Rough estimates suggest we're currently spending about a billion dollars or less on AI safety. So, we're probably underinvesting by more than a factor of a hundred by my calculation. And this seems like a tremendous mistake to me. Um, next point. You might ask, well, what would we do with this hundred billion dollars? Are there really hundred billion worth of things to do? I I think there probably are. So, first slow down. Just don't go as fast, right? And while you're slowing down, invest in safety. Give us more time, more of a runway to make sure we do this safely. And you know that that would be very helpful. Second, when you look at how uh Google deep mind started, they first did these very narrow things. So, from the the first paper, remember medical innovations are incredibly valuable. Well, you know, alpha fold, protein folding, you know, that could still kill us all, but it's much less likely to kill us all. So, let's do a lot more of the narrow AI, particularly in medical research, and go slower on the other stuff. And while we're going slow, invest in safety research. It seems like something along those lines is definitely in our interest. Another thing that really jumps out at you, you know, it's it's interesting that Dario Ammedday, Sam Alman, Jeff Hinton, um these people were very concerned about safety. OpenAI was founded because of safety, anthropic founded because of safety, and yet we're in the middle of this race where people are racing faster and faster. I think it's a classic, you know, patent race dynamic where each each of the racers says, \"Look, whether I race or not, if we're going to die, we're going to die. So I may as well race because after all I'm probably safer and if we don't die I get to be the most famous person who ever lived and that I value. Right? So so I think there's this real big negative externality and there's a question of how we get the labs to internalize these externalities. I think a policy I find intriguing and worth considering is tax GPUs. And I don't mean a 10% tax. Maybe it's a 10x tax. I don't know, some huge tax to slow them down and use the revenue to fund revenue to fund safety research. And this has the advantage, by the way, that you know, when you're thinking about China, to the extent that China uses our chips, which is less and less true every day, I guess, um, but this sort of taxing GPUs would be a way of slowing slowing everyone down. So, um, bottom line, I think this is a a problem that, you know, hopefully it doesn't happen. most probably maybe it doesn't happen but it does seem like even small probabilities make it incredibly worthwhile to think much more carefully about it. So thank you. [Applause] So on that cheery note um you can yeah I bring my slides up. Thanks. Um so first just to remind you how happy AI is. I wrote my discussion, submitted it this summer, and then I just uploaded it to Gamma AI, and I didn't even tweak the slides that Gamma AI provided for me. So, these amusing, cheesy pictures are are kind of fun. Um, okay, great. But it did a pretty good job. So, here we go. Um, so yeah. Wow. So um I think the core argument here and I'm really going to focus on the 2025 paper because that's a conference volume paper. Um there's a sense in which like the the numbers we can talk about but like the core takeaway you kind of have to agree with right which is if there's a substantial probability which a lot of experts seem to think that we're all going to die like maybe we should be doing something to mitigate that risk right um and so I what I do want to spend a little time on is you know So Chad basically is going to calibrate this using a federal value of a statistical life calibration, right? So how much money we spend to make cars safer or airplanes safer or the workplace safer is going to be um extrapolated essentially to thinking about how much we should spend on this ex existential AI risk. And Chad um alluded to this a little bit in his last slide, but I want to talk a little bit about two elements of that, which is first he doesn't really spend much time um talking about who exactly is going to spend this money. I think it's implicit that you know he's using US GDP to talk about the US value of a statistical life. So he's thinking about the US government, but we got the whole rest of the world out there to think about. Yeah. You guys are getting the benefit. Yeah. Okay. Well, to discuss. Okay. And then you know I think the other thing and this is really the philosophical challenge is should we view even forgetting about future generations. Should we view does society actually in the data in a sense in our lived experience view extinction risk as the sum of the end of every individual life. Right? If you're bootstrapping off of the value of the statistical life, it's just everybody dying is the sum of every individual dying. Um, and so I want to think about both of those things a little bit. And so one of the things I thought about and this has come up a couple times today is you know it's the value of a statistical life which is and COVID are both situations in which both the spending you do to mitigate risk is somewhat appropriable right to either the individual who stays home or the nation state at least who's doing the the the spending. And that's not necessarily true for this AI risk if I'm going to blow up the whole world. If the AIs are going to blow up the whole world and make the whole planet extinct and it's not true for asteroid risk, right? So I decided, you know, we've talked about asteroid risk. Asteroid risk is the real deal. So I want to think about asteroid risk because I think it's in some sense a better analogy than what we spend on auto or airplane safety because one it has extinction potential which you know a faulty car really doesn't have. Two it has global impact and therefore all of these kind of externality impact issues. And then three, maybe the way we're going to solve it could be something around research like research on AI safety. Many people might also argue that climate risk is uh existential risk. It sure is. I think the difference here is that unlike climate risk, there aren't actually waring economic groups with an incentive to think differently about what the risks are. You know, in surveys of the United States population, 91% of people think NASA should be doing asteroid uh risk mitigation, and 60% of Americans think it should be NASA's top priority. So unlike climate stuff, no one seems to want to be obliterated by an asteroid, right? So it's incredibly popular, right? So, it's kind of the best case for our willingness to spend on extinction level risk because it's incredibly popular and it's kind of cool, right? So, what are the risk uh probabilities? Uh NASA tells us a little bit about that for a local event which is like wipe out San Francisco and maybe cause tsunamis with like unanticipated effects elsewhere. That's like a one in 20,000 annual event. um an event which NASA says causes the probable collapse of civilization. That's one in 700,000. And then maybe just to kind of fix ideas, remember we're observing asteroids all the time. And in 2024 there was an asteroid that was a city killer size that um at its peak NASA was estimating had a 3% chance of a collision with the US. Right? So those estimates are varying over time. So these are smaller than what the estimates that Chads are working with, but it's certainly in the range of expert opinion, right, about what the AI risk is. And we can multiply. Okay, so we do spend money on this. That's the whole point. Um, and so the double asteroid redirection test is the main way that the US spends money. And I'm not going to go into detail, but you know, we should we are pretty grim. We're feeling pretty grim after Chad's talk, but you know, in 2022, we did successfully demonstrate an asteroid deflection. Over the last 10 years, the investments we have made have probably massively decreased our asteroid risk. Okay. So, I do some handwaving to make that I do use real data and real numbers on how much we spend. There is some handwaving about the probabilities. I would say there's less handwaving about the probabilities than Chad's doing. So, I'm feeling comfortable about this. Um, but over the 2015 to 2024 period, we spent two billion dollars on this project with a with an ongoing annual budget of 300 million. I think it's plausible to say based on what I've read that in my astronomy grad student that I had to hire to help me with this um a plausible permanent having of the probability of a devastating asteroid effect. Right? So that means I could take those numbers and back out what the value of a statistical life we're using to fund this program is. And it's like.75 million far below the benchmark of 7.5 million that Chad uses for the similar time period. Right? So we this is incredibly popular and we don't spend anything like what we spend on airplane safety or car safety. Again maybe because it has this issues. I just want to briefly note that the US spends money on this. Europe actually spends money on this. GDP per capita in Europe is about half the US and they actually spend about half of what we do which kind of tracks that as a fraction of you know GDP they're doing about the same thing that we're doing now I should have said this but this is dollars per American life and dollars per European life there's a whole rest of the planet which is both not spending anything but stands to benefit um from these expenditures Um, you know, quickly, uh, the ESA, again, this is very popular and the ESA is very proud of the money that they're spending on this project. Um, and that's a t-shirt from the ESA. That's a one slide I actually added uh over the GPT. Um, so what are the takeaways and what does this tell us about AI, right? So I think AI risk mitigation faces even greater international coordination challenges than asteroid defense. First of all, and Chad alluded to this at the end, AI development carries significant immediate economic benefits creating free riding temptations across countries. Right? If we were to slow down as much as Chad is suggesting, you know, there's this strategic behavior, right? We can't necessarily tax maybe we can tax chips and if we're exporting them, right? But we can't any individual country can't necessarily tax away incentives in other countries. And there's some really aberrant, you know, incentives here. Um, and I think this government uh cooperation is nent and controversial compared to space cooperation. So what do I think is the path forward? You know, spending at Chad's contemplated levels requires a consumption sacrifice and political will well beyond what we've demonstrated with asteroids. Again, despite the fact that asteroids are actually a pretty popular category to spend on, what are some things I think that means we should have as a research priority going forward? I think we really need to think about what does a global coordination system look like? There are actually surprising between the US and Europe cooperation mechanisms with regard to asteroids. So we have um secunded workers to the Hera mission that the ESA is is doing for example the US government. What does that look like? Like how do we think about some kind of international governance? What is a framework for eliciting AI safety spending? And I think this is the challenge that makes the challenge he's already posed even more formidable. So thanks [Applause]"
  },
  {
    "id": "G5teYbovYJ0",
    "title": "Artificial Intelligence in Research and Development",
    "url": "https://www.youtube.com/watch?v=G5teYbovYJ0",
    "presenters": [
      {
        "name": "Iain M. Cockburn",
        "affiliation": "Boston University and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=xMVq8WQAAAAJ"
      },
      {
        "name": "Rebecca Henderson",
        "affiliation": "Harvard University and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=CNl0aI0AAAAJ"
      },
      {
        "name": "Scott Stern",
        "affiliation": "Massachusetts Institute of Technology and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=OG1wD-AAAAAJ"
      }
    ],
    "num_presenters": 3,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nBenjamin Jones, Northwestern University and NBER\nArtificial Intelligence in Research and Development\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The presentation titled \"Artificial Intelligence in Research and Development\" explores the impact of transformative AI on the research and development (R&D) process, particularly focusing on how AI can enhance the production of knowledge and ideas. The central research question investigates how the integration of AI capabilities can change the marginal returns to intelligence within the R&D framework. The presenter emphasizes the need to consider not only the share of tasks that AI can perform but also the quality of AI's performance in those tasks, positing that high levels of intelligence may not always lead to proportional improvements in outcomes due to potential bottlenecks in the research process.\n\nKey findings highlight the importance of three critical factors: the fraction of tasks performed by AI (gamma), the productivity of AI in those tasks (CAPM), and the strength of bottlenecks in the research process (theta). The presenter argues that understanding these parameters is essential for assessing the potential transformative effects of AI on various outcomes, such as health advancements or firm productivity. The framework developed in the presentation aims to provide a conceptual foundation for measuring the relationship between AI capabilities and R&D success, suggesting that as AI capabilities improve, the marginal returns to intelligence in R&D could also increase, enabling faster progress in achieving desired outcomes.\n\nThe implications of this research are significant for both economists and policymakers. By framing AI's role in R&D through a microeconomic lens, the findings encourage a re-evaluation of investment strategies in AI technologies and human capital. Policymakers may need to consider how to foster an environment that optimally integrates AI into research processes while addressing potential bottlenecks to maximize the benefits of transformative AI. The insights gained could inform future research agendas, funding decisions, and regulatory frameworks aimed at harnessing AI for societal advancements.",
    "upload_date": "2025-09-29",
    "days_ago": null,
    "has_transcript": true,
    "word_count": 7581,
    "char_count": 39606,
    "transcript": "It's a pleasure to be here. Thanks for having me. I'm really sorry I missed yesterday. I was teaching in Chicago so I couldn't get here until uh evening time. Uh and but I'm glad to be here today. Uh and okay, so this paper is trying to think about what AI might do or transformative AI ultimately might do uh through its work in the R&D or knowledge ideas uh production function. Um, and I'm kind of was motivated uh from this essay in part which inspired this January meeting that got me thinking along these lines and interested in writing something uh just to read it out loud. I believe that in the AI age we should be talking about the marginal returns to intelligence which I thought was an interesting sort of concept uh and trying to figure out what the other factors are that are complimentary to intelligence and that become limiting factors when intelligence is very high. We are not used to thinking in this way to asking how much does being smarter help with this task on what time scale. But it seems like the right way to conceptualize a world with very powerful AI where I think powerful AI also sort of means these servers full of you know cheap genius millions of cheap geniuses running in parallel. Okay. So with that kind of remmit, I wanted to think about how we might you know start to think about how AI capabilities particularly around intelligence for example might or might not lead to um massive change in our ability to progress various kinds of outcomes and naturally I went to a task model to think about this. Um so you know we can imagine there's some automated task and some labor tasks and so there's some fraction that are done by machines which would include AI. Um and of course there's a big uh literature going way back and you know Pascal has of course done incredibly important work in this space among many others. Um I want to say that you know what where I'm going to go here is that there's sort of a share of tasks that are done by machines which is going to be very important but also there's how good you are at tasks which more from my work with Xiao Jay Lou. I want to bring in this sort of uh that's where the intelligence would be. I sort of we want you know really smart servers. because we can't just think about the share of what you do but also how good you are at it and how much that does or does not make a difference. Okay. So the paper is basically going to take that kind of approach and apply it to uh research and development and it's going to be micro you know in part for the reason that I want to be sort of broad in the application areas we might think of right so we often think about these models of tasks in general equilibrium or with you know um you know TFP growth maybe as the ultimate outcome but of course we also care a lot about health for example and we might really care about hey how AI and R&D could sort of solve cancer or various other kinds of issues um and we want to think about it as part of product development and we want to think about it at the sort of productivity or profits of a particular firm or how you would do entrepreneurship as a sort of you know as with AI or something. So I wanted to kind of be a little bit open and so in other words what I'm going to do is I'm going to imagine that you have some generic outcome Z which is a thing you want to improve. So that could be for example very narrow like I want to determine the shape of a given protein or at least the folds that would be like alpha fold. Um maybe it's broader. I wanted that's sort of part of a broader set of uh tasks that would make me come up with a drug therapy that would solve a particular cancer. Or maybe I want to think about Z as like super broad like how long do we get to live in the world or similarly you could do the productivity of a firm in a particular sector to an industry to a TFP you could think about this narrowly or broadly and then for any given outcome once you've decided what that outcome you care about is like you know drug therapeutics then you would have a set of tasks that would be relevant to that research outcome that you would undertake that could either be done by AI or labor or some other machine and then we want to bring in the idea that AI is going to sort of radically improve okay so that's what the framework is before and I want to ask a bunch of questions and maybe some others. It's like what is the effect of large advances in AI? Is transformative AI possible? Like what would be the mapping you need between sort of AI capabilities advance to actually give you that kind of TAI like outcome? What would this powerful AI do mean meaning like millions of geniuses on a server? Uh and then what is the marginal return to intelligence? Okay, so um you know I'm giving a very simple framework in a sense. It's it's going to leave out things that I think are probably important. At the same time, I mean to sort of get us focused on three things that are in the framework that I think you kind of have to address. Maybe there are other things you need to address, but I think if you want to take a stand on this, you really have to address these three things, which are first, what is the fraction or share of tasks that the AI will do in the research process? I'm going to call that gamma, 10%, 40%, 90%. How good is AI at these tasks? In my model, that's going to be CAPM. That's kind of a measure of machine productivity on average. Okay. And then how strong are bottlenecks which is you know we could think about from like a CES parameter or from a generalized mean parameter. That's why I'm going to think about it. We'll call that theta. Okay. I also want to say so I'm going to just basically do conceptualizations kind of abstract conceptualizations and see what you have to believe to get from AI to TAI or whatever. But I also believe this can also form a measurement agenda because I'm going to say that these three objects are like the three things you need to know if you were going to try to make a strong claim empirically. Um, okay. So this is very standard. I don't want to show you lots of equations and I'm you know so the only different here is that that equation one normally on the left hand side we put y for like output or GDP. Here I'm putting z dot. So it's an ideas production function. So it's a it's a function of do I have a pointer? I don't. So Z dot is just like Z is you know my longevity if I get a given cancer and Z dot would be like how much longer I want to live. Z is the current state of knowledge at that. That's actually I'm going to kind of ignore all of that kind of longer run imp implication between how the state of knowledge affects your rate of progress very important in macro models. But here I'm going to really just look at your sort of short run. If I just surge AI's capabilities in which case ZT is fixed like how much faster am I going to go at least in the short run. I'd also argue that in a lot of contexts the short run is going to be the upper bound on how fast you're going to go. All right. And then the next thing in brackets is just your standard aggregator where the little Rs are your tasks, your research tasks. So R. Okay. So go down a line standard stuff. The task can be performed either by machines which have like on any given task J you're going to have a machine productivity M using some capital input X or you could do with a human which case you have some human capital H and you apply some labor little L. age I'll just keep it constant across all the tasks just for simplicity you could have a lot of heterogeneity and how good the machines are at different tasks okay um and then machine tasks you know this sort of key uh pair parameters can emerge here it's you could think of it as a parameter you could think of it as endogenous depending how you do this model but gamma so some fraction of tasks can be done by the machines um and the rest one minus gamma is going to be done by labor okay um I'm going to I'm micro so I'm taking prices as given there's some price of using a machine or some price of labor just for now. Now, that's important, right? Because the excitement about AI can really be twofold. It's that its M might be really high at a task. Wow, it's really good at that. And a lot of our kind of g whiz AI stuff is like, wow, look what it can do, you know, and all these things we ask it to do. But of course, the other piece is that the MW, the cost of using AI can be a lot less than the cost of using labor W. So a lot of the kind of relative gain you can get from an AI is that it's also really cheap you know for for whatever task you are are giving it. Okay. And then theta up in the top there right that looks like a rel parameter but theta in this written this way right it's also the it's the it's the generalized mean parameter. So theta is one like that's perfect substitutes, right? Infinite elasticity substitution, but theta is one is just you're just adding up all the rs just make theta 1. That's each one weighted by di that's just the arithmetic mean of all your task outputs. Okay. And in that world if you made one of your tasks like really really productive you know in an arithmetic mean when one variable goes way up the whole thing goes way up. It's pulling very strongly upward on progress. But if theta is less than zero, which is the case I'm going to look at and I'm going to argue later that well the empirical evidence is not fully developed by any means. Uh that's probably a better assumption. Um then that's where bottlenecks start to come in. Right? So if theta is negative one it's the harmonic mean okay which you may recognize and harmonic means for example are like brutal and this getting you close to leont which is of course theta is negative infinity then all that matters is the thing you're bad at and bottlenecks matter. you know, theme that keeps coming up and Susan was just talking about as well. Okay, so I I'll come back to that and Okay, so then again, I I this is the pretty much the only equation I'm going to show you and it's not important that you really address it. I just want to set up what's the problem I'm I'm I'm answering with a theory uh and what am I not answering? So I'm saying imagine you have a given R&D budget like you have a dollar to spend in R&D and you could either hire labor or you could hire machines and you're going to allocate that across the tasks. Okay, so the optimization problem here is simply that you're going to say there's these vis, you know, there's these M's telling us how good the machines are right now. There's this H for humans. There's some prices for AI and and labor. That's the mu and the omega. And I'm just going to optimize my allocation of a dollar of R&D spend to kind of give me the fastest rate of progress I can get for that dollar. Okay, so what is it? And that's what it turns out to be. But don't worry about it. It doesn't really matter the form. I just want to point out that like so if you increase a dollar of R&D, you get some more Z dot. Here it's linear because I haven't had any congestion, but we could do that too. But the important thing is that's kind of like a return, right? So I put a dollar of R&D in. I've optimized my allocation. How much Z dot do I get out? You could literally make it a return if you put Z dot in dollar terms. Like if it was health and you attach like a quali and then you had like a value statistical life, you could sort of say, okay, I put a dollar in, that's what I get. Or maybe a productivity sense, I get some value added on Z dot and then I put some money into R&D. That gives you a real return. Okay, so think of a private return. The other point is in my last bullet here is that um by you know increasing AI capabilities we can see how we change that marginal return. What's the marginal return to intelligence for a dollar I put in I got this much Z dot. If I take up M radically now what's the return? Okay, so we can think of it that way. So I'm here as you can see I'm trying to answer the marginal returns to intelligence question. Last point is there's a cap M. Now the big M is basically an average of all the little M's. Okay, but it's a generalized average again. That's not arithmetic average. So it's going to depend on bottlenecks. Okay. All right. So that's kind of summarizing. All right. Now, it turns out, so let's go to the first question. What happens if you radically increase machine intelligence? By which I mean M. Now, this is actually and maybe not what we mean. So I just be careful what I mean here. There's actually two kinds of AI capabilities here, right? There's sort of how good are you at the tasks the AI does, M, but there's also how many or what share of tasks does the AI do. So, I'm going to just do here in this thought experiment is I'm going to hold the share fixed. It's whatever it is. And now I'm just going to take you to like as far as you want to go in terms of your M. And then in a minute, I'm going to do the other thing. I'm going to expand your share of tasks you can do. All right. So, fix the share of tasks. Now, let M get bigger. Okay. Define lambda some multiple. Okay. It turns out that this theory allows you to kind of think about things in ratios. And so I actually don't have to take a stand on what M is right now or any of the little M's or what H is or even what the prices are because it turns out it all gets summarized in SXT which is the current expenditure share on machines as opposed to labor out of your dollar of R&D. Okay, so that turns out to be like a little cheat code for empirically. So if you think you know something about SXT and maybe Brian may talk more about this then then all you care about is lambda which is how much I multiply your average productivity and b which is just a function of theta. So you have to take a stand on your on your bottleneck parameter. Okay. So that's an equation that comes out. It's it's actually sort of sort of simpler to execute than you might think because we can just sort of use this cheat code of the of the uh of the uh expenditure share on machines. So I'm going to take that as the third in the following graph. But maybe that's too high. But you know, we can talk about that. Um, for different values of theta. So what am I doing on the x-axis? I'm multiplying theta by a lot. Like this is logs 10, 100, a thousand, a million on the right. And this is the rate at which Z dot multiplies linearly. Okay. You can see that if theta is low, like take theta negative 1, that's the red line in the middle, you're actually capping out at just over two. So you could get like infinitely intelligent machines operating on the current set of tasks and you multiply the rate of progress by two. So it's very modest because because theta is equal to negative one harmonic averages again are brutal on bottlenecks. The point is that it doesn't really matter how good you get at this because the bottlenecks the things you're doing with labor uh or maybe other machines that aren't AI um are holding you back. Okay. All right. But again I'm also not letting you expand the share of things that the AI does. And I think we're also trying to imagine maybe these smart servers full AI now take on a lot more tasks from labor. Okay, so what does that look like? Well, it's a different equation, but I want to show it to you. Um, then you then you get this result. Okay, so here what I'm doing on the x-axis kai is what percentage of labor tasks are still done by labor. Okay. So like 10 to the negative 1 is you've already gone to like the AI does 90% of the labor tasks. 10 to the -2 is the AI does 99% of all the labor tasks. So it's and I'm expanding that. So it's it's whatever we're doing now. It just takes over 90% 99% 99.9%. Okay. But I'm not letting you get smart. Okay. I'm holding MT fixed. So I'm saying whatever the current state of sort of smartness is. So this is not your geniuses and servers. I'm just going to extend that across everything. And then you can see that again if theta is negative one or something low, you actually are capping out a pretty low multiples. Okay. And the point is that basically what this is assuming is is that the machines, you know, sort of aren't really that good in some sense. So if we just switch to machines and sort of the the productivity versus cost of machines isn't so great, then you know, you don't get that much of an advantage. I mean, per dollar spent. Remember, this is not general equilibrium. And the other thing that could happen here is you would say, \"Oh, I'll spend more money on R&D, you know, as you get you are getting more productive at R&D per dollar spent.\" So you might say, \"Oh, well, there's going to be another kind of quantity effect, scaling effect, which may happen.\" Of course, a lot of R&D in the Americas being cut even though it seems very valuable. So it's like, you know, it's market failures and it's not entirely clear that we, you know, we get we got our normal sort of, you know, indogenous equilibrium going all the time without the political economy thinking about the political economy. Anyway, okay. So where are we? So it seems like if you just get really smart machines or you just get you know really massive expansion of tasks that's not actually multiplying. I mean two twice as fast is isn't bad. I mean but it's not like what you might think. Uh and of course the answer to what's going on here is bottlenecks. It also means that you want to really think about any kind of massive gains that we're going to get from AI. You have to really expand on both margins. You need to do both lots more tasks and you have to get really good at them. Okay. So what's going on is because bottlenecks are brutal and I think you know the simple math that I like about bottlenecks is you know if if theta is equal to negative 1 what's the harmon harmonic average of the number one and infinity it's two okay and that's sort of what's happening in the model the the arithmetic average of one at infinity is infinity okay but and the in leant is one of infinity is one right but as soon as you get to theta less than zero you're really just kind of muting sort of how good you are at certain things uh and not others and I would argue that you know Moore's law you know we have 10 to the 17th more floatingoint operations per dollar today is kind of a real world aversion of this like we basically gone to more or less infinity in a lot of tasks and we don't use computers for one task because like all over the place right and not that in every software application it's that good but like think about a regression think of think how much faster you run a regression with a million observations than you would yourself you spend your whole life inverting one big data regression coefficient matrix and you get it wrong also I get it wrong anyway Maybe you're better than I am, right? You know, and so, you know, the gain in sort of productivity on running regressions for economists is basically infinity. I mean, it's like it's like unbelievable. It happens in like a minute with a cheap computer, right? But obviously economics hasn't like our level of insight is is probably improved and we probably hopefully move faster than we did before, but we haven't gone to like I don't know you guys. I don't know how fast we we go, but it's not it's not as impressive, right? So, this is all bottleneck stories. Are bottlenecks reasonable? So I had this completely different paper about how you assess credit in teams that looked at what happens when you take a a bunch of people together and you do the fixed effects where they move across like us you move across papers inventors move across patents and then you try to estimate that theta parameter in a CES production function and when you do that basically what's interestingly what's the median when you do it across hundreds of science fields and hundreds of patent fields it's actually negative one or maybe slightly less than that so I mean that that's this is not machines versus humans but it's somehow you know it's different inputs into the same production function. People are probably doing specialized things. It looks like the is equal to negative one. Amdall in computer science computer scientist here. They get really excited. Neil, you're going to get excited, right? That's law is like this kind of main idea in computer science that the overall performance of the system. How does it relate to the component performances? And guess what? It's the harmonic average. It's negative. Um law is negative one. Um you can do various thought experiments. Um and the second one I think is again we've kind of got at and if which is that like we like regression regressions are kind of an amazing advance like for economists right but a lot of other fields have this telescopes microscopes x-ray spectrometry you know there's so many machines that we use that they're sort of comparison to human senses of observation are basically infinitely better right and they've given us enormous insight empirically but somehow that hasn't like necessarily like I think it's very helpful but it hasn't like radically shifted the rate at which we understand cosmology particle accelerators I know bra will talk about particle accelerators but that's another great great example and when you see that when you see like over important tasks that we do in our research massive increases in productivity but sort of more limited and muted overall progress that's like a strong indication that we're sort of in the harmonic average land not in the arithmetic or even geometric average land TAI but you can get there okay and when you do both margins you can get there because if this is sort of what share do you place. So now here you're down to 0 2 that means that the AI is doing 80% of all the human research tasks and then this is kind of logs on M like how smart are these machines your powerful AI if you can do a lot of automation and you have smart machines it gets better in fact if you can get to like 80% automation even with theta1 and your machines are only 10 times as good on average then you actually multiply it by 10 my line here is when you get 10x uh uh is one definition of transformative AI increases in the rate of progress. That said, so I think it is probably possible, but this is only one research outcome. There's every the world is a bunch of different Z's, right? And if you thought about TFP in the real economy, we think about bottlenecks, chips or whatever was one example. But you know, you know, in macro, we think of like the elasticity substitution is like 0.5 between capital and labor, which incidentally, by the way, that's sigma that's theta equal to negative 1. uh if you convert it um and in health like in health obviously your your your health is not the average function of your organs sadly right you know many are the ways we can fail right and that that's a low theta uh health system so we can have radical improvements on some things if we solve cancer but if we don't solve all the cardiovascular issues or chronic pain or mental illness you know where are we so so there's just much much more uh to do and so I think that actually gives you kind of a nested so there's like there's the bottl with any given research process to improve like come up with a drug for that thing and then there's like well you got to do it across all these other things and so you're gonna kind of run into this second layer which makes me think about what you guys were doing in terms of networks of of production functions and things and also maybe they're end roots but that you can get around and substitute but it does suggest that there's uh it's complicated so I'm going to try to coin a phrase I will fail economically meaningful AI EAI and I I'm only going to say that because I know we're supposed to only talk about TAI but if you think TAI is difficult and then we're like ah AI what does it do anyway I mean TI is like incredible you know what I'm going to call economically meaningful AI which I define in the paper as like 1.5 times progress that's like much more feasible and if you add it up over time you get to somewhere very very very profound 1.5 in levels or rate rate so if instead of if TI is 10x the rate if I back up to 1.5 obviously if I lower it it's more easy and I can get much nicer diagram here u but also you know 1.5 5 times the rate of progress since the industrial revolution going forward. That's three industrial revolutions in 150 years. It's who knows what technology they can't even imagine them. I think the average the median real income of the uh US household becomes $4.5 million. I mean so these are like it's hard to imagine what we're even talking about and that's just economically meaningful AI not um TI. Okay. Uh last point and I should stop is that I also think there's a measurement agenda here. uh you guys were very helpful in terms of helping me think about benchmarks. So I'm keep meaning to come back to you but I'm moving it's coming together very late. Um so you know AI loves benchmarks right because you measure how you're doing you compare the different models but also you train to sort of beat those benchmarks right it's a very very central part and there are many many many of them so it turns out that the the parameters like the share of tasks how good you are at task and relative to humans and then the bottleneck parameter would give you a way in any particular research area with a benchmark design to in fact estimate these parameters in the context of some real research application and in fact paperb which is one of them is does that paper bench is like can the outcome is can you replicate these ML papers so it's kind of like a real research task not just like can you solve a math problem they then code like 8,000 subtasks right of to that you have to do then they have the AIs do them all as well as the humans the AI is better at only some of them that gives you a kind of a gamma potentially what share would the AI do you can see how much better it is at those because if you know just how much time it takes a human how much time it takes the AI you get very simple measures related to the model and then you could get theta up just by uh experimental variation of inputs and outputs to C theta in a given area. Anyway, so I think that there's a way to take this framework and apply it in benchmarking or maybe design benchmarks to fit this framework where you can say not just like wow g whiz it can solve the math problem but like will it really radically accelerate progress in what area? I think it probably varies highly across different areas like pure math, software design, drug design that be wildly different with these parameters in a way that would be very that could be enlightening. Okay, sorry to go on a little long but thank you. Thanks very much. So I think I am probably the only person in the room who actually has run a regression on a Marchant calendar sorry calculator uh using a from a cross productds matrix. Yes. But uh fortunately that that that didn't last very long. Uh that was my that was almost my first job when I was working as a research assistant for Swiggelicus was was checking a cross productduct inversion for for a regression. Um okay so which one goes green probably goes. Um well first of all I really enjoyed this paper. I have to thank the organizers so much for inviting me to discuss it. Um I like thinking about it and so forth. Um we know what the core question is. Uh so I won't repeat it. Um and his approach is this model is really very elegant. I really I really like it. Um he talked during the presentation enormously about the bottleneck side and I have no disputes with that. I think that's a very interesting thing to to focus on and so forth. Um I had some reservations about the model which I spent a lot of time on. Um I am now sat more or less satisfied. Um but I will my discussion basically takes has three parts. One is just to tell you briefly why I was puzzled with the model because I think um there's a limit to what it can do because of the fact that these M and the gamma uh the the average productivity of the machines and the share that they're used for are actually jointly determined uh in the model by the from the primitives which are really the individual machine productivities and the cost of the machines as well as the labor productivity and cost and they the the net result is that if you really you know wanted to take a derivative with respect to gamma it's buried in m also and it becomes very complicated. A simple way to say it is simply that if machine productivity increases the what you want to do with machines also changes. The share you that you want to do in machines also changes. But when you step back and you do static analysis of a one-time change in gamma or m the analysis goes through and I think that's what Ben focused on and I'm now happy with that. So we'll well I won't talk more about that. I have another slide in case we have time, but it's not really worth spending time on. So, the first thing I got concerned about, but then I decided that again that this is more complicated than than I was thinking. Um, was Ben alluded to a capital cost share of uh existing capital cost share of one-third um to proxy the machine software and AI um share. I'm not sure where he got that actually, but um because I I was pretty aware of the evidence and I thought it was around 10%. So I went and looked um and I find the following um OECD data averaged across countries and this is without the US to great extent because the US uh has not reported these numbers until very recently. um essentially says that the capital cost share is 10% and the labor share is something like 50% and the um other current costs which are mostly correlated with labor uh because they're supplies um and things like that are uh are 40%. If you look at the recent numbers for the US, I have the 2021 numbers. I think Ben had looked at the 20 numbers maybe. I'm not sure. Um uh but in any case they they're not they don't change over time. Um what you find what you find is that the machinery software and IP products which you could argue are part of that package uh are at most 7% of R&D spending uh business R&D spending in the US. It's even lower for government by the way. Um there's probably some rental of capital hidden in the current cost. In fact, I I would be pretty sure of that. Um, so it's got to be more than 7%, but it can't it's unlikely to increase the number to 33%. However, um much AI use in R&D is oneoff own development um by researchers, which means it's in labor cost um not in capital cost anyway. Um so I that raises a you know some more questions that I'll talk about a little bit later but I will just tell you that um some of his multipliers um get smaller when you um when you assume a capital share of 10% um and his theta equal minus one I have no quarrels with that and you assume that machines could do half the tasks which is what he assumes in the paper um in his you know thought experiments Um whereas if they if you shift the share of tasks performed by machines completely to AI that change okay the the change in gamma um the productivity gain becomes enormous um 25 versus 2.25 uh because the initial initially um capital is very high pro high productivity because it's cost-effective. what he means by gamma is stuff that's cost-effective. It's not just that it can do the task, it's cost effective at the task. And he that he sort of swept over that, but that's, you know, that's important. Um, but it's but its actual small share now is only 10%. So that means, you know, essentially you get a huge productivity gain by um shifting to machines. Um, okay. But and this is my butt. Um right now I think a lot of AI use in research is being programmed, executed, interpreted by research labor. So you know in a sense the higher share of AI but it you know there's a deeper problem here which basically is that the survey evidence that we have on how do you use AI in your research um shows that it's hard to separate the task. Um it's a it's every you know just think about a literature search if if you throw AI is very good at doing a literature search you know I'm sure all of you many of you here have tried that and um however you're going to process that and do and do things with it and separating that into two tasks. Yeah, I can do that but I'm not it's not clear to me that that makes there's a lot of meaning associated with that. Um I I uh copied a slide out of a paper by Chuganova and Harhof and co-authors from the Mox Plank Institute where I was last week and um uh where they have they have surveyed researchers at the 1700 uh researchers at Moxplank institutes in Germany and the frownhoer institutes in Germany. So both pure research and applied research uh more developmentoriented research and um and essentially what they find if you look through the list you may not be able to see it but an awful lot of this is to help write research manuscripts to conduct literature reviews to help with grant applications to help write presentations I even did that I mean my slides started out uh as a output of chat GPT uh I gave it my discussion written discussion and it gave me a nice outline um and so forth. Um so the point is that that many of these tasks are the joint product of labor and AI. Um and it's really reinforced by um by reading that. Um so I spent some time in the paper which will probably end up on the cutting room floor given the space limitation. Um looking at um a re the evolution of a research area that I knew well because I worked on it in the late six in the 60s well actually the mid60s and the late 60s um and had watched evolve uh as it went forward which was bump hunting in particle physics. Okay. looking for new particles basically. Um and that has the advantage of there's the Z is kind of at a broad sense the Z has not changed. Now at a narrow sense of course people created a different Z they were looking for but at a broad sense it hasn't changed. uh and it's basically the changes are mostly a response to advances in computing um which have been so substantial although some of the changes are you know in physical uh equipment um uh inventions. So, so back when I was first working with with uh with in this area, it was a human- centric process um where you photographed the interaction and you took many many photographs of interactions. Quantum mechanics says the outcome of the interactions is random, right? But it has a distribution and you would um uh low-level technologists would actually physic physically scan uh the picture and um you know that would be turned into a computer program to fit um and so forth and you you looked at every interaction. What happened as the later advances was people designed equipment that could reject things they weren't interested in and only keep the ones they were interested in. um you could have many more interactions as a result. I mean you know you could look at tens of thousands hundreds of thousands instead of looking at um looking at several hundred um interactions when you an analyzed and um now people are using uh machine learning and AI for data analysis in the way of in finding you know interesting patterns in the data. The Higs Boson discovery was did make use of machine learning and AI. I I have some sites in my discussion of all this. While this happened, the number of authors on a published article increased from a handful, you know, I was on articles with like four people or whatever, you know, to hundreds, you know, where most of the page with the abstract is taken up by the list of the authors. You can have fun, you know, going to look at Fisrev's letters and seeing the list of authors for the Higs Bosan article. or sometimes the author is only listed as a consortium, you know, because it's so large. Okay, the articles got longer and more detailed, much the way our articles got longer and more detailed over the same, you know, 50-year period. Um and my point up here with my lessons from the example are first of all the diminishing returns um uh which is you know for any given line of research as you know Bloom at all showed and I think it's generally true. It doesn't mean there's diminishing returns overall. It just means for any given line of research there's diminishing returns. Um as research advanced the budget number of researchers increased you know suggesting that and indeed the Higs bosen was very hard to find um the moving target um I think that's important the improvements in technology basically allow researchers to tackle more ambitious problems which increases complexity and cost of research which means that you know this fixed budget assumption of course you know is just not a for a Z as you progress you you you basically you're you're you're you know appetite is created by the eating so to speak. Um uh and this final thing is that increased automation didn't just replace human tasks. It created some some new ones such as a considerable amount of software design and development tailored to this particular activity. Um and much of that is still done by humans. Now of course the so you know some of that will end up moving moving to AI if it hasn't already um the the software but it's a new task. Um so um I thought a little about um you know example the example suggests that AI's impact may be less about pure efficiency gains and more about enabling the pursuit of previously intractable problems. No new news to anybody here I don't think. I mean that's just you I just want to emphasize it the future um research priorities I thought of you know I thought about a few um uh I'm sure there are others um as Ben has suggested um detailed empirical work on the R&D task structures the bottlenecks you know in his model the bottleneck is uniform across um that may not be a good picture but you know wonder what would happen if that wasn't I suspect that you know we're totally Leontf you we're almost at Leontf already and and so it's still going to be the you know um the leaky u whatever um dynamic models that account for expanding research frontiers um just the dynamics in general I mean that was the problem I had with the gamma and the m was basically that you can't do dynamics unless you recognize that m that m is determined by gamma and gamma is determined by the underlying ms um and um I think the organization of research teams that's the other thing that changed um during this period the evolution and um and so that's but that's you know right up with the the labor the labor papers we heard earlier um tell you that um same thing the sort of change in the in the what skills are needed and what skills aren't needed as a result of uh AI and and this final question is um whether this experience with computing enhance enhancements which we've seen everywhere you know with the software enhancements um whether that really helps us inform us about the impact of transformative AI or economic uh I forget economic enhancing AI meaningful I've already forgotten it but in any case all right economically important AI um but basically the idea there being um that uh it transformative AI may be more powerful and harder to think about than just reducing uh reducing the cost of floatingoint computations. Okay, thank you. Thank you, Bronin."
  },
  {
    "id": "by7k-H7sZ8A",
    "title": "Genius on Demand: The Value of Transformative AI",
    "url": "https://www.youtube.com/watch?v=by7k-H7sZ8A",
    "presenters": [
      {
        "name": "Paul M. Romer",
        "affiliation": "New York University",
        "scholar_url": "https://scholar.google.com/citations?user=y0u78yoAAAAJ"
      }
    ],
    "num_presenters": 1,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nAjay K. Agrawal, University of Toronto and NBER\nAvi Goldfarb, University of Toronto and NBER\nJoshua S. Gans, University of Toronto and NBER\nGenius on Demand: The Value of Transformative Artificial Intelligence\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The presentation titled \"Genius on Demand: The Value of Transformative AI\" at the NBER Economics of Transformative AI Workshop centers on the economic implications and research opportunities presented by advancements in artificial intelligence (AI). The primary research question addresses how AI, particularly through deep learning, can enhance our understanding of data and potentially reshape economic theories and practices. The discussion highlights the rapid progress of AI technologies and their implications for human decision-making, suggesting that AI may surpass human capabilities in various domains, particularly where human judgment is influenced by noise and bias.\n\nKey findings from the presentation emphasize that AI has the potential to improve decision-making processes by reducing the inherent noise in human evaluations. The speaker argues that algorithms could outperform humans in many tasks, not merely by making accurate predictions but by eliminating variability in judgment. Additionally, the discussion touches on the idea that as AI continues to evolve, the distinction between human and machine capabilities may diminish, raising questions about the unique aspects of human intelligence and decision-making that could be replicated or improved upon by AI systems.\n\nThe implications of these findings are significant for both economists and policymakers. As AI technologies advance, there is a pressing need for a research agenda that explores the economic impacts of AI, including the potential for AI to replace human roles in various sectors and the ethical considerations that arise from such transformations. The presentation advocates for a proactive approach in understanding and shaping the future of AI in the economy, suggesting that policymakers should prepare for a landscape where AI not only complements human capabilities but may redefine them entirely.",
    "upload_date": "2025-09-29",
    "days_ago": null,
    "has_transcript": true,
    "word_count": 7703,
    "char_count": 41363,
    "transcript": "Thanks very much Anton. Good morning everybody. Um this conference we've done some version of this conference um NBR conference on AI u since 2017. 2017 was the first one. Uh Avi Gulfarb and Joshua Gans and I coordinated the first one in the in Toronto. Um at that time the invitation uh contained this description when we were trying to convince people to come to Toronto for a conference on economics of AI. Uh the context is this. Imagine back to 1995 when the internet was about to begin transforming industries. What would have happened to economic research into that revolution had the leading economists gathered to scope out a research agenda at that time? Today we are facing the same opportunity with regard to AI. This time around, we are convening a group of 30 leading economists to scope out the research agenda for the next 20 years into the economics of AI. Scholars who were uh accepted the invitation were asked to write up and present ideas around a specific topic related to their expertise. For each paper, a discussant was assigned throughout the conference in presentations, discussions, and debates. Participants weighed in with their ideas of what the key questions would be, what research has already shown and where the challenges will lie. uh AI researchers Jeff Hinton, Yan Lakun and Russell Kunoff attended providing useful context. Of course, since then Jeff won the Nobel Prize. Yan joined Facebook and um uh created the the primary uh open- source uh model uh in the US. Uh Russell Kudenov uh became the head of AI at Apple uh after that conference. Um the conference uh was unique because it emphasized the work that still needs to be done rather than the presentations of standard research papers. Participants had the freedom to engage in informed speculation and debate about the most important areas of inquiry that resulted in this book. I think uh quite a few PhD programs uh uh use this uh book now and that is going to be a key output of this year as well. So this year there will also be uh a book like this one. uh so all the papers you see presented and all the discussants are invited. The key thing here is the timeline. Jim Purpa has put this on a fasttrack timeline so that uh the book is published quickly. And so the uh after the sessions today u we ask that you send the uh the manuscripts back by October 1. Both the papers and the discussion uh essays if you want them in the book. Um this book has 24 chapters and about 35 if you uh include also the discussants they they all remember the time of this book this this conference was 2017 there was no chat GBT no no bells uh in AI uh no Whimo cars driving around San Francisco and there is you know the chapters touch on different views of AI there is only one chapter here that I would say is a proper preface to today's conference. And you might be thinking, oh, that must have been a chapter written by some young whippers snapper who, you know, is uh one of the people that's working away in in a lab at OpenAI or one of those companies. Uh but it's not it's a chapter that's at the very end of the book and it's uh an essay. I'm going to read you a couple of excerpts from Danny Cannaman. During the talks yesterday, I couldn't understand most of what was going on and yet I had the feeling that I was learning a lot. I will have some remarks about Colin Kramer uh and and then some remarks about the few things that I noticed yesterday that I could understand. Colin had a lovely idea that I agree with. It is that if you have a mass of data and you use deep learning, you will find out much more than your theory is designed to explain. And I would hope that machine learning can be a source of a hypothesis. Sendle that is that some of these variables that you are identifying are genuinely interesting. At least in my field, the bar for successful publishable science is very low. We can we consider theories confirmed even when they explain very little of the variance so long as they yield statistically significant predictions. We treat the residual variance as noise. So a deeper look into the residual variance which machine learning is good at is an advantage. So as an outsider actually I was surprised not to hear more about that aspect of the superiority of AI compared to what people can do. Perhaps as a psychologist this is what interests me most. I'm not sure that new signals will always be interesting, but I suppose that some may lead to new theory, and that would be useful. I do not fully agree with Colin's second idea that is useful to view human intelligence as a weak version of artificial intelligence. There certainly are similarities, and certainly you can model some of human overconfidence that way. But I do think that the processes that occur in human judgment are quite different than the processes that produce overconfidence in software. Now, I turn to some general remarks of my own based on what I learned yesterday. One of the recurrent issues both in talks and in conversations was whether AI could eventually do whatever people can do. Will there be anything that is reserved for human beings? Frankly, I don't see any reason to set limits on what AI can do. We have in our heads a wonderful computer. It is made of meat, but it's a computer. It's extremely noisy, but it does parallel processing. It is extraordinarily efficient, but there is no magic there. So, it is difficult to imagine that with sufficient data in the future, there will remain things that only humans can do. The reason that we see so many limitations I think is that this field is really at the very beginning. I mean we are talking about developments like deep learning that took off 8 years ago that is nothing. You have to imagine what it might be like in 50 years because the one thing that I find extraordinarily surprising in what is happening in AI these days is that everything is happening faster than we expected. People were saying that it will take 10 years for AI to beat Go. The interesting thing is it took less than by an order of magnitude. This excessive speed at which things at which the thing is developing and accelerating I think it is very remarkable. So setting limits is certainly premature. One point that was made yesterday was about the uniqueness of humans when it comes to evaluations. It was called judgment but in my jargon it is evaluations. Evaluations of outcomes are basically the utility side of the decision function. I do not see why that that should be reserved for humans. On the contrary I would like to make the following argument. The main characteristic of people is that they are very noisy. You show them the same stimulus twice and they do not give you the same response twice. We have choice choices conditional on the same stimuli. What can be done with AI is to create a program that observes an individual's choices. That program will be better than people at a wide variety of things. In particular, it will make better choices for the individual. Why? Because it will be noisefree. We know from the literature that Colin cited on predictions that there is an interesting tidbit. Take some clinicians and have have them predict some criterion a large number of times. Then develop a simple qu equation that predicts not the outcome but each clinician's judgment. That model does better in predicting the outcome than the clinicians themselves. That is fundamental. It is telling you that one of the major limitations on human performance is not bias. It's noise. I may be partly responsible for this as when when people now talk about error, they tend to think of bias as an explanation. That's the first thing that comes to mind when there is an error in human performance. In fact, most of the errors that people make are better viewed as random noise and there is an awful lot of it. Admitting the existence of noise has implications for practice. One implication is obvious. You should replace humans by algorithms whenever possible. Even when the algorithm does not do very well, humans do so poorly and are so noisy that just by removing the noise, you can do better than people. The other is that when you cannot replace the human by an algorithm, you try to have human simulate an algorithm. The idea is that by enforcing regularity, process and discipline on judgment and on choice, you reduce the noise and you improve performance because noise is so pernitious. Yan Lun said yesterday that humans would always prefer emotional contact with other humans. That strikes me as probably wrong. It is extremely easy to develop stimuli to which people will respond emotionally. An expressive face that changes expressions, especially if it's baby shaped, gives cues that will make people feel very emotional. Robots will have these cues. Furthermore, it is already the case that AI reads faces better than people do. Undoubtedly, robots will be able to predict emotions and develop an emotions far better than people can. I really can't imagine that one of the major uses of robots will be taking care of the old. I can imagine that many old people will prefer to be taken care of by friendly robots that have a name, have a personality, and are always pleasant. They will prefer that to being taken care of by their children. I want to end on a story. A well-known novelist wrote to me some time ago that he's planning a novel. The novel is about a love triangle between two humans and a robot. What he wanted to know is how the robot would be different from the people. I proposed three main differences. One is obvious. The robot will be much better at statistical reasoning and less enamored with stories and narratives than people are. The other is that the robot will have a much higher emotional intelligence. The third is that the robot would be wiser. Wisdom is breadth. Wisdom is not having too narrow a view. That is the essence of wisdom. It's broad framing. A robot will be endowed with broad framing. I say that when it has learned enough, it will be wiser than we people because we do not have broad framing. We are narrow thinkers. We are noisy thinkers and it is very easy to improve upon us. I do not think that there is very much that we can do that computer will not eventually be programmed to do. So with that heading to transformative AI uh we'll begin with just a quick run through of the introductions and then Joshua uh will kick us off with the first paper. Uh so just name and affiliation please >> Ram Touy from the Stanford Institute for Economic Policy Research. this building. >> Dublin Norris from the International Monetary Fund. >> Alberta School Barcelona. >> Christina Langanger, Stanford Digital Economy Lab. >> Chad Jones, Stanford GSB. >> Christine, Stanford GSV Econ. >> Lisa Abraham Rand. >> Oh, Andrew Co MIT. Tom Cunningham. I just left open AI now model evaluation and threat research. >> Phil Traml Stanford digital economy lab. >> Kevin Chad Stanford Ecom. >> James Chow, Blue Brook beta. Abishek Nagra Berkeley. >> Susan Young, Stanford Digital Economy Lab. >> Zoe Hitig, Open AI and Harvard. >> Hazel Brown, APIN philanthropy. Tomasaki Center for the G of AI. >> Way University of British Columbia. >> Luis Garano LC. I'll be tweeting the papers. If somebody objects, please tell me. >> John Male, University of Gway. >> Yanna Marinesco, University of Pennsylvania. >> Michael Schwarz, Microsoft. David Rothschild, Microsoft Research. >> Lee Lockwood, University of Virginia. >> Joshua Gans, University of Toronto. >> Diane Coyle, University of Cambridge. >> Neil Thompson, MIT Future Tech. >> Jay Agarwal, Toronto. >> Eric Bernolson, Stanford Digital Economy Lab. >> Joe Stego, Columbia, Max Ventura, Colombia. Avi Goldfire, Toronto. >> Matt Wines, Harvard Business School. Detool, Northwestern. >> Kevin Bryan, Toronto. >> Carol Curado, Georgetown University. Uh, Brahman Hall, Berkeley. Jillian Hadfield, uh, Johns Hopkins University. >> Dan Tffler, Toronto. Kath from Taka MIT. >> Dan Berran Columbia. >> Elliot Ash from ETHZ. Sendelan from MIT. >> Pay Shahidi MIT Islam. >> Christy Co Stanford Digital Economy Lab. >> John Bis University of Cambridge. >> Roy Bahhat Bloomberg Beta and Berkeley. Daniel rockan >> Andre Fracken Boston University in Amazon >> Anguin from the IMF. Thank you. Um yeah so uh again uh just to be sure uh we are going to record the presentations and the discussions. Um and we are also uh recording the Q&A but my understanding is we are not releasing those in YouTube. Uh and then one additional point uh Luis and anybody else who is uh interested in doing that are uh going to tweet about the content of the papers and discussions. Uh if you are not willing to be tweeted about uh please say so at the beginning and uh Louis and I hope everybody else as well uh is going uh to uh yeah respect that. Uh okay. And uh with that uh oh yeah sorry I forgot Anton Cornick Virginia. And with that uh let's start with our first session. Um >> Joshua >> Joshua is presenting. Uh you've got 20 minutes. The timer is right here. Thank you. Thanks. >> All right. Thanks very much. Uh this is a paper with my two presently um human co-authors Jay Agual and RV Gab. Uh we call it genius on demand. Hopefully that'll become apparent. Uh we took uh this uh charge uh uh seriously of what what this conference is about. uh uh you know how do we take this uh pronouncements that are coming out of Silicon Valley assume that they will occur and uh analyze the implications of them and and the one we focused on was uh Amod's country of geniuses in a data center and he sort of said a few things about what a genius was uh uh by the way the first one there in terms of pure intelligence it is smarter than a Nobel Prize winner across most relevant fields So that pretty much at last we cover everybody in the room is is being replaced. Uh according to this um and you know it has things smart people you talk to and other things that you would people normally think about in terms of uh geniuses. Um so we thought well how do we you know let's assume this occurs assume this occurs. What are its impact on uh some aspect of the economy? and we chose uh knowledge work. Um so you have a genius in a data center. Actually you don't have that you have many geniuses in the data center and um the question is what the question that Jay and Arvy and I wondered is what are we going to do with these geniuses? Um it it you know people take it for granted that uh uh geniuses have uh uh you know it's ob obvious the geniuses would be useful. Um but you know you have to sort of think about uh and they might be useful eventually um uh but there'll be constraints. And so we're interested in this idea that we might get the country of geniuses before we actually know what to do with them. Um and so given that how might we forecast what is going to happen with those sorts of very basic toolkit of economics shortrun and long run. Short run stuff is fixed long run stuff is is is flexible. Um and so root first to sort of do this would be to take an empirical route. Uh so you we'd look today uh because we have human geniuses uh as as Eric noted uh and uh uh we can look what happens to the organizations that have you know they they don't have millions of geniuses but hundreds of geniuses and and look at how you know just wondrously efficient and dynamic they all are in organization. And so we we thought about doing that and then the first one that came to mind of those organizations was like MIT and we wondered hm well that is not you know that is not what I think the Silicon Valley folks have in mind uh and and the question is sort of like you know why you know we have experienced many geniuses but somehow things don't seem to be the panglossian world that that that they're expecting. So obviously our route in this paper is to do a theoretical job and the job of economic theory especially is very useful in these contexts of being able to logically forecast what might happen based on unreasonable premises. That's basically the whole idea. So the the first thing was actually to say, you know, how would you define a genius um as a a a a type of labor um uh and examine uh what happens when those are no longer scarce, which is basically the simple uh thing. And so that's what this paper is. Um and so what is a genius? Okay. Uh you know, of course takes one to know one. Okay. But um I got to do the normal thing and distill it down to something far less interesting than anything. Um so the inspiration we draw upon is is John von Noman uh world accredited and certified genius. Um and a story about something called the fly puzzle that was in his uh biography which is an excellent read by the way. Um uh and the fly puzzle was this is that you have two bicycles uh 10 20 mi apart and they're riding towards each other at 10 mph and there's a fly that happened to be sitting on the one of these bicycles when it started going and it can travel at 15 miles hour and the question is when the two bicycles meet in the middle how far has the fly traveled from wherever it was going at 15 miles an hour we can fly back and forth nondirect I don't care well that sort of thing. Now, now um uh Susan, what's the answer to this? Um sorry, no. So, anyway, so the the the the answer is actually very very simple, by the way. Uh it's it's going 20 miles an hour, 10 miles an hour, 20 m coming to each other in an hour. They'll travel uh they'll meet in an hour. Uh so, how many how long does it take a flight to go in an hour? It will take 15 miles. That's very simple. uh you don't have to be a genius to work it out if you understand sublim the algebraic construction but that is not what John von Noman did John vineyman didn't do that what he did is when he he solved it he uh he he used a slow way to calculate what distance the fly covers on the first northbound leg of the trip and then on the second southbound leg then on the third etc etc finally to sum the infinite series so obtained and and basically sum the infinite series to And uh that's what he did. He didn't use any sort of trick and it didn't extrapolate bits and pieces of knowledge breaking down the problem, all that stuff. Just solved it from first principles. None of us do that because it would be too costly for us to do that for starters. But you know, like we've we've learned not to do that. But geniuses work from first principles uh at potentially higher cost. And so that's what a that's the definition. A genius is someone who knows specific things and then can answer questions put to them based on related things from first principles. So engaging in the cost and they just have lower cost than everybody else of getting to that answer. Um obviously in order to do a genius we have to think about other people. What about others non- geniuses not a geniuses? Um, and the story I was inspired by was um the crow in the picture from Esop's Fables whereby a crow uh um uh had a problem. It had a a beak of water. There was water at the bottom of it. Um it couldn't its beak wouldn't leave uh uh uh get there to for it to drink. But it understood uh that if it put pebbles in there, displacement would work and bring the water up and it could drink. Okay. So it took two pieces of knowledge and put them together to infer and guess a solution to the problem. Okay. And so that's the definition of knowledge genius. It's a different sort of knowledge work. It takes existing knowledge and makes inferences to guess the answers to questions. Obviously in the fly problem, we were able to break it down and get an exact answer. But for other ones, we're extrapolating different knowledge, making guesses. We're getting answers to the questions, but the questions are not perfect. Okay. So that's the that's that's the that's what everybody else does. Okay. And the apparatus for being able to model this came from a paper that's published earlier this year in um econometric called a quest for knowledge by Carnell and Schneider. And then uh subsequent to doing the present paper I realized that this is very interesting research topic and so I extended it beyond just the stuff we talked about today. Um so imagine and this is Carnell and Schneider's setup. Imagine you've got a a universe of questions on the real line. Each has a unique answers that are realizations of a brownie in motion of a random variable big yx or we'll call it truth. Uh that means that questions close to each other will have you have similar sorts of answers. Um and existing knowledge is defined by the space of what we know. Okay, little points of yx pairs that we've already worked out. A decision maker is basically what they're trying to do is get the they if they if they they can either take an action or not take an action. If they take an action, what they get is how close that action was to the true state of the world. A very traditional matching model in in in information economics. Um and uh so so that's that's what they're trying to do. If you knew the answer, you'd obviously get one. If you don't know the answer and you guess, you get something less. Um so a genius is someone who takes this universe of questions there's existing knowledge now because we've got a 15page limit on these papers uh the version we've got in the paper has just got one point of existing knowledge uh that we're going to do which will capture what we're we're talking about here it's a so the universe of questions is just on the 01 line the existing piece of knowledge is right there in the middle and so the genius at that existing point of knowledge knows that piece of knowledge and can find answers close to it. It can find the answers perfectly close to it by expending a cost ad of x - x0 something that's a function of the distance away from the piece of knowledge. the further away from the knowledge that they are, the more costly it is to for them to find an answer. And of course, that has the usual uh convex costs that we would normally assume on whenever you do anything in economics, we start with, right? We know know the rationale. So, so the the value of the decision the one minus the cost in this case is this parabola here. Okay. So, um they're obviously very specialized. So the the idea of the genius is they're just using this piece of knowledge and they're going off of it and sometimes they can't do other things. This was a favorite of ours from from graduate school here at Stanford. Um people's doors uh it speaks to us. Um what about others? Others have to make conjectures. They're making going to make guesses. They're going to take the existing knowledge and say I know this. What's my best guess of the answer of the question I happen to be answering based on that knowledge? And they'll make a conjunction. And these conjections are normal distributions. They have all nice properties etc. And and basically to to jump to the the the important part is that the routine workers uh not a geniuses uh will be making these guesses and if they think they can make an educated guess uh they'll take an action and if they take an action they get one minus uh the the uh mean the absolute error uh or the mean squed error in this case. uh related to the variance of the of the brownie in motion. And that turns out in this in this very simple setup to be a linear function uh peaking at x x0 equals a half and going straight down afterwards. Um so the closer you are to your existing knowledge, the better your answers are, the better your guesses are. If you're too far away, it's worth not worth guessing at all. Okay? In in just made some assumptions for this. Okay? So um if you put the two together, the geniuses are better at everything. If you have a genius, use a genius is is the is the answer here. The genius we have a a question answered. Um uh uh and it's and the geniuses and the uh routine workers are different by assumptions that we led that led to different curvature assumptions on these value functions and they just pop out of the model. Okay. Um, now obviously you can do some playing around with some of these. If your genius was more specialized, it might cross over at certain points. And Louise had some thoughts about that. I'll leave it to him. Um, but but what we want to do is we want to think about well now what? So imagine that you've got an organization and there's a manager for the organization, a human. It doesn't human agent doesn't really matter. their role is to get questions in and say uh do I need do I need uh Dr. smarty pants to do this or can I give this to somebody else right and so um and they they don't know that you know they they know from the space of questions how far it is from the existing part of knowledge and things like that they know and so they they've got they they establish a routing rule for that if it's this sort of question go to the genius if this sort of question go to the routine person and of course they understand the scarcity of that so they've optimized that process so if you take they're going to care about the absolute advantage for any given question that comes in. What's the absolute advantage of giving it to a genius or not? It's always better to give it a genius if you have one, but of course, you don't. So, you've developed practices to ration. And so, the absolute advantage is this sort of uh function here. In fact, the genius and the routine worker do exactly the same job when you've got a question that we already know the answer to. That makes no that makes obvious sense, right? um as you go further away uh the relative advantage of the genius uh rises until such time as you don't can't give it to the to the um uh routine workers anymore. It's they can't they're useless. Uh in which case as you move further aside the advantage of the genius go down. So the relative advantage goes down as well. And so if you had a limitation on the supply of geniuses, you just cut across this absolute advantage curve to effectively get our comparative advantage. And you get sort of three uh uh allocations. All the stuff that's close to existing knowledge you give to the routine workers even though they're absolute advantage, not as good as it. Um and you give the harder problems uh at the edge uh to the geniuses. And that's how you allocate your scarce resources. As you alleviate the scarce resources, you increase the amount of geniuses obviously and you change if you change the routing rule. Okay. Now, finally 15 minutes in the talk, let's talk about AI. AI genius comes into the thing. An AI genius just to give us a fighting chance works exactly like a human genius works out the things from first principles. That's the definition. But has a higher cost of doing so. As I said, we're giving us a fighting chance here. But they're all over the place. You have as many of them as you want. Okay. So, what happens then? The red line there is the AI genius. Um, and but we going to distinguish between the short run and the long run. We've got preAI allocations. In the short run, this is our central point of the paper is the manager doesn't know yet what to do with the AI. He knows they're a genius, but he doesn't and they can handle genius problems, but doesn't know what other problems that they can do in a genius-like way with that sort of knowledge work. They're used to this other thing. So, in the short run, the routing rule is fixed. And if the routing rule is fixed, what's going to happen is the AI geniuses are now going to cover all the missed opportunities of the human geniuses because we're still rationing them. and we'll reallocate the human geniuses accordingly within that to even harder problems because they've got lower cost. Okay. In the long run, the managers are going to say, \"Oh, well, now I understand what I've got with this abundant thing and I'm so I'm going to reoptimize. Now, of course, I no longer need the routine workers at all. I already assume that they're not as good as everyone.\" And so, the the the the re the shortrun allocation of geniuses, they're still scarce, so they still do what they were going to do. Um and then the uh AI geniuses then move to cover the routine workers. Okay. So that's it. That's the forecast of the future. You can take it there and make your investments accordingly. Um now so what what's the what's the point of this? Um the point of this is is okay if you accept a few things. the geniuses of these agents who work from first principles uh but they're kind of specialized uh as a result of that the routine agents you know they they just that's why why MIT is a is very good at some things and can't can't fill out paperwork that's because we've got specialization okay um can't communicate with people can't wash apparently um so anyway uh that was maybe that's out of date that horrible stereotype okay um routine agents um uh are those who make inferences from existing knowledge points and managers choose who answers each question. So you've got that fixed in the short run. AI geniuses boom, explode. Basically, the managers know some things what to do with them. I know they can do the problems that my current geniuses were because they're supposed to be geniuses, but I didn't realize because I've lost it or I didn't have an ability to work out what other things that they can do. So it takes some time. So in the short run, the routing rules, the demand use factors are fixed. In the long run we understand what we want the geniuses to do so we can change our routing rules. The premise of this paper is basically of this all happening is that working from first principles ultimately bit out uh beats out inference in terms of knowledge work. And so the fundamental thing I think that they're saying with geniuses and data centers there's something fundamental about first principles relative to guessing. I don't know if that's the case. Um I I put that fly uh problem to the highest level reasoning model in GPT5 pro uh and it solved it the inference way. It solved it with chunking it down. It didn't go and do the sum of infinite parts. But I've noticed that other situations that's exactly what it does and it could have been the nature of that problem already being existing in language etc. So, so I kind of think that's interesting and if that's the case and you know it's it is a hard problem think for us now when we've got questions whether we need a first principles answer or whether we need it can be drawn on existing knowledge. Um our entire premise of being here in a university and everything else is that existing knowledge we're producing the existing knowledge. We're producing the existing knowledge. Our bet in the world is that our job is to produce the existing knowledge because that is what people need to use. But if it's the case that it's not that and it's better for each question to be answered at first principles, what are we doing here? And with that existential angst, I'll end. Um so thanks for having me and and uh it's it's really great to be able to to discuss this paper and to be to be and to be back back in Stanford. Um so uh what's actually before I start discussing the paper from which I I learned a lot and and had fruitful interactions over the summer. Um what is what is going on in terms of the allocation of knowledge to to to different types of people? I think we have to set potentially contradictory facts which are not going to be contradictory when we try to allocate workers and AI. How how do we allocate them and how how does how does that uh seem to be changing? Um we have evidence from payroll data from the canaries of the mind uh paper for example from the seniority based technological change paper that um there are sharp declines in entry- level jobs. It seems like junior coders and junior uh customer agents are getting are getting hammered at at the film level. However, what we do seem to think to see is that silver firms uh evidence also binopsson. So bino versus binopsson is is the name of the slide. Um these guys are getting helped by the AI, the customer support agents. We also know that taxi drivers in Japan, they get a routine uh routine uh expert AI and they actually do better when they are juniors because they don't know where they're going and the system tells them. Uh we also know it for BCG consultants and for uh writing uh for writers. Um how do we think of that within the context of the paper? Um remember there is the there is the there are two types of workers. The routine workers that have um this linear loss function and the genius who have this quadratic one. Uh and the manager is going to allocate by absolute advantage. Um so my main comment here u Anton encouraged me to kind of talk about this and offer alternative frameworks. So my main comment here um some of my my my comments have already taken on in the paper is is this geniuses or are we actually thinking about something that is useful but not necessarily genius and and the issue is the is the is the quadratic curvature. The quadotic loss means in a very possibly sensible way that when you have AI h it's close to a point the actual loss is pretty pretty small. AI is second order, right? AI close to the knowledge is actually pretty good. Whether humans are actually losing much much more dramatically. So when you actually keep instead of doing this this this one of disruption that Josh introduces when you actually keep the routine workers and keep everybody in the picture and thanks for Josh for for sending me the picture uh that that um that I that I needed for for for this point what you actually see and you shouldn't be surprised is that the AI operates with this quadic curvature and I think in a reasonable way near the existing knowledge and the routine worker ers are still staying around even in the long run. It's a green there. You can see it in LR in green. They're staying around for the problems that are a little bit further out from the existing knowledge. So, I think this tells us something about how the displacement is happening, but I'm not sure if it's about AI, sorry, geniuses. I think it's about AI actually having relatively small losses close to its training set and humans being able to operate further out. Of course once AI becomes really good then the humans are blown away. Um an alternative view is to think of problems as uh as as as of AI as solving a problem of allocating of of replacing does it knowledge. So let me tell you a little bit about it in the context of a of a paper by Talamas uh Edward is is here. So think of it this way. AI is replacing uh tacid knowledge and the key inputs that you need in order to produce are time and knowledge and so like in an AK model like AL model just time and knowledge you're going to organize optimally the work as workers handle routine work cases the experts handle the exceptions and a hierarchy is going to reflect knowledge not power the analyst the manager and the partner the resence and the attending physician the line worker and the engineer in the sover case the junior guys who pass the calls to the experts etc etc so each project involves a problem you draw a problem and uh you if you don't know it you ask for help what happens with AI in a world like this um so it and say okay let's suppose that AI is moving in a line and is getting better and better at replacing at doing solving a certain set of problems so as AI ad advances we can deploy it to solve solve problems. So um so and and we're going to do something that is not kind of the exercise in the paper which is we're going to introduce transformative AI. So suppose that it's smarter than all of us but crucially suppose the price of computer goes to zero and the choice of autonomy is going to be essential. So we're going to have to regimes the the baseline of of talamas and this is going to be the key issue for our discussion of doom and gloom or not doom and gloom. So the key issue is are opportunities infinite? Once we have AI is AI scarcer or more abundant than opportunities in compute is abundant relative to human times then there is an opportunity because of compute and there is still a role for humans. Um on the other hand in the new regime that is not the main experiment in the paper if compute is really abandoned so we run out of opportunities then it's going to replace humans. So let me just run to these two experiments. The first experiment in the left column you have the binosome versus rion uh stylized facts. So if AI is autonomous and is basic, you're going to get, think about it, it's is solving a set of problems of simple problems. If it's autonomous, if it works on its own, it's going to displace lots of workers, push their salary down because they're competing with the AI and complement at the same time the seniors. Why? Because the seniors now can use the AI to get more leverage for their knowledge. They can have more problems because AI is solving a share of them. If it's not autonomous then we have the prinson lead type of story which is there is the novice augmentation they are using the AI to help themselves solve more problems on the other hand if we have an expert AI if it's not autonomous we have great compression AI has to be used by a human every human draws a problem it it knows solves it or it doesn't and nasty oracle like Enrique likes to say and AI solves it. Or if it's autonomous because the number of opportunities is not as high as the as the number of AI because computers abandon relative to humans but scars there's more opportunities always we're still going to use humans and we're not going to replace them. So it's going to be similar to the great compression except with less money for the for the humans and more for the AI. But here's the regime they don't necessarily study which is the r equals zero. Um here computer is completely abandoned. There is compute for everybody. In the autonomous case we get on steroids if if the AI is basic. We get on steroids the cases that we have had before. Superstar humans that are like they have super abundant AI. They can leverage themselves. Satin Adela can have all of these AI workers and he can just kind of lord over it. That's the autonomous case. Or the non for free because this this AI supposedly is autonomous is is is free. Uh remember we're now in the abundant case where it's completely completely free. In the non-aututonomous case, we again get no augmentation. The AI needs humans. So the humans are are working assisting the AI. On the other hand, here in the expert case, we do have the dystopia or like in some of the scenarios yesterday, we can call it utopia. The CI is bigger, is more smart like in the like in the guns at all paper is smarter than all of us and can displace us is for free. So really, it's kind of pushing our wage down and it performs more task with no with no human contribution. On the other hand, if it's non autonomous again, it needs to partner with AI. The AI on its own is worth nothing. So the humans are are actually benefiting from that. Here notice there is no gain from the capital owners because the R is equal to zero. So who is getting the surplus? If these capital owners are not getting anything and this was the discussion we were having yesterday. This is basically all consumer surplus, right? All our problems are getting solved for free. Um so in that sense I'm not sure if it's a full dystopia. So um what's the kind of what's the takeaway of the two models? how to think about the architecture of knowledge in in these two worlds. So if you think of the AJ paper uh as George was was explaining okay so eventually AI is going to take over everything fine but how is it going to be taken over everything is going to be taken in this functional form or if you think of it as having this second order laws around existing knowledge then it's first going to actually going to be taken over things that are close to existing knowledge then the humans Humans are in the the routine workers are in the outer ring and then they get pushed out. The e the talamas kind of world gives us a similar implication although AI takes over the routine workers first. I was not clear emphasizing but when AI gets gets better I hope people are clear that the routine worker likes it because if AI is very good I am a nurse in Bosswana. I couldn't diagnose illnesses but now I have an AI that allows me. So there all these people are getting cured thanks to me using AI. So as AI gets better, it actually does help the routine workers. But it's very important in this framework and I think that's the insight of of what they're doing is to think about two prices or two one two prices that are important and one decision that is important. The prices that are important are of course the wages and the comput the and the and the price of compute relative to what is the scarcity of compute. how abundant is compute which is also playing a huge role in the AG paper and um the choice on whether compute is going to be autonomous or not autonomous if if it's a copilot we go to the great compression where basically we all kind of can use this computer uh to gain um to gain uh output thanks very much Anton few seconds over [Applause]"
  },
  {
    "id": "HCLAJhYurEY",
    "title": "Making AI Count: The Next Measurement Frontier?",
    "url": "https://www.youtube.com/watch?v=HCLAJhYurEY",
    "presenters": [
      {
        "name": "Diane Coyle",
        "affiliation": "University of Cambridge",
        "scholar_url": "https://scholar.google.com/citations?user=W7kX8XUAAAAJ"
      }
    ],
    "num_presenters": 1,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nDiane Coyle, University of Cambridge\nJohn Lourenze Salan Poquiz, University of Cambridge\n2025, Economics of Transformative AI Workshop, \"Making AI Count: The Next Measurement Frontier?\"\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The presentation titled \"Making AI Count: The Next Measurement Frontier?\" explores the significant challenges and opportunities in adjusting official statistics to accurately reflect the transformative impacts of artificial intelligence (AI) on the economy. The primary research question centers on how statistical systems can adapt to measure the direct and indirect effects of AI, particularly in capturing changes in service quality, productivity, and the economic value of free AI services. The presenter emphasizes the need for improved data collection methodologies to account for AI's evolving role in various sectors, including the measurement of AI-generated outputs and the integration of AI into existing products and services.\n\nKey findings include the recognition that while AI services can be captured through market transactions, many AI offerings are available at zero price, complicating their inclusion in national accounts. The presenter discusses the necessity of developing satellite accounts to reflect the value of these free goods and the importance of quality adjustments in price deflators to account for improvements in AI capabilities. Additionally, the presentation highlights the need for better tracking of the AI workforce and the data inputs that underpin AI systems, suggesting that current labor force surveys may lack the granularity required to capture the complexities of AI-related occupations.\n\nThe implications of this research are substantial for both economists and policymakers. As AI continues to reshape the economy, there is a pressing need for statistical agencies to innovate their measurement frameworks to ensure they accurately capture the economic contributions of AI. This includes not only recognizing the value of free AI services but also improving the visibility of AI's environmental impacts and workforce dynamics. By addressing these measurement challenges, policymakers can better understand AI's role in driving productivity and economic growth, ultimately leading to more informed decisions about regulation and investment in AI technologies.",
    "upload_date": "2025-09-29",
    "days_ago": null,
    "has_transcript": true,
    "word_count": 5133,
    "char_count": 26763,
    "transcript": "Thank you. Um I'm very glad to be here um in uh in this conference. So I guess a lot of the papers that was presented in this uh this conference so far is are a lot con are conceptual, right? So yeah, it's mostly frameworks. Um I can imagine that if we want to move towards empirics, we need to have good data to capture some of the changes uh brought about by um artificial intelligence. Right? So the way we're thinking about this is perhaps so what are the what are the changes that we can uh that uh the what are the changes that we can uh think about for the statistical system to adjust uh if AI becomes transformative. So the way we're thinking about this is you know like AI has the opportunity to sort of transform the economy in many ways you know better quality of services um and we've discussed that a lot and I I guess um this exercise is to think about how we c how how we can better capture those changes using official statistics or data. Um so in this exercise what we're trying to do is think about once the AI becomes transformative how should official statistics change how do we better qu how do we how should we better measure that change using using uh using data so um so the focus of this presentation is not it's not uh it's not necessarily measuring AI but the changes uh that AI that um that um that would come from transformation information. So the way we do this is that we focus on some of the measurement challenges arising from the transform uh from the transformation of AI. So we looked at the AI value chain. So we looked at service flows provided by AI. We looked at inputs um required by AI systems and we looked at the cascading impacts of AI to other outputs and we looked at how process are changing because of AI and we also think about time a lot right so um so the way we so I guess the good way to start is to think about how um how we can capture the direct uh the first order effects of AI. So what are direct impacts of AI? Now is a uh so um would AI services be captured in the national accounts? So you know I guess as an economist I'm I'm obliged to answer yes and no. Um yeah so I guess so the way we think about this is you know purchase AI we can capture this through transactions right so they they're all captured through market transactions and they all come into the national accounts. However, we have to think that we know that some of these services are actually price are zero price. So there's a free version of uh Grock, there's a free version of Chad GPT and does that these are not explicitly reflected in the consumption side of official statistics or the consumption side of the national accounts. Um I know that you know we can um we can we can make make an argument that the that you know these are uh these are uh captured through cost subsidization but you know some people will argue that there's still value above that cross subsidization and there are a lot of proponents to and then there are a lot of um there are lots of suggestions to think about how we can better capture that. I think ER um um Eric Eric is um working on uh on this uh in this aspects as well and um the 2025 SNA task group recommended that perhaps we can think about um you know free uh like these type of goods that we call free goods be reflected in some sort of satellite accounts right so you know that's one thing that we have to think about um but you know if we're able to capture AI as a free service we have another layer a problem like thinking about the free goods that are already existing. Think about thinking about Google you know um say you know Twitter or uh social media LLM's L&M functions are increasingly being embed uh embedded in these zeropriced services. So you know the way we can think about this is this is a sort of quality adjustment or quality improvement in these services. Now in the national accounts quality improvements should be measured as a volume change right so the way we can think of if we're able to capture um free digital services as some sort of you know satellite accounts in in in some sort of satellite accounts now we have to think about how do we think about um adjusting for the quality of these goods and services. So now we have to think about deflators to improve to capture the quality change or the volume change arising from the quality from the quality improvement of of these ch uh services and even for AI AI transactions or AI services that are captured in market transactions. We know that AI services are are improving a lot, right? So, you know, like last year we just had G GPT3, then we have GPT4, we have GPT5. So, we have these constant improvements in the quality of LLMs. Again in the national accounts quality quality change should be captured as a form of volume change and you know um official compilers of official statistics have done a lot of work in terms of adjusting for quality change for many goods and services. That's why they they're doing a lot of these hydonic regressions. And um now we have to think about how do we can we um c can can can we adjust our price deflators to better capture those quality changes from uh from LLM services. Uh so from these LLM services. Now that's just from uh that's just from uh uh that's that's some of the points thinking about how uh AI services um consumed by households. But you know I think one of the reasons why AI is increasingly um is is increasingly important for people uh studying productivity would is you know like it has the ability to improve uh improve production. Um and you know right now this is this data this data is from the management exitation survey. We know that adoption rate is relatively low at the moment. This is 2023. It could get uh really really fast. Now one of the question that we have to think about is can we make um AI usage more visible in official statistics such as the national accounts right so yeah we know that you know B2B transactions that's reflected in national accounts if um if say you know a company like um um K KPMG subscribes to Chat GPT services but you know some um but but we can think about some firms employing the the uh employing um the free versions of Chhat GPT and um in other LLMs um how can um can we reflect that in in official statistics as well. So right. So also we talked about LLM functions being embedded in some uh some other services. We can also think about LLMs being embedded in other digital products. say the Microsoft uh Microsoft co c co-pilot you know uh or or over or over leaf uh mo most of these products are uh are increasingly uh being most of um are most of these products are increasingly um you know uh are increasingly uh capable of LLM functions. Now again these are quality improvements in these services. So we have to think about how do we capture some of these improvements in the quality of those products which could invol which should involve um some adjustment in price deflators. Um also another thing that we have to think about is um AI you know um creative outputs capital uh uh AI generated creative outputs. So if you think about uh if you think about LLMs um creating um advertising jingles right so in the national accounts um if you if a firm if a fir if a firm buys the rights to a certain jingle that's capitalized as intellectual property but what a firm starts using LLMs to um generate jingles to uh you know for their advertising campaign how do we value those um do we even capitalize those in the national accounts. Um so do do we capitalize those in the national accounts? So I guess you know there are probably ways on how to infer how much um AI diffuses to uh to other industries using um using um like data from investments in um research and development and software. But you know I guess like one thing we have to think about is whether we want to uh we we want to do so uh we we just rely rely on inference or do we have do we want to improve our you know input output tables or supply and use tables to have timely indicators of AI exposure to and how AI um creates forward linkages to other industries so we can uh perform um various types of analysis. Now moving over to the input side, we know that um a AI AI systems require a lot of uh inputs like physical infrastructure, data centers and um right now so you know we can think about we can think about this in such a way that if um if AI becomes transformative these these uh data structures will be spread uh spread across the world uh different locations. Now we uh we have to ask the question that can statistics reflect to which how AI systems you know uh can can official can official statistics reflect these things right so um also if we h if we want to think about the carbon the environmental footprint of AI environmental accounting is usually a national is usually a national um a national exercise right so I guess the one of the ways uh moving going forward is to think about how can we improve our capital tables uh in order to reflect those uh data centers um that are spread across the world. If we think about these data centers as strategic infrastructure, we we want to make them visible in uh in our uh in our official statistics to think about um how exposed we are and you know and go into the put the environmental footprint of AI perhaps um perhaps we can think about um multi-re improving uh multi-reional output tables to better reflect those carbon footprints. Um if we als if we also want to if you also want to keep track of the AI workforce the problem with the AI workforce is that there's no standard occupation where AI where we can uh where we can say uh people that works on AI right so it's usually spread across different occupations and it's usually specific in terms of roles and skills so if we want to keep track of AI workforce I guess one way uh one way to go move forward is to um is to um you to improve granularity in some of the statistical instruments that we have. So if we have our currently our labor force surveys are usually good at you know well you know they they they capture occupations right. So um I guess one way to move forward is to more is to think more about capturing task and time devoted to task right to uh to perform some of the analysis that we need for um uh to keep keeping track of this work workforce and perhaps maybe also think about complimentary data sources like time use surveys or admin data or maybe admin uh online platform data to have a better grasp of what uh what the AI workforce looks like I don't think we don't think that you know current statistical instruments such as labor force surveys are are enough to do that um still in inputs if you think about um AI they they use a lot of training data now the 2025 SNA recommends that um data be recorded as an asset in the 2025 SNA um I guess you know there are a lot of issues regarding measuring data Um we can talk about you know like depreciation rates or um like price deflators. Um but you know a a good um some fundamental problem with with uh with this uh with this exercise is how do we really capture the value of data especially when the value of data is very context dependent. It depends on how the data is being used and they're not and uh and usually data is not observed you know the the uh the sale of data is not observe in uh market we don't usually observe um the the sale of data in market transactions right um now moving forward to uh moving forward to the the indirect impacts of AI the second order effect of AI we think we can think about how AI would substantially improve the value the the the quality of our goods and services or products and services. So for instance you know we can have AI assisted diagnostics chat bots in uh virtual assistants again these are quality improvements in services right so now I guess one thing that we have to uh we have to think about moving forward is to how to better capture those changes. they might not uh they they they would likely not uh reflectly not reflect in the value added of these services or gross output of this service as measured by the national counts at the moment or they might not reflect in how you know the the the price deflators that uh that we currently use. So this so moving forward we have to re we have to really think about how to capture these quality adjustments in price in in the price of plators that we're going to be using in uh for official statistics. So you know one of the some of the ways that we can uh we can uh we can think about moving forward is to use non-traditional data sources can probably use market analytics platform usages and computer sentiments to you know comp to s sort of augment so uh sort of complement some of uh some of the statistical tools that we're currently using now also you know that's just the the market side there's also an issue of the public sector side um you know um right now public sector output this measure in terms of cost. There are a lot of efforts to measure um quality adjust quality adjustment quality adjusted um public sector output or public sector productivity but mo mostly those uh efforts are centered on measuring outputs over inputs. So you know if you think about um if you think about the health sector the number of um consultations that you that we have or in terms of police the number of arrests I guess you know if if AI is uh going to be very transformative in such a way that u AI would you know in improve public sector productivity I think it's better to start thinking about outcomes rather than outputs right so that's some of the things um also I think there's a lot of discussions in the past few days on how AI would transform the nature of work. So you know there's a lot of automations uh there's there's a lot of there's going to be a lot of automations this going to be uh reflected in um you know this this going to be reflected in many ways I guess one of the ways that uh one of the things that uh compilers of official official statistics should um should look at is you know we they have to uh keep track of the labor share. Um so there's going to be a lot of reorganization that's going to be happening in ter in in firms and again um and and again these uh these changes are going to happen in the task level right so if we're going to seize a lot of uh organizations we have to think about capturing these changes in these tasks that are going to be changing within the firms and these are not captured by uh by by uh by our annual uh annual firm surveys or even labor or four surveys that are currently being conducted by comp by by official statistics. Um we all we also think about whether or not there's going to be an uh productivity uh a paradox of efficiency. So the way we can think about this is you know AI would going to be eliminating a lot of inefficiencies uh would would be eliminating a lot of inefficiency she and efficiencies in the short term. Um primarily by compressing some labor intensive t uh task into you know or timeconuming processes. um this will if this will this would likely the way we think about this is this would likely result to a shortrun decline in output but you know um I guess general equilibrium in um general general equilibrium effects would eventually raise output in the long run but um yeah that's something that official statistics have to uh compilers official stat statistics have to uh be mindful about So yeah, so moving forward to how we think about um time and how we should measure changes in time. So I guess you know there's I think there was a lot also a lot of discussions uh in the past few days about how uh the potential of AI to free people from routine type co cognitive co cognitive task right so you know at the present um so at the present we have labor force surveys but again um so but yeah but but but again how do we capture these changes in uh in task right so h how we capture these uh uh task right so so yeah at at present it's very challenging to capture the shifts we uh using only hours works in uh and employment data would not uh would not allow us to reflect some of these changes in official statistics or you know what's happening with the wage distribution so we really need more granular data in that aspects as well um also going um going forward to um h to household production you know um I guess um one way we one way we can think about this is AI would have the potential to improve household production. Um we can have um you know right right now we have AI powered vacuum cleaners. We could have humanoid robots in the future doing a lot of the household production. Right now um of uh compilers of the national accounts produce this the household satellite account that captures household production. Um most of the time the way they do it is to use wages as a proxy for how much the how much value house household um h the value of the of these services these household services right or the value added from these uh services when uh when we start to sh when when we start to shift to uh to more automated household production I think we have to rethink how we can value these h uh uh household productivity. So I guess you know this as just as a final remark um you know recog so we recognize you know recognizing AI as a separate software is a small step towards the right direction and in the SNA the SNA is already moving towards that but I guess our main point here is that exi existing statistical frameworks and statistical instrument will struggle to capture AI's broader impacts right so we need what our our our main point here is that we need granular data outcome oriented measures and we need more collaborations with business for for uh for the uh with uh with businesses for uh for their data and more global collaboration um to produce this information. Um this is not an exhaustive list. I'm sure you uh many of you can think about other things that uh official statistics should uh should improve to measure to better measure the impacts of AI. Uh but these are the ones that um we think are important. So yeah, I guess that's my last slide. Thank you. [Applause] Um, okay. And thank the organizers for inviting me here to to uh do so and yeah, let's sort of start. So um this is a very strong paper. Uh we really should really should take the time to look at it. uh it really takes on a broad range of issues as John has shown and and really in an attempt to address sort of policy issues surrounding what the heck is happening to the economy with AI. The central point of view is that that there are changes that are taking place now and we need to put new data structures in place so that we can track the mechanisms of change that occur as we move towards trans you know transformative AI. Um, so it's my talk will sort of toggle a little bit between the now and where we're going. Um, but I I think that that was the spirit of of of their paper, which is that um if we don't know where we're going, um, we we really have to cast a a a broad net. And and that's what they've done in this paper. They step us through a whole lot of of needs and challenges. It's a pretty long list. Um, and it's always asking for more granular data and more information on what the mechanisms of change would be. Um, so and that isn't what we have in our data now for instance. So, um, so the topics I discussed a bunch of topics in my review and I'm not going to do that here. I'm just going to talk about three things. Um, and I'm going to go back and forth between the here and now and where we're going in the future. So, I'm going to start with considering the technology that we have now uh and how it would be depicted in data. And this is just a simple u graph that I think sort of says a lot um in that we used to be in a world where software was handcrafted or where the knowledge was given by human experts. Um it was used widely in business with the IT revolution. Um but you have human pro you know you have labor developing software and that it's being used and it's assisting in some way uh to produce output at the um firm level. Now we have machine learning AI and we have a whole other structure for producing a software which is now AI software or AI enabled software now um it's not TIA yet um but the the stuff is being produced now it's being put into place and um and used in much the same way uh as a as the traditional software was used. So what Diane and John do and I've changed the output to outcome. Um and and I really should have animation where I blow that up sort of exploding uh like yesterday. Um because that is just really where the explosion is. outcomes are at the firm, in the labor market, in the household at a minimum. Uh let's forget about the public sector for the moment. Um and if we want to know what's going on, we have to have adequate data to track what's going on at at all these all these points. So, a lot of this has to do with how we interpret AI um or TAI. Um a narrow way would would be to just look at software like that graph uh suggested. Uh but AI is broader. Uh we've heard about a lot a lot here so far. Uh, one thing that I think is consistent with some of the issues um that were brought up by Dan, I wouldn't say he really talked about it. Um but if we have this these sort of what I see as complimentary views of what's going on one is the task approach which looks at things through the labor market lens and the other is a um uh you know the the organizational change view. Um, I think that we didn't see a whole lot of the organizational change view in their paper, but it's a pretty simple matter to think about it. Um, there's a recent McKenzie report that I thought was interesting in that they they assess the likelihood of business functions within firms. And this is a generic firm. It could be producing anything, but these are sort of functions that are common. across firms um and they assess the likelihood that it would be transformed well in the near future by generative AI. And basically you can see at the top with a light blue dots that I look at those and I say oh that's happening now that isn't happening in the future. Um, and if it's just only happening in a small way now, it's going to happen in a big way in in the future. I think what's also interesting is what's at the bottom. Um, it's not going to stay at the bottom. Um, but no, pricing is at the very bottom and then corporate strategies just a little bit up from that. Um anyway, this is just an assessment of of how you could think about how firms may be changing organizationally. Uh what's going to be outsourced, what's going to be insourced, uh what different paths they may take if you want to use Dan's model. Um might emphasize supply chain versus doing something inhouse. Um if we had data on business function, we could see how it was being transformed, but we we don't. Um and John and Diane call for data on tasks. Uh we have some that's enabled the task approach to be used, but we don't have information on the time that it takes to do a task. That's a big deal. Then they have I mean really the um discussion about time as a measurement dimension is is really uh a very nice part of of this paper and as many of you know Diane wrote a recent book and she's got a whole chapter in there on how this can this will be what we need if we are to keep track of what's going on in in households uh businesses too But uh definitely households. And then finally um there's many impacts on consumer welfare and we don't have very good measures of of of uh consumer welfare. So that's also something that we need to be thinking about. Um and a lot of people are but that's sort of where we're going. So um let me now back up. If you go back to that chart about the technology, um, software and software R&D is actually a natural starting place to think about what's happening right now. And I'm going to I'm going to just take a little time to tell you this because Eric started off this conference saying we don't see AI in the productivity statistics. I'm going to tell you that we really do and it's in software and software R&D. And if you just think for a minute what everything is around you, the tons of resources that are going into developing these AI models, um it shows up in the macro statistics, believe me. So I just use a standard framework of calculating production and use effects uh that's been used to estimate the economic impact of steam, electricity, it get the effects are big from software capital and software R&D alone. And by the way, software R&D covers not just the R&D that is done within software firms but also the software R&D that is embedded in other R&D. Okay. So um and I just have one of the numbers many numbers in a recent paper that I wrote here on this in this bubble uh which is that 50% of the growth in labor productivity that we experienced from 2017 to 2024 which by the way labor productivity grew at a 2% annual rate which is its long-term average by the way so solid rate of growth. Half came from 6% of the economy. The 6% of the economy that's largely around here. Um that's half. If we hadn't had these AI investments and the generation of more software, the production of more and more software, um we we would still be in a productivity slump. Okay. So, I think you can hang your hat on that. Um, but let's Oh, that's a chart of the software R&D. It explodes. It just it explodes. Um, I I I won't go into the details. Last 30 seconds. Yeah. Um, John talked a lot about quality adjusted prices. That sounds sort of like con. It is very conceptual and it is difficult. Here's here's a real problem. You have the price of Microsoft's Office 365 personal plan the same now with Copilot which has a large language model a GPT5 model embedded in it access full access to it costing the same as the product that was available in 2016. though they're not the same product. You have to make a quality adjustment. And oh, by the way, this year Microsoft alone claims it's spending $13 billion for the further development of co-pilot. So, it's not a matter of linking to show no change. I mean, you really have to figure out like what is happening here and it's a challenge. So with that um I have some backward-looking conclusions. There's tremendous problems now uh even isolating software like we did in our paper. Uh and forwardlooking we need new measurement dimensions time and business functions. Labor markets look at tasks not jobs and the consumer we need to conceptualize and measure consumer welfare in appropriate ways because free goods may be a problem now. Eric, you're going to be busy in the future because AI will exacerbate it. Thank you. [Applause]"
  },
  {
    "id": "ifdSUKrJxqM",
    "title": "Artificial Intelligence, Competition, and Welfare",
    "url": "https://www.youtube.com/watch?v=ifdSUKrJxqM",
    "presenters": [
      {
        "name": "Daron Acemoglu",
        "affiliation": "Massachusetts Institute of Technology and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=q-dKbN4AAAAJ"
      }
    ],
    "num_presenters": 1,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nSusan Athey, Stanford University and NBER\nFiona Scott Morton, Yale University and NBER\nArtificial Intelligence, Competition, and Welfare\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "In her presentation at the NBER Economics of Transformative AI Workshop, Susan Nathy explores the intricate relationships between artificial intelligence (AI), competition, and welfare. The primary research question revolves around how AI adoption influences market dynamics and the distribution of welfare across different sectors and populations. Nathy emphasizes the need to consider both the static and dynamic aspects of economic models, particularly in light of the rapid changes brought about by AI technologies. She highlights that while AI can potentially enhance productivity and welfare, its effects are contingent upon market structures and the timing of changes across various sectors.\n\nKey findings from Nathy's analysis indicate that the competitive landscape in AI may mask underlying market power dynamics, which can lead to increased prices and reduced innovation. She argues that the benefits of AI adoption, such as lower consumer prices and enhanced productivity, may not be evenly distributed, particularly in developing countries where the profits from AI may accrue to a small number of firms or individuals. Furthermore, Nathy points out that the presence of market power can exacerbate the negative impacts on displaced workers, especially when AI adoption leads to significant structural changes in the economy. This calls into question the effectiveness of traditional economic models, which often overlook these complexities.\n\nThe implications of Nathy's research are significant for policymakers, as they underscore the importance of developing robust competition policies that address the potential for monopolistic behaviors in AI markets. Additionally, transition policies are crucial for supporting workers displaced by AI, ensuring that the benefits of technological advancements do not come at the expense of broader economic welfare. By advocating for a nuanced understanding of the trade-offs associated with AI adoption, Nathy urges economists and policymakers to rethink existing frameworks and consider more realistic assumptions that account for the dynamic interplay between AI, competition, and welfare.",
    "upload_date": "2025-09-29",
    "days_ago": null,
    "has_transcript": true,
    "word_count": 5626,
    "char_count": 30184,
    "transcript": "All right, our next speaker is Susan Nathy talking about AI competition and welfare. All right, thanks so much for having me here. This has been such a fun couple of days. Um, so you might think I'm too old to get nervous before talks, but um I'm I am nervous before this talk because I actually didn't take any very much macro in graduate school and I had never solved the general equilibrium model before this summer. So, uh, be be gentle with me. Um, so one of the I I actually made this slide like 15 years ago and I actually haven't changed I mean changed it very much. Um, because I've been going to talks and panels and discussions about what's the impact of machine learning and digitization and AI on the economy and one thing that makes this very hard is there's just so many feedback effects. Now I think after COVID I don't have to talk on the details of this slide as much because we all saw so many feedback effects and we also were somewhat surprised about some things moved quickly like we could reallocate the location of white collar workers very fast but other bottlenecks we didn't expect appeared and sometimes we see that it's like just one bottleneck if I can't get the chip for the car there are no cars and the prices of cars go up by $10,000 and people are sad and containers and things so there's a lot of there's a lot of these um these feedback effects where when you have a bigger change like the transform transformative change in AI we're talking about um we really do need to think through these things and and and one way to do this is to talk about scenarios and that's something that we did in an earlier conference this week which I think is really helpful because otherwise just doing tweaks around the edges of your general your current state just doesn't get you there. Now I there's a that those pictures though we tend to in economics mostly reason about them in a static cross-sectional sense and we look for an equilibrium because it's too hard to do all the dynamics. Another observation I have and that I've been making in a lot of these arguments is that we really have to think about the relative timing of changes and some things move fast and some things move slow. So like if we're going to get to the point we're on the beach with drones dropping us dachquiries, we do need to think about whether it's even possible to get there because something might have moved fast and something else might have moved slow and we just can't get there. And so I'm putting there's no political economy on this slide. That's actually one of the things I might focus on if I was just thinking about that. But even just focusing on the pure economics, you know, we have these disruptions, these societal changes. Um we have prices and wages that you know inflation. Um we have restructuring of production. We have focused labor market disruptions. And some of those things happen very fast. But then other things like increased or improved government services, more nurses, more teachers, labor augmentation, competition policy, transition policies for workers. We've never been good at that. Why are we going to and and the idea that we're going to do it really fast now sounds a little bit unlikely. So in but when I think about all of these things and what to talk about one of one thing in many many of the panels and discussions and conferences I go to people are really not focused on competition policy and its implications. So today I want to just highlight things things that we already know but we somehow keep leaving them out of our models and analyses. Um, and so I want to kind of bring them to the forefront. So some people say, why are you worried about competition in AI? It looks pretty competitive. It's more competitive than phones or search engines. That's great, but like search engines look pretty competitive for a while and so did phones. Um and so you know you have to really think about like why why those were not competitive over a long period of time spending a couple years in doing full-time antitrust. You know you see across the whole economy an industry can sound competitive but then somewhere somebody's got a bottleneck whether it's a poultry processing facility or something else and whoever's got that bottleneck takes a big margin. They also tend to extract above and below them in the value chain. So like you know here maybe chips maybe that's getting relaxed but like chips can slow down the whole thing and create cost for the whole thing if they have market power. And whoever has market power also finds a way to figure out how to hang on to it in very very very clever ways because there's a lot at stake to doing that. Here's just like one possible vision of a future. When I do this longer I have like multiple visions. This isn't actually the one I think is most likely, but just, you know, things may feel competitive now, but like if AI and agents get very big, you know, that might like take over the phone and Google is going to be the one who sees your email and your location and has all the all the three million advertisers shined up and Google products and so on. So they're they why would they like let other people play very as nicely with on their phone? And so you might have a Google ecosystem with your home, your thermostats, everything else. If somehow chat GPT and Apple managed to get along over a long innovative period, you know, there could be a chat GBT ecosystem on top of the iPhone or maybe OpenAI buys Apple or something else, but there's another one of these. Um, and you know, there's a whole chatbt ecosystem say, \"Oh, great. There's a duopoly.\" But like that's not always great on the enterprise. You know, it is a lot harder to get to monopoly on the enterprise, but you can imagine a world where there's an enterprise assistant that takes over a lot of stuff even though other people continue to exist. So, you know, igopoly um doesn't seem crazy. And if the whole economy is building on top of this stuff, that's like a tax on the economy. So, what I want to talk about today is this is sort of a summary of the ideas. Labor saving AI adoption increases welfare broadly if consumer prices fall alongside wages and if workers and it's not it won't be too bad distributionally if workers are sufficiently productive in alternative sectors where AI is less substitutive or where AI augments rather than replaces them and of course governments and university research can affect whether AI augments rather than replaces them. A second key point is that and I'm like I'm working with the World Bank on their global development report for 2026 on AI and I've been thinking a lot about developing countries. Many countries will import AI and more broadly even if you're the US profits may acrue to a small number of individuals or firms and that might not lead to broad distribution and the macroeconomic feedback effects that we'd be looking for. So one insight I have is that you know people have been building a lot of like close economy task allocation models about automation but actually a GE model with an imported factor of production may be a better model for many countries or even a country that is participating in the value chain if it's concentrated and they manage to evade taxation or have political power. The simplest GE models though of this type emphasize mechanisms that aren't a good fit for AI. And so we suggest like focusing on more realistic assumptions to highlight the key trade-offs from AI that should be in those models. And then a key point one thing we want to really introduce and the trade models help us with that is that upstream market power for AI is going to affect wages, prices, industry structure and ind distribution. And in a trade model it's very easy to just see this getting sucked out of your country. Um and and also these high AI input prices put a wedge between the prices of goods and the labor input cost which can allow aggregate welfare losses for all groups in a country as well as even more focused harm on displaced groups whose labor is displaced. So competition policy as well as policybound transitions determines whether this is good or bad. So the way I'm going to approach this, I'm going to talk briefly about two benchmark macro models. Um I'm going to talk about some of the effects that I think are like robust and and are the ones worth focusing on and also highlight some that I think are present in normal models but may not be the ones worth focusing on. So in particular um when you know when AI comes in it's going to shift productivity. Now, usually you look at local effects like in tariff policy, you're looking at like, oh, some group is replaced and so actually you want to put on a tariff to help that that that group. One of the things I want to highlight here is that when you get these bigger changes, you may have kind of lumpy shifts in the the the sectoral skill bias in the sense that like you first may you might displace unskilled workers and then displace skilled workers or vice versa. So those bigger changes can lead to in principle more like double whammies on displaced groups like they can be bad. They can be hurt by AI and they could be hurt by tariffs that and they would also be hurt by market power. It's not not always but it's possible. The second point is in a we have a benchmark model with one non-traded good and we think about in the the common things that happen there in the literature are that like a one sector gets more productive and the problem is like they're sucking in all the resources they're sucking in all the labor. Um but that's not what we think is going to happen with AI and so those macro like benefits and costs kind of change. So then that leads us to our main model which is also flawed and it still has some of the bad like the bad assumptions of the basic models but it does at least allow us to isolate I think the things we're talking about in these rooms we're going to have discrete choice to automate and then when an AI monopoulos raises their price which would be like a tariff or something that normally would protect the displaced workers because you've just automated them all together it doesn't bring them back in. So, you can expand a sector without sucking labor back into it and that's going to leave the displaced workers um in in more of a bind. The second thing is that with like sort of thinking about like a discrete technology adoption, the AI monopouist has some room to play with and they can really suck out the profits of an industry which actually can restructure the industry. And so if you think about AI as an input, if that's has market power, we don't just get higher prices, we get less innovation, less variety, and so on. So that's another problem. If you're like an importing country, and this GPT is supposed to come along, it's not going to have the effects you want it to have. Okay, so that is basically what we're going to say in a nutshell. We'll see of how much of the formalism I get through. And I'm I'm going to I have equations on the slides. um but mostly you don't need to focus on them. This is just for the people who are used to these models. So it's concrete what I'm talking about. So the first benchmark model we have two traded goods a foreign AI input. So there's fixed world prices. It's a small country. And so what really drives the impact of a fall in the cost of AI or like which is kind of like the introduction of AI that's like going from arty to trade in the trade models. What really drives things is which industry is which sector is more skill inensive and so say unskilled workers there the impact to them from a change in price is driven entirely by whether the AI adopting sector is more skilled or unskilled. Um and the fact if the if an AI monopolist raises its price that's always bad for the country. Okay. Um, one of the things that we we highlight though is that it's not crazy again if you think about a more transformative change that like skill intensity as the input price changes could actually fluctuate. So you know you you at first AI might substitute for unskilled and then skilled and so on. So let me actually just jump here like this is the sort of the classic case people might think about. Imagine that as prices rise the the you end up getting less skilled. So as prices fall you get more unskilled. Then we can look at what happens to the wages of unskilled workers. And those wages will um those the wages are sort of the integral under the curve. So if we go from like the entry which is a low price to infinity a high price if that integral is positive then entry is bad for the unskilled um workers. But if you increased if you're going to get that overall integral to be positive then a little bit of a an increase in the price of x actually helps the harmed party. So it restores distribution and whichever side you're on, the party that's harmed is helped by a little bit of a of a raise in price. But if we have something that's more monotone from like a larger change, you could actually have the harmed party. So that's like the integral under there could have one sign, but the marginal derivative could have another sign. And so what could happen is that the in the the entry of AI could be bad for unskilled workers, but a monopouist raising the price of AI hurts them even more because you've gone you you flipped from being skilled um you it's it's a reversal in terms of like the techniques that you're using or the skill bias. Then if we move to one but the unrealistic thing about this is that you're exporting and so if when technology changes you can just like export to the world that seems kind of unreasonable um and there's also nothing non-traded. So a second benchmark model that people use is one where there's maybe a one untraded good and one traded good. And just to um kind of summarize what the macro implications again is very standard model is that you know if you get this new factor of production that raises your productivity you kind of the whole economy shifts into exporting B on the world market and you pull resources into that sector and that can that can um raise prices in another sector. These are bombol's mechanisms, Dutch disease mechanisms, but as as I mentioned, I don't really think those are very realistic if we're thinking about something where it gets more productive, but actually you you can't lean into it because like the whole world is hit by this shock. So there's not like all these people who want to buy your cheap exports and you um you're not actually pulling any labor. So the the classic kind of general equilibrium effects are are not there. So the third model we introduce is one that has a little more richness, but it's still like a very simple from an IO perspective. It's still like a very limited model, but it has at least a few of the main effects we might want. So, we're going to allow there to have both sectors be non-traded because we're going to that's going to be a shorthand for the fact that you can't just start exporting your way out of this that really the marginal utility of each sector is is diminishing and that's going to limit how what where you can go with your newly productive sectors. Um, and we have in the in the AI producing sector, we have free entry and differentiated products. So we can think about sort of welfare effects of of fees. And so we have a little simple model with free entry. There's um the there's markups um and we have quantity determined in equilibrium. Crucially the fees you pay to the monopolist will be transferred abroad. And so that's that's sort of it's like a a following your terms of trade. you're but you're not able to um con reconsume feed those fees back into the economy. And so then the the welfare ratio can be decomposed into the I into a a cost effect. And so if AI comes in and it's very cheap, then costs go down and that's good. But if a monopouist charges you a high marginal price, then those costs don't go down and maybe you don't get a lot of benefit domestically because because the AI provider extracts the surplus and actually you still are paying a marginal cost for labor. They they have to lower it because when you've fired all the workers, the wage goes down and so you you you still can't charge too much because there's all these fired workers floating around and so people might not adopt your technology. they might just stick with the low-wage workers, but still you can raise the marginal cost of this thing up to the wage of the of these displaced workers. And so you may not get a marginal cost benefit once you funnel through the market power. If the monopolist fee is going to be a fixed fee they charge, they can also maybe use nonlinear pricing. If they use nonlinear pricing, they're going to extract surplus that way too. So there'll be less domestic firms. your industry will be more concentrated. It'll still have scale economies and maybe more scale economies because now you have to pay this fixed fee in order to survive. Um aggregate income can shift and there can still be um price effects in in the the the um non-traded good. So if you um put all that together um this is formally showing that that you know this thing this could be great if like AI is free as you could still have distributional problems but aggregate it would be fine but if you but if you have AI expensive you have distributional problems and aggregate problems. Um, so, uh, so let me just, um, I already said that verbally. Let me just, um, jump to the the monopoly part, which is the part that's that's technically new, although I think it's very intuitive. Um when we when we think about the pricing of a foreign AI provider, they do have to think about whether the firms will adopt the deviation condition for an for a firm to adopt the AI technology or not. When there's a fixed cost of adoption, it depends on the equilibrium wages in the other sectors. And as I mentioned, those will go down if you replace them, if everybody else adopts. Um but so the so the AI provider is going to figure out how to how to price generally they're going to be on the frontier. So they're going to extract as much surplus as possible so that the firms are are not tempted to to use cheap labor. Then we show that you're generally going to be on the frontier that actually unlike a standard IO model you'll use both a fixed fee and a marginal fee. That's because both of them feed into income and both of them ne neither of them is is efficient. So it's not the case that you want low marginal cost and you just extract a fixed fee. You use both tools. And if if the if the policy maker just tries to cap the marginal fee, they'll increase the fixed fee. And if they try to cap the fixed fee, they'll increase the marginal fee. So you really need sophisticated regulation of prices to keep things in. So just to close up with policy implications, the first point is that looking at a closed model where the profits recirculate is may miss a lot of stuff and I think many people have that in mind when they think about the problem of having a capital intensive economy but now let's go even further like you can't really tax it. It's very hard to tax it. It's leaving. Um a second thing is that this um non-traded scarcity and transition and issues um if we have people getting moving into another sector, we need to really think about what are they going to do in that other sector. Are they productive in that other sector and are there things we can do to make them more productive or can the government actually procure more of some of those sectors like nursing and so on and can they invest in making the workers more productive? that's going to really determine what happens especially on the distributional side and then industry structure and variety you know we this monopoly power is actually going to hurt innovation in the local economy in a whole bunch of ways so in the simple CES framework it's number of varieties but I think that all of this could be extended so I'm out of time I'll just I'll just mention that I'm not trying to think that this is a very different model than anything has been done before except for like the monopoly stuff. Um, and the the effects are familiar, but I really want to focus people on the on the crucial importance of the market power for like the sign of the welfare outcomes and also the crucial role for policy and like mitigating those effects. And I hope people will figure out some good workhorse models that where it's just easy to incorporate those effects so we won't just assume them away for convenience. Thank you. All right. Thank you, Susie. Well, I wanted to say of course thank you to the organizers and I want to talk about first of all why it is that this paper is so fantabulous. So let me sort of uh let me let me set this up. So if you think about the mission of this conference, it's that we're told to suspend belief and believe that we'll be transformative AI which is going to replace labor in far less time than we think. And we've been told to think through that. And then a natural response to that assumption is therefore to focus on how that will affect humans as suppliers of labor and you know as a result you know think less about humans as consumers or people who buy stuff. But what makes this paper fantabulous is that Susan and Fiona have really worked with this idea that we have a labor uh replacing technology and explain to us why consumers and the way that people consume still matters even if that is our primary assumption. So that is what makes this brilliant and I would say very sort of cunning to bring back competition policy into this uh into this volume. Now um what I'm going to do uh is I'm going to first of all in this discussion sort of say some incremental stuff about how I might slightly rewrite the paper uh and then go on as is appropriate for a discussion in a conference such as this with my own musings on the topic. All right. So, my major comment about the paper right now is that it is a paper written by exceptionally clever people for exceptionally clever people. And that's brilliant, but I'm just wondering if, you know, maybe spelling out um the purpose of the paper a little bit more simply might mean that when this volume goes out on the internet, lots more people get why it's so important. And so my thinking about this is just that it's okay to double down on the fact that this is taking a model of trade and just you know in the way that in the presentation you have this sort of lovely set of bullet points saying that a lot of people ultimately in the economy uh will live in a country where AI is imported and we need to think about that just sort spell out why that's important. I mean, because in some ways, you know, you could read this paper as being the ultimate EU nightmare, right? And see sort of nod, right? This is a basically a EU nightmare of a paper. um you know a completely different audience could be uh you could go into Washington and talk to a very different kind of policy maker and say look guys this is the consequence if we lose the China right and that might play well with some other kind of policy makers and then of course what Susan knows very well and I was just so touched that she mentioned it during her talk she's spearheading this effort at the World Bank to really think about the consequences of AI in the developing world, you know, and when we've got say agrarian sectors and large agricultural sectors, you know, this is so important to really think this through. And so I think just sort of spelling that out for everyone, you know, it it's probably incredibly obvious to you, but just like sort of spelling it out, I think would really make the paper up front and get people to sort of work through it, right, when it becomes um, you know, exceptionally clever. All right. Uh so maybe I just sort of like basically say we just sort of start the paper saying many countries import AI and that sort of helps people understand why this is such an important approach. All right. Uh so now let me having sort of just um managed to sort of done down the paper but hopefully increase its audience I you know I I wanted to give some of my own thoughts um just when what I thought about when I was reading it. So, as SA mentioned, you know, the main model is model free and you know, in it, she gifts our um AI producer with uh nonlinear pricing. And you know, and it was interesting when you read the paper that's talked about quite a lot in terms of its implications. You know, I thought maybe that was exciting and then I was like, no, no, no, that's just sort of standard, you know, rent extraction. You have two part tariffs, it's easier to extract rent. But then, you know, as I was musing about this, it made me realize the importance in some sense when we're trying to build these models of thinking hard about exactly what that monetization model is going to look like and then sort of thinking through how it enters the production function. So, you know, this is this is not for this paper. These are just my musings. But I was reminded, especially because we're back in Stamford, and I was going back to my PhD days taking IO. And at that time, Tim Breznahan was uh teaching IO and obsessed about Microsoft. And so basically, we just have semesters of Microsoft. And that was wonderful. I learned a lot. I learned a lot. But um one of the uh one of the things that's always struck me um is him describing how antitrust regulators at the time when they were thinking about sort of how to encourage competition in this new world of the internet, you know, got very fixated on the browser and then their theory about how market power is going to manifest actually was in the channel. Now you probably probably most people are too young in this room to remember that browsers used to have this tab called channels. Uh if we clicked on that it would give a take us to all or Disney or something like that. And there was a big fixation with the idea that that was going to be the major instrument of pocket power and that uh the browser would be able to extract very large fixed fees from uh content creators and that would have some of those horrible upstream consequences of concentration that Susan was specking about here. you know when you look at the uh the the sort of findings of fact about this you know the actual monetization model of a lot of the internet was never mentioned there was no talk about advertising there was no talk about data you know and it's just interesting and so then I sort of started thinking well given that in the internet browser era we maybe had the wrong monetization model and as a result maybe looked at the wrong things you know what can we think about what do we know about what monetization might look like uh if we want to uh regulate it in uh in the future and generally economists of course we don't really like making predictions because we know we'll probably be wrong right so we don't like that um but what I discovered on Wednesday was there this entire field of scenario planning and where they make really precise predictions like tables of predictions and uh you know Susan was referencing it was such interesting exercise but we were given these tables of predictions about the economy and you know that sort of struck me and and this is what I wanted to just just bring to the rest of the group. So the way the scenarios were presented to us is that you know at least we started off I think it was a continuum of not a bad scenario doomsday scenario and the not so bad scenario was sort of like the s more typical IT economist belief that change is going to be slow not much happens uh whereas doomsday scenario was that basically we have sort of a a madmax sort of like just terrible wasteland Um and you know the way they parameterized monetization in these scenarios was interesting. Um in one of them we had a fremium model oddly specific premium model. Um and then doomsday it was that AI monetizes personal data at unprecedented scales. Now the first thing which is interesting to me is honestly they actually look quite similar to me. I've got to be honest. I I look at them and I'm like, \"Well, why are you giving away this product for free to 95% of people if you're not monetizing data, but you're not allowed to crawl crawl with it.\" So, that's just what we were told. But anyway, however you take it, given that they seem similarish and you know, these are what young people were sort of thinking along the margins is going to be happening. uh you know it does seem good to sort of think well you know what happens if we end up in this world where actually monetization is done through data again uh maybe through uh some kind of advertising support you know how will that affect all these models right so it's sort of got some interesting implications because it's almost Susan as though your model is in the good world right where we actually have a price um now it depends you Again uh I just want to be clear I'm not clear I'm not sure in a world where the actual monetization model is the monetization of data is the big problem is going to be use various ways of interpreting it I think what we could say is that again we'd be in EU nightmare world right if we monetize data and that becomes the uh primary engine of uh monetization but we might again end up in a world where like today uh rich eyeballs in developed countries cross subsidize a lot of technology for the developing world. So it's you know it's interesting to think through uh this scenario. All right. So that's really my big thought. Obviously this was a terrific paper. I want to again give all credit to the authors for basically taking the scenario given in this volume which was so focused on labor supply and bringing it back to people as consumers and reminding us why that matters. And I think that's just a huge contribution. So with that, I'll say thank you. Thank you, Catherine."
  },
  {
    "id": "FBdf9Y70fMg",
    "title": "2025, Economics of Transformative AI Workshop,  \"AIâ€™s Use of Knowledge in Society\"",
    "url": "https://www.youtube.com/watch?v=FBdf9Y70fMg",
    "presenters": [
      {
        "name": "Austan Goolsbee",
        "affiliation": "University of Chicago Booth School of Business",
        "scholar_url": "https://scholar.google.com/citations?user=NfK5YqkAAAAJ"
      },
      {
        "name": "Chad Syverson",
        "affiliation": "University of Chicago Booth School of Business and NBER",
        "scholar_url": "https://scholar.google.com/citations?user=tBelJ_AAAAAJ"
      }
    ],
    "num_presenters": 2,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nErik Brynjolfsson, Stanford University and NBER\nZoe Hitzig, OpenAI\nAIâ€™s Use of Knowledge in Society\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The presentation titled \"AIâ€™s Use of Knowledge in Society\" by Eric and Zoe at the NBER Economics of Transformative AI Workshop explores how transformative AI could fundamentally alter economic decision-making structures within firms and markets. The central research question examines the implications of AI on the traditional economic principle that decentralization is efficient due to the localized nature of information. The presenters argue that transformative AI challenges the assumption that local information is inalienable and that human information processing capabilities are inherently limited. By utilizing an incomplete contracts framework, they analyze how the ownership and control of both physical and information assets can shift in light of AI advancements.\n\nKey findings indicate that as AI technology evolves, it increasingly enables the codification and transfer of previously tacit knowledge, which could lead to a preference for centralized decision-making over decentralized approaches. The presenters illustrate this with a comparison of two scenarios: one where information assets are inalienable, which favors decentralized ownership, and another where AI allows for the movement of these assets to a central authority, potentially enhancing economic efficiency through better coordination. Additionally, they highlight that transformative AI can expand the capacity for information processing, further tipping the balance toward centralization.\n\nThe implications of these findings are significant for both economic theory and organizational practice. If transformative AI continues to erode the boundaries of local knowledge and processing limitations, firms may increasingly centralize decision-making to leverage this newfound efficiency. However, the presenters also acknowledge potential counterarguments, such as the enduring challenges of codifying certain types of knowledge and the possibility that AI could empower local decision-makers. Ultimately, the research suggests a reexamination of the dynamics between centralization and decentralization in the context of AI's growing capabilities, with important considerations for policymakers and business leaders navigating this evolving landscape.",
    "upload_date": "2025-09-29",
    "days_ago": null,
    "has_transcript": true,
    "word_count": 4080,
    "char_count": 23484,
    "transcript": "All right, welcome back everybody. Our first paper in this session is on AI's use of knowledge in society uh with uh Eric and Zoe. Zoe's presenting. Great. Thank you so much and thanks everyone for the exciting workshop so far. What we'll be talking about today is the potential for transformative AI to transform the structure of economic decisionmaking in firms and markets. And in particular, we'll talk about how transformative AI invites a reappraisal of some canonical arguments for the economic efficiency of decentralization. So just to all get on the same page, you know, organizations and markets are information processors which take in information and knowledge and produce allocation decisions. And of course this gives rise to a crucial principle in all of organizational economics and thinking about the boundaries of the firm which is that it always makes sense or often makes sense to colllocate knowledge and decision-m give decision-making power to those with the relevant information. But then of course this principle gives rise to a tension between decentralization and centralization. When decisions are largely interdependent or even a little bit interdependent, all else equal, it makes a lot of sense to have one decision maker coordinate all of the decisions so that you have a a large amount of coordination um across these interdependencies. On the other hand, you know, going back to Hayek who whose famous essay titled the use of knowledge in society we are largely inspired by. He argues that of course all else is not equal. A lot of the information that we need to produce allocation decisions is dispersed. It's tacit. It's uncodifiable. And so often decisions are best made locally. Another way of putting that, just to kind of hammer the point home in language that will be more amendable to formal analysis, local information is often inalienable. It's trapped in particular physical locations, in particular minds, in particular circumstances of time and place, as he says. In addition, information processing capacity is often bounded. Right? If you give me, you know, all of the information that Amazon has about its um about who's buying what, I with my brain cannot process all of it to decide how to stock Amazon warehouses. And what we'll be arguing today through a really simple property rights type framework is that transformative AI really begins to erode these premises. It begins potentially to erode the idea that local information is always inalienable and that information processing capacity is bounded. So I'll spend most of the time today focusing on this argument which will advance with like the scaffolding of of an incomplete contracts framework. We'll talk a little bit about counterarguments and I think we'll have more of a chance to talk about counterarguments in the discussion. We'll also give a little bit of early empirical evidence though we're not supposed to focus too much on what's happening right now um for the premises of this conference and then we'll close with some concluding thoughts on what this means for politics. So, as I said, we'll be working within a kind of textbook incomplete contracts framework with two agents and two assets. We'll think of one of the agents as an entrepreneur or or a local manager and the other agent is a headquarters, the kind of central node in some broader um organization. One of the assets will be a physical asset, a tangible asset of some kind and the other will be an information asset which is initially possessed by the entrepreneur. To make this concrete and to fix ideas, we could think of the physical asset as some kind of retail store um that and the entrepreneur is a local manager and the headquarters is the headquarters of some franchise. The information asset here could be local knowledge possessed by the entrepreneur about local demand, local customs, potentially some local relationships. Now, of course, the key idea of an incomplete contracts framework is that the residual right of control, whichever agent here has the residual right of control, is going to influence the incentives to invest for both parties. Right? So here the ownership regime is what we'll call row which maps each of the assets into one of the two agents. And again this is nothing um particularly uh new here. Um, we have once the ownership regime is determined, the agents make their investment decisions taking into account the potential for unforeseen contingencies that bargaining will break down and therefore taking into account whoever has the residual right of control. Importantly, we're assuming here that there can be no production at all unless both assets, the physical asset and the information asset are both present. And that's relevant both for the joint surplus, which we'll call V, and the solo surplus, what each agent can do on their own if they have ownership of those assets. So, we're going to compare two cases. Now the first case we'll think about is maybe the case that Hayek had in mind where information is inalienable. It cannot travel easily across boundaries of the mind across boundaries of time and space. And because in this information asset is inalienable, we only have to consider regimes in which um in which the entrepreneur continues to operate that asset. And what we're really considering here is what happens when you shift control over the physical asset, which you know again we can think about as the physical store. Um if we're imagining some retail chain. So going to go through all of this pretty quickly. It's in it's in the paper and all of this is is sort of the the usual intuition from incomplete contracts. When the entrepreneur owns both of the assets, notice that there's an extra term in their first order condition. When they are optimizing and choosing their investment level, they are going to choose a higher level of investment because they are foreseeing that they in the event that um in the event that bargaining breaks down, they will be able to continue to operate that store and continue to reap the benefits of the investments that they've made in the joint surplus. Whereas on the right hand side um on the right hand side the headquarters will not be taking that into account because they will not be able to operate um and will not be able to reap any benefits of the relationship specific investments they made. So there's a very clear story here that when the information asset is inalienable then we know that the entrepreneurs ownership or decentralized ownership yields higher joint surplus. Let's now think about the case which we think is a reasonable case as we begin to see what transformative AI is capable of which is that information assets are in fact more and more alienable and this can come through a few different channels. On the one hand transformative AI has the ability to digitize explicit knowledge. We see this in benchmarks that measure knowledge um on like specific PhD level tasks um like GPQA or the MMLU. These are sort of knowledgebased benchmarks that large language models are doing very well on. Transformative AI is also beginning to codify tacet knowledge. So things that we used to think of as very hard to describe and you can't really you can't really communicate them in rule-based ways. Well, now we have cotification of various forms of tacet knowledge like language learning, like driving a car. AI is learning how to do these things. And of course, there are forms of knowledge that are being discovered by TAI that were never even um able to be represented in a human brain um especially in the medical domain. So now let's see what happens in our framework when we compare the case where the entrepreneur owns both assets to a case that is now made possible um by transformative AI. A case where uh where this information asset can be moved to the headquarters. Here we have a very different story than what than the story that we had before which is that when the information asset is inalienable now the surplus maximizing regime will depend on the marginal values of the two investments and also going back to what we said in the beginning if there's any sort of interdependence in coordination or sorry interdependence in decisions then centralization will often weakly dominate you know for other reasons. um as well even if these marginal investments are sort of up in the air. So just to take stock, we've looked at two different cases and we've seen one particular way in which transformative AI starts to begin to point more towards centralized control as opposed to decentralized control. We'll mention one more potential channel through which um transformative AI may be pushing economic efficiency toward uh centralized control and that's this point about information processing limits. As I said in the beginning, you know, if you give me all of Amazon's data and I don't have any um any access to statistical models, I can't do all that much with it with my one brain. So in some sense in the past before we had any kind of information technology let alone the technologies we have now there was a real limit to how much information any one person could process. So if we imagine a case now and I'm going to do this a little informally for the sake of time. If we imagine a case where the headquarters is now contracting with many entrepreneurs and let's imagine that if we look at each of those bilateral situations independently the headquarters ownership is optimal. There may still be a limit to how many information assets you can move to the center. And yet, as transformative AI pushes this bound on information processing upward, we see that more and more information assets can be moved to the center. So again, yielding new possibilities to tip the scales toward centralized control over assets and decision- making. So those are the two channels that we want to we want you to be thinking about when we think about the new possibilities for centralized control. And now we'll talk a little bit about the potential counterveiling forces that may push against these two forces that we've outlined here. Well, first of all, I can imagine that some of you are thinking that there are limits to the kinds of knowledge that can be codified. That's definitely um a theme of today. You know, we're talking about what what exactly a genius is doing. Um and of course, today's systems have certain challenges that may or may not persist. Um AI challenge AI is very challenged when dealing with tasks that aren't well represented in the data set. Challenges with the long tail and of course there are a lot of challenges in embodied knowledge and robotics and there are things where it seems to be that that humans will continue to have an advantage and you know have forms of knowledge that cannot be codified. Um I'm going to say that to some extent this is fighting the premise of the workshop. Right? We're we're trying to take really seriously this idea of transformative AI um of the the geniuses in the data center. You might also think that even a fully AI based economy could be partially decentralized. And now what you might have in mind here is that there are just literal like physical limits that will always persist. If you have a rover on Mars that needs to be making decisions in real time, it may not be able to ping back to the control center on Earth and and may need to make some autonomous decisions. In that sense, we'll argue that those are sort of small cases and we could we may still see a world that would be vastly more decent more centralized than it is today. You might also say that AI can empower the periphery as much as it empowers the center. You know, we we might imagine that all of those same forces, all of those codif all that codified knowledge may find its way to the edges and empower those those individuals, those local store managers to really powerfully operate on their own. But again, I want to point out the real power of interdependence in decision- making, which makes it so that coordination is so much easier when a single decision makers is making all of the decisions. I also want to point out that there's emerging evidence in machine learning and reinforcement learning uh that suggests that single agents can often beat complicated mixture of experts, multi- aent setups. There was an interesting paper from Salesforce AI that was looking at single agents versus multi- aent setups in search that came out just last week and showed that single agents are much better. So, it's an interesting area to be watching in machine learning and reinforcement learning. The last point I want to make that we can come back to later on is that there may be legal requirements and other forms of regulation that naturally put a limit on how much centralization there could be antitrust occupational licensing and other sorts of legal frameworks and ideas like whether AI will have certain legal rights to hold bank accounts or to have various forms of personhood or various forms of liability. All of those decisions and legal frameworks will really affect the degree of centralization that is possible. Um that is seems absolutely correct and those sorts of questions are more in the realm of politics than economics. Here we're advancing an argument about economic efficiency and what kind of direction economic efficiency will push toward. And I have to say at this point it's a little late in the talk to mention that of course I'm not we're we're not the first people who have ever made these kinds of arguments right I mean this is a a long debate that goes back to the socialist calculation debate and of course Hayek sort of won that of course there's a a war fought over these ideas that was the cold war which in some sense decentralization one there there was there were ideas about how computers could run an entire economy from a control center which is this picture which is project cyersin from Salvador Aende's Chile which he didn't get a chance to um see play out and it failed. So we might be asking is this time different? Why should we think that there are new possibilities for this kind of centralization on a massive scale when all of these things have have failed in the past? Well, we think the technology is fundamentally different. And we also think that there are forms of centralization that are on a nearly national scale that are already happening um today. This is Walmart's headquarters in Bentonville, Arkansas. It looks really nice. There's a like rooftop bar, like seven miles of uh biking and hiking trails. And you know, Bob Hall said it best when he said Walmart is in some sense the epitome of central planning, right? I mean, it's really incredible to think about how they managed to organize this national retail chain from one massive headquarters in the middle of the country. Um there's also rising concentration which we could think of as early indicators that things are moving in a more centralized direction. It's really interesting to hear some um leaders of highly leveraged AI native startups talking about their strategy which include you know instead of creating specialist silos they hire versatile generalists who can solve problems across domains. And just to be really clear as we move into this uh con concluding section, this is a positive argument. You know, we do not at all think that centralization would be a good thing. In fact, we're quite concerned about the potential implications of a highly centralized um economic decisionmaking world. Um and you know there's a literature on the various ways in which economic power begets political power. Um and of course we're concerned about how this could lead to decreases in incentives to invest in human capital and what that erosion of education might mean for democracy. And you know just want to kind of remind us all here we re we're revisiting this key tension trying to understand how things have shifted since Hayek wrote his famous essay and how things could shift further with transformative AI and we really just want to highlight that you know Hayek had a had an economic message as well as a political one. You know, he in in connection to his ideas about economic efficiency and what decentralization could promise in an economic sense, he was also thinking about what decentralization could promise in a political sense. And what this suggests to us is we might have to think really deeply and try to undo some of our intuitions that might come from a different technological era. Great. Thank you very much. Our discussion this AI AI you have 10 minutes. Okay. This is um really interesting paper that is thinking very big. So it was a lot of fun to read and try to understand and speculate. So thanks to the organizers for giving me this opportunity. Um so you heard the the starting point is this idea of the use of knowledge in society from Hayek. There's this uh framework that says what the price system is for is information aggregation. That's fantastic. It is too hard for any one individual to solve it. Um and even the single controlling mind in possession of all the data can't do it. Okay. Now in the '60s and 70s there started to be a push back on this idea that we felt that like okay well IBM might solve the problem or central planning might solve the problem and there was a genuine worry that like it wasn't obvious in the 70s as far as I understand that the cold war was going to be won by the United States and its allies uh and there was a sense that maybe computerization information processing was going to enable centralization Fast forward to today, we have, you know, maybe we're building a brain for the world. This is the second quote that starts uh the paper. Uh and it's not just the information aggregation, but the underlying processing power. And with information uh aggregation and processing power, enough knowledge, uh a transformative AI could pass could surpass the ability of the pricing system to allocate resources. Yes, the pricing system is amazing, but let's face it, it's not perfect. Maybe the machine can do better. And so this assumes a centralized large system. And so the takeaway from Bolson and Herdzigg is that power accumulates. Then they do a model. So that's the intro. And the model is a little more modest. Um that's what models have to be. and it says well let's start with a two-party and complete contracts framework thinking about an HQ and a franchisee and eventually they go to multiple franchises and the we're going to assume that AI codifies previously tacit local knowledge that the franchises had and we're going to assume it also lowers the cost and raises the benefit of this central information processing what's going to happen mechanically you can see that we're going to get centralized ownership centralized decision-making And uh if that were the entire economy, a push towards central planning. Then at the back end of the paper, it says, well, economic power leads to political power. And if economic power leads to political power, we should expect a more centralized authoritarian uh model of governance and perhaps even the end of good education. Okay. It's pretty dark. It's so dark as I read it that I don't think science fiction is dark enough. I could not find an example that crossed 1984 with uh the machine in control and sort of controlling all of us humans. So, um that's a possibility. There's nothing in this paper to argue like that's not that's not how this thing could play out. But I think there's another possibility. If we take their model as given, if we say this is the model that's going to lead to centralization, I can change one assumption and create a model that leads to decentralization, which is the model assumes tacet, tacet knowledge resides locally. But many of the things that AI does are not the things that are about the local entrepreneur. They're about the things that occur in headquarters. So imagine that all of us, every single local business owner had the marketing skills, supply chains, data analysis, and all these other things that their headquarters does. So if that happens, if we all have access to a country of geniuses in a data center, the franchisee doesn't need the HQ anymore. It needs the model and we got to think about the power of the, you know, embedded in the model, but it doesn't need the HQ anymore. in which case we end up with the opposite story. So if uh AI codifies previous tacet knowledge but it's the HQ's knowledge not the local entrepreneurs uh and if it raises uh lowers the cost and raises the benefits of information processing for everyone then we get dispersed ownership in decision-m and a push toward dispersed decisions. And I would argue we have no idea at this point which one of these frameworks is the right one. So you put forward an interesting hypothesis. It might very well be the right one. We might end up in a world darker than our darkest science fiction. But we might not. And um you even irrespective of the choices we make as a society to the extent that there's a Wii in that. So we could have individual empowerment through transformative AI. There's lots of research that suggests individual empowerment in creativity and elsewhere in entrepreneurship through digitization. Now there's some subtleties here because we you know some you know uh Joel Wald Fogle's work says well we saw decreasing power of the studios uh and the publishers relative to the individual creators but at the same time we had so we had decreasing power of the studios and publishers increasing power of the creators and of course increasing power of the platforms and how this all plays out uh in the context of AI I think is an open question. Now, what I just said isn't necessarily rosy. So, even if we get decentralization, I want to point to the paper we had right before the break, decentralization can be pretty awful, too. Decentralization of information uh can be really dark. Decentralization of the ability to create advanced weapons can be really dark. So, uh, this is there are forces towards centralization that, uh, as I read the paper were painted as quite dark, but that might be great. And if we're focused on the information environment and news, maybe that's wonderful. Uh, if we're focused on the access to advanced weapons, maybe that's wonderful, too. So to summarize um I agree that AI has this potential to centralize information processing and decision-m concentrating economic power and uh perhaps even political power. But with the same model, if we take this model as the right model for that conclusion, with a little tweak, we can get the opposite result. Both forces are likely at play. The same technology enables centralized coordination and makes every single individual more capable. So as an aside, the model does not include the third actor who is the AI producer. Okay? And that could change things depending on all sorts of stuff that remains unmodled. So you have the headquarters, you have the entrepreneurs. I think there's some implicit idea that the headquarters is going to become the AI model. That's not obvious to me. I feel like that's another actor that needs to be modeled. Uh and so I would argue it's an open question whether and when under what circumstances transformative AI is going to centralize or decentralize economic power. Thanks. [Applause]"
  },
  {
    "id": "6MSSvxDz06o",
    "title": "Science in the Age of Algorithms",
    "url": "https://www.youtube.com/watch?v=6MSSvxDz06o",
    "presenters": [
      {
        "name": "Aidan Toner-Rodgers",
        "affiliation": "Massachusetts Institute of Technology",
        "scholar_url": "https://scholar.google.com/citations?user=VQmXH8EAAAAJ"
      }
    ],
    "num_presenters": 1,
    "description": "https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025\nPresented by: \nSendhil Mullainathan, Massachusetts Institute of Technology and NBER\nAshesh Rambachan, Massachusetts Institute of Technology\nScience in the Age of Algorithms\nEconomics of Transformative AI Workshop, Fall 2025\nAjay K. Agrawal, Anton Korinek, and Erik Brynjolfsson, Organizers\nSeptember 18-19, 2025\nSupported by the Alfred P. Sloan Foundation grant #G-2023-19618 and Microsoft",
    "ai_summary": "The presentation titled \"Science in the Age of Algorithms\" by Sendel and Rambachin explores the transformative potential of algorithms and artificial intelligence (AI) in scientific research, drawing parallels to the historical impact of electricity on manufacturing processes. The core research question examines how AI can fundamentally alter the \"factory floor\" of scienceâ€”not merely by enhancing existing methods but by reshaping the very framework through which scientific inquiry is conducted. The presenters argue that the traditional scientific method, often viewed as a linear progression of theories competing for supremacy, is inadequate for many scientific disciplines, particularly economics. They propose that the real benefits of AI will emerge from its ability to identify and generate anomalies within existing theories, thus facilitating the development of new hypotheses and models.\n\nKey findings highlight that scientific progress often hinges on the identification of anomaliesâ€”instances where existing theories fail to explain observed phenomena. By employing algorithms to systematically generate anomalies from datasets, researchers can uncover new insights that challenge existing theories and prompt the formulation of alternative models. The presenters contend that this algorithmic approach could enhance the scientific process by making the discovery of new theories more structured and accessible, rather than relying solely on human intuition and creativity. They also emphasize that many scientific fields, including economics and chemistry, operate as \"patchwork sciences\" with no single best theory, suggesting that a collection of theories may better reflect the complexities of these disciplines.\n\nThe implications of this research are significant for both economists and policymakers. By recognizing the limitations of the reigning champion view of science and embracing algorithmic methods, researchers can foster a more dynamic and responsive scientific landscape. This shift could lead to more robust policy recommendations based on a deeper understanding of the multifaceted nature of economic behavior and other scientific phenomena. Ultimately, the presentation advocates for a rethinking of how scientific inquiry is conducted in the age of algorithms, positioning AI as a catalyst for innovation rather than just a tool for efficiency.",
    "upload_date": "2025-09-29",
    "days_ago": null,
    "has_transcript": true,
    "word_count": 5761,
    "char_count": 31159,
    "transcript": "Uh the first paper uh will be sendle's paper with a shish and um and it'll be discussed by John McCale. Uh so Sendel over to you. All right. Thank you very much. Um I'm just going to jump right in. This is with the Rambachin. It's science in the age of algorithms. I think um this is something that doesn't need much motivation. And I feel like if I was to use a canonical example, Alphafold has really uh been an example that has motivated many people to think that algorithms, AI, whatever you want to call it, can have enormous power in science. And so what I want to do here is I want to take the transformative uh charge and kind of step back quite a bit. And the way I want to do that is I want to use a possibly uh inaccurate but I think largely accurate uh story from uh electricity. The story goes that when electricity came uh its productivity benefits were not felt as strongly immediately and that was because it took some time to absorb electricity to get this. The physical nature of steam power is that there's a big thing producing energy and so the factory had to be laid out around the big machine. When electricity came they just replaced the steam power with wires. that had some benefits, but the real benefits came when they said, \"Wait, I can actually move the machinery any way I want.\" And so the entire factory floor changed and there were big benefits. That's the metaphor I want to carry through today. What was true of electricity and manufacturing, I want to argue that the big benefits from science will come not from electrifying, but by fundamentally changing the factory floor for science. And this was an interesting exercise for us because to reflect on the what is the current factory floor required a lot more stepping back and that's what I that's a journey I'll try to take you on today to do that I want to start with what I think is the default view that was in the back of my head of what the factory floor of science is and it's probably what we all think of when we say the scientific method. I'm going to call the scientific method the reigning champion view of science. There's a science that's the best we have. the theory that's doing well. We run a horse race and that horse race either the best theory supplants is supplanted by someone else and then that's the new best theory or that's  popper however you want to think about it the the sort of reigning champion view of science is what what I want to take as my as my default view and it's the view I think I have in the back of my head and many of us have and I will argue this is a good factory floor that is it's an effective factory floor for some sciences but it's a terrible one for many others including economics. And what I want to argue is that AI is not going to disrupt the factory floor because it's a technology in and of itself. It's going to disrupt the factory floor because the factory floor was simply not working for a whole host of sciences. It was an incomplete description of the act of science. But like with many problems in life, you kind of ignore the problem if you can't do anything about it. And I think we've ignored this problem for quite some time. I hope to convince you that this is true in economics, but I also hope to convince you this is true in a wide variety of other sciences. So the way I'm going to do that is I'm going to start with some provocations. Um, and then I'm going to describe based on that what I think the new factory floor will look like and then talk about what we need to get there. Great. So let me start with some provocations. So the provocations are each going to take apart a part of the reigning champion view of science. So the first part I want to do is well we're running these horse races. Where does the other horse come from? We've got a theory. We know how to deal with it. Where on earth is this other horse coming from? You'll notice this kind of an interesting problem to dig into. So let's go to something that I think we all know. Let's think about the evolution of expected utility theory. So expected utility theory beautifully formalized, beautifully aimatized. Then prospect theory showed up. That other horse arguably quote won one. It had many predictions. The other Z didn't. And now since then we've had a few more theories and they're still bubbling. Where are these coming from? It's like super smart people just being clever. Well, let's dig in a little bit. I think we can actually take apart where they're coming from one by one. After expected utility theory, a sequence of things happened. The most noteworthy of which it began with the LA paradox. LA said, \"Hey, this is a really nice theory. Here's two menus.\" And if you look at these two menus, in menu one, people choose lottery A. In menu two, people choose lottery B. It's like, yeah, that seems intuitive. Said unfortunately, that's inconsistent with the axioms of expected utility theory. Then there was something called the common ratio effect. And then Conori 1978, a whole host of these anomalies. And that is what kind of led us to prospect theory. But things went on. You might not have followed. There was a whole host of other anomalies and that in turn was what led to the modern-day new theories that are still being developed. My point here is that the process by which this new horse shows up is actually quite a structured process. It's the process of what I'm going to call anomaly generation. I'm going to describe an anomaly as a minimal example that a theory cannot fit. Anomalies are fantastically interesting things. Their smallness gives us a clue into what they are. Think of when we think of empirical work, we say, \"I had a huge data set. LA said, I only have two data points, two menus. Also, I never bothered to collect the data because it's so obvious what's going to happen.\" So many of the great experiments in science have this feature. So, if you just go back, pick your favorite experiment. Double slit experiment. The double slit experiment is literally light through this thing with two slits, light through this thing with one slit, carefully balanced to have the tiniest possible data. This has been true. Go back, read game theory. This is how game theory. Oh, here's Nash. Oh, here's a supermarket paradox. Oh, Zelton. Oh, here. These anomalies are very central again and again in the evolution of every science, including the physical sciences. How do we usually find anomalies? How did LA find the anom LA paradox? Well, let's just think of what is what at its essence the discovery of an LA paradox or a double experiment really is. We observe the world. We know the theory. We contrast it. the world with a theory. Al said, \"Those axims seem good. My intuition suggests something's wrong.\" And then there's this meticulous crafting of this object called an anomaly. What's intriguing is every aspect of this is human. But you'll notice every aspect of this could be algorithmic. After all, data to predictor, algorithms are great at that. Contrasting with the theory, well, a theory is a formal object. So why not contrast intuition with this theory crafting that seems a little fuzzier but maybe we can work our way around it. So in this work that Ashish and I did what we tried to ask the question is can we just build an algorithmic generator the inputs into an algorithmic generator are nothing more than a data set and a theory and what it needs to do is to ask propose I think if you measured at these exact two points I think you will find there's a real problem with your theory that your theory simply cannot explain so produces anomalies it turns out this activity is doable in the interest of time I won't uh get into how we did it uh partly because you know max is quite theory. Who knows what that is? And it but it turns out these things actually work quite well. If you go back to the LA paradox and you all you do is you give it data on lottery choices and you give it expected utility theory, it actually produces a large set of anomalies. It recovers all the conment toki anomalies, recovers the LA paradox and produces a whole host of other anomalies not in the literature. The reason I'm telling you about this paper, and I will just unabashedly say uh out of sheer laziness, I'm just going to focus primarily on papers I'm working on just because that was by far the easiest option, but it is not uh any implication of quality or anything, but it just lets me tell the story easier. So, as I look towards a new factory floor, one of the elements I want this story to suggest is that algorithms can suggest new theories and hypotheses. this thing that was off camera. Oh, where did these new theories come from? Can suddenly be on camera and be part of the factory floor. Great. Bring what is off screen onto on screen. I'll skip this bit because it's needlessly provocative and I have a little bit of time. So, so let me move to the next provocation. So, here I want to take the word best in the sentence best theory. That's a funny word uh partly. Well, it's like really in your face. um uh and I want to do a thought experiment. Suppose you're interested in writing a model of health insurance. How would you model it? Well, you could move asymmetric information. You could talk about behavioral confusion. You could talk about the game between providers. It's a lot of choices. Say you say asymmetric information. Well, you could talk. Is it asymmetric information about health? Is it about willingness to pay? Is it asymmetric information about discount rates? Okay. Let's say I pick health. How do I model consumers? Do I use expected utility theory or do I use hyperbolic theor? There's a ton of choices in every modeling exercise. If there's a single best theory, why do we have so much freedom to pick? Because there isn't a single best theory. Economics, like many other sciences, has not been accumulating a single monolithic theory that is somehow the right theory. We have been accumulating a collection of theories. For example, I said, \"Oh, we've seen how EU has a bunch of anomalies.\" I am confident that many of you, all of you who have written a model with expected with choice under uncertainty probably wrote an expected utility model rather than writing prospect theory. Even though you've seen all those anomalies, we don't view failure as determinative. We should shelf that model all around. We'll always do prospect theory with hyperbolic discounting. We've been collecting these theories because we are a patchwork science. Many sciences are patchwork sciences. It'll come to surprise you. For example, we've recently started some work on chemistry. And chemistry is actually a patchwork science. Quantum mechanics is just too iller at the level of aggregation that chemistry operates at. And in fact, chemistry is a collection of local theories such as protonation, EAS, which like us chemists use human intuition to make predictions of. And it's not just chemistry. Solar flare prediction, medicine, astrophysics, biology, all of these fields lack a monolithic theory. They are collections of patchworks. So the second thing I want to do is to argue for you that the notion of a best theory is misleading. And we know this like we don't have a best theory. So let's take apart the other thing. The word apply. I really went overboard with this animation. That's clear to me now. I have regrets. I have many regrets. Um, so let's go back to this thought experiment that I talked about. When you made these choices and if someone makes them in a seminar and I made a goofy choice, you know what people say? How would people tell me you made a stupid choice? What would they say? How do you make this choice? Well, we use this phrase first order. In this case, this is first order, which seems like an abuse of calculus. But I think what people mean to say is this is important. I know this to be important. Where does any of that come from? On what basis are we saying that? It's because app the testing of theories is not an isolated experiment such as a double slit experiment. In the patchwork sciences, every application of a theory is revealing to us about which theories seem to give us more leverage where. So we are not simply accumulating the best theory. We are not doing it through controlled experiments. Application is not a secondary activity. Application is not a secondass citizen that happens after we've built theory and now they've got the right theory. We're just going to go apply it. It is through application we learn a very valuable in fact the pivotal piece of information. I will just um well if you want to talk to me I will walk you through this. I would argue this is actually true of physics as well. I'm happy to talk you through this. I've written it down in some ways that electricity and magnetism and thermodynamics actually came from application rather than the other way around and Newtonian mechanics is a push but I agree it works for quantum mechanics and relativity but we'll skip that. I think I'm just being exceedingly provocative because I haven't gotten much sleep. So the final element of this that I want to push on is the word theory. It's going to happen again. Oh my god. Uh is the word theory. And to take apart the word theory, what I would like to do now is to ask what is a theory? Well, I think in economics we all go back to Samuelson at some deep level. What a theory is, it's a precise formalism where the derivations do all the work with counterintuitive implications. I mean, that's what would make fantastic theory. So then I did a little experiment. I said I went to a bunch of people and I said, \"What is your favorite theoretical idea from economics?\" I'm I'm put like what I often hear, but you might have some favorites, but people love opportunity costs. People love the prisoners dilemma. They love the market for lemons. These all have an interesting shared component. They all involve noticing an aspect of the world you didn't notice before, and they apply to many different things, but if we're being honest, they give you just a single insight. And in fact, the insight is in the name. It's actually not that hard. Opportunity cost is not hard at all. So we have this tension between what we think of as theories but also what we prize as theories. Objects are qualitative. Not much derivation. Their strength is in their breath. So I might almost say there are two categories of theories. Burrowing theories which you make a precise formalism. You derive derived derive get something. And sprawling theories where they seem to bind to many things and give you lots of insights. So final element of the factory floor is I want to argue that we have to think hard about qualitative theories. For this I'll make a side. I love this paper so I plug it everywhere. If you have not read this paper by the physicist Eugene Wignner, the unreasonable effectiveness of mathematics and natural sciences you should read it. It's fantastic. His argument is something like this. He's having a meeting a friend who it's about two friends who are meeting. One of whom is a statistician. And the friend says um so what do you do? Oh yeah. Well, I'm able to say whether people like soap in this area. I said, \"So, you talk to everybody?\" He said, \"No, no, no, no. I only talk to some of them.\" It's like, \"How can you do that?\" I said, \"Well, just like if I get, you know, just enough people.\" He's like, \"That sounds weird.\" And then he says, \"Fine, fine. What's this number here in this formula?\" This thing said, \"Pi.\" He's like, \"What? What's that?\" I said, \"You know, in grade school,\" he said, \"Now you're just messing with me. What on earth does the circumference of soap of circumference of the circle have to do with whether people like soap or not? And Eugene Wigner's point is that is the strangest thing you've probably ever encountered in your life. Why did pi show up in the central limit theorem? And he then goes on and says this happens in physics all the time. One way of thinking about the success of physics according to Wner is that mathematics has a remarkable inductive bias towards the physical laws of physics. One way of thinking about our problem with qualitative theories is mathematics is not proven to have the same inductive bias as say language or other media. And so what I want to argue is mathematics is a language of theories because mathematics is interpretable but algorithms do not face this constraint and we have the ability to think and rethink what should the language of theory be and so what should the new medium of theorizing be. So to take it all apart, putting those provocations together, I want to propose that the new factory floor is very different. It's one that recognizes accumulating understanding and using understanding are in a two-way street. They're deeply connected. What we are accumulating is a patchwork of theories like this in some format that we have to learn and understand. The other thing we are accumulating is the question which theory where which I want to argue could just be an ML model that is something we're very familiar with which is a foundation model but in this case it's not a foundation model for prediction it's a foundation model for answering the question given context what theories are most relevant in this context which is after all what we mean when we say first order but that is a very automatable activity. Second, because it's a two-way street. Notice each deployment of the foundation model improves our ability to answer this question and that is a two-way street. The second component I want to argue uh the the last component is to argue that every time we use a theory, we have the potential to expand the theory. What do I mean by that? In any application, we should just contrast the theory with the pure blackbox ML model. Why do we do that? Well, because we can answer the question, does the blackbox ML model find some predictable emitted signal? And if we can answer that question, we can then convert that predictable signal into a new theory. That's what we saw with the anomalies thing. That's the contrast of these two things. So this is what I want to argue is a new factory floor. What do we need to get there? This will be the last uh things. First, I want to argue we need people and it fundamentally needs people even in the in well maybe not the full limit of transformative AI but in my rendition and the reason I think it needs people is because fundamentally algorithms and theories live in a data frame that they can model. The interesting thing about humans is we appear to see things not in already decided data. I think this needs theories because theories are the thing that gives us formal bridges across data frames from high to low uh data environments. And what it requires is a new kind of AI. It requires AI that has better capacity to develop and communicate the world models that it's finding. I think that's what a lot of AI people are trying to produce and I think that's one of the things we'll need. So let me conclude this by saying I think I've given you a whirlwind tour along with changing font sizes of uh the new factory floor. Let me just conclude with a quote from Bertrand Russell which I find very interesting. In a nutshell he says you know if you ask a mathematician or a minologist what body of truths has been ascertained your science they'll give you some you know very precise answer. If you ask a philosopher which he was uh they'll say I don't really think I've got anything to say. And then he says why is this happening? He says it's happening. He has a very interesting theory. He says it's accounted for by the fact that as soon as something becomes definitive, actionable, we stop calling it philosophy and that philosophy is that thing of working in such a nebulous space that suddenly when it becomes tangible enough and concretized enough, it now becomes its own field. Those transition moments from philosophy to out of philosophy are very interesting to me. And I think we're about to have one such transition moment. That transition moment is I think philosophy of science which has always been in this nebulous area is suddenly going to become an engineering activity and the building of the new factory floor will be the engineering analog of the philosophy of science. All right. Thank you. Thank you so much uh Sandella. It's actually uh just an incredibly thoughtprovoking paper and uh really enjoyed. This is this is not mine. That's mine. That's mine. It's yours. Your time is up. My time is up. He set he set the timer. So the discussion is actually joint on believe it or not with with our chair and Alex Alex Otal but I take full responsibility for the slides. uh and to say uh to begin with a remarkable sort of feature of Senell and Ashishi's work uh is that they are both involved in in sort of building um AI algorithms for doing science but also able to step back and uh really think about the the the the broader implications uh and this particular paper is very much uh in that sort of uh philosophical mode and I really enjoyed uh reading it. Now he went a little bit beyond the written paper that we uh uh that we saw. Uh so this really uh focuses very much on that uh that that written version and I really see it as doing two main things. Uh one really asking the question what does this what does the arrival of this new class of intelligent agents tell us about the existing factory floor? Uh and then the second question uh which sent very much focused on in his in his presentation. uh how might or should uh the factory floor of science be reorganized given this new class of intelligent agents. Um so in the paper they go through the existing factory floor of science and very convincing description in terms of theory test and and use but then look at various things that are happening offscreen or offstage um and it's kind of useful to go through that. Uh so one thing that's really come out I think of uh Sendel's uh work uh is the the whole process of hypothesis generation that can be seen new patterns in the data or seeing anomalies or or or surprises uh that generate these new hypotheses. This is typically not part of the sort of the formal description of the factory floor that we see. Uh going back to a theme from uh earlier this morning that is tacid knowledge. uh another thing that these algorithms have have revealed is the importance of this uh tacet knowledge you know how we map from our symbols whether it's in form of words or mathematical symbols or diagrams to meanings in the world and I think it's the failure of symbolic or good old-fashioned AI partly has revealed the the importance of this titled uh another thing they emphasize in the paper is uh uh what's happening offstage is developing sort of new concepts or constructs and associated sort of measurement instruments. Uh again, typically not focused on in our more formal descriptions of the factory floor. And then there is something that that in a sense is more negative. Uh one thing algorithms have allowed us to see uh is what can be predicted sort of from the data. uh and how uh uh often our theories actually uh predict sort of very uh little uh the the or squares are low in terms of the overall variation under dependent variables. Uh but I think uh it's also revealed that we sort of cling stubbornly to the need to have these uh causal explanations. Uh so and I think this is uh kind of stepping back. This is one of the uh the the things that we can get from this AI revolution allowing us to kind of step back and think about how things even work at present. And for me one of the things one of the lessons I take away as an economist that we don't think enough about sort of the cognitive side because uh many of the things that send describes here are cognitive processes. Another celebrated view of the factory 4 which I think is very much aligned with what Sindell is talking about is Richmond Feman's view. I won't read out the quote. Uh but really the thing that's kind of happening offstage uh is the hypothesis generation or what he actually called guessing. Um so to give the these off-stage elements or off-screen elements a name that we have used in in in our own work, it's really bringing out the the importance uh of uh a term that's used a lot in philosophy and in cognitive science but not in economic and that's uh abductive inference. uh but for us I think we can really kind of think of it effectively as causal judgment and it goes back to the American uh philosopher of the late uh 19th and early 20th century Charles Sanders purse that believe it or not is actually how you pronounce his his name and I think this really uh uh is very much aligned with with what Sel is talking about. So he defines abductive inference as you see this surprising fact C if A were true C would be a matter of course hence there's reason to suppose that A is true. Uh so inance this is a third uh form of inference beyond uh deduction and induction. So we can think of deduction as deriving the logical implications of sets of assumptions and axioms. Uh induction as uh uh generalization from observations. For economists, we can think of the uh abductive step is how we choose those uh assumptions to begin with. And this is a really difficult problem uh in that it involves search over potentially vast space of combinations of of assumptions. Somehow uh human cognition is able to do this. It's actually somewhat of a mystery to uh to to cognitive science itself. Uh and I think it is what AI is struggling with. Uh maybe not everybody here will will uh will will agree with that. But if we were to pick out uh something if if we were to think that AI is going to continue to be a tool for science and in other areas, it must be something that will remain robustly in the human domain. Uh and uh I think if you were to pick out one thing that's uh uh most likely stay in the human domain, maybe not indefinitely for some time, it is this ability to do abductive inference. So then the second part is uh you know given that we now better understand uh the the factory 4 and what's going on uh offscreen how should the factory 4 of science be re be reorganized given uh this new class of intelligent agents this is a quote from the the written version of the paper but uh and again I won't go go through it all because of of time but effectively the kind of world modeling that we do as humans uh could be uh done uh by machines not alone but uh having a very substantial role and I'd like to actually relate this back to something else I think that send is famous for this distinction between uh y hat problems and beta hat problems or prediction uh problems versus causal identification problems and I think a lot of what's motivating send's dissatisfaction with the current factory floor uh is that you know our our theories uh seem to explain so little of what is apparently explainable uh as revealed by these uh prediction models. So to give a symbol to what I think he's looking for, it's something like this that would have the uh the predictive power uh of our prediction models but also allow for for for causal interpretations. Uh so I think that is what uh he would like the new factory floor to look for. But now just to uh kind of push a little bit uh it does his vision as uh really set out in the paper in particular does seem to uh involve quite a lot of outsourcing uh to the algorithms just to look at the last bit there memory is not stored in researchers in intuitions but is stored in the model uh itself. Um now there is definitely continued roles for humans uh and Selen talked about you know humans and the algorithms working together and in particular uh he sees a role for humans in developing new uh constructs and and measurements associated with those constructs which are indeed very important but you could think of the human is now kind of nudging the model along a little bit maybe that's a little unfair it's more than just nudges uh but but but I think there is quite a lot of outsourcing involved so we're going from world models in the head to world models in the system. Maybe there should be some brains in that system as well. But there still is a uh substantial transition. Is it feasible uh at the way um AI is progressing? I definitely uh wouldn't rule it out, but I would say that we're a long way still from it. And I think some of the best work on this is actually done by uh Sindell and co-authors with Keon Vafa in in particular uh where they show these models that predict very well are not discovering these these underlying uh constructs uh and I think there's a other evidence to show that as well. So we may get there but it will be challenging. But I want to just kind of really close by raising a different question and that is really uh you know how desirable would it be uh particularly uh when there is a lot of outsourcing involved. Uh now this is you could ask is there a trade-off and and and does it matter but to the extent that as we move to this uh uh new I don't know what you'd call that that that symbol for the for the new world but that that certain human understanding could be uh could be lost. uh and does that matter? Uh Sendell, I think, has been one of the most articulate um advocates for the uh idea of thinking of AI as a bicycle for the mind. Uh at the other end of the spectrum, I don't think this is what he's advocating is that it would be more of a self-driving uh bicycle. Now, what your reaction uh is, it could be either either of those uh those two, but I think what they're really looking for is something more like this. uh and hopefully the brain is still uh doing uh doing the steering. Uh but but but can we actually achieve that? So this is my last slide. Um and a Jay hasn't even kind of seen seen this one. So uh this is just pure me. Uh so I'm sure at MIT uh send will be familiar with the work of Josh Tenebomb, but he's been involved in the writing of two uh papers. uh one that goes back a few years which is uh entitled building machines that learn and think like people and even though they don't use the word abductive inference in that paper that's really what they're describing that was what would be needed uh for machines that think like people but they have a more recent paper machines that learn and think with people and set out a number of requirements uh for this this uh team so you understand me I understand you and together uh we understand the world kumbaya And the sort of the question is uh sort of uh how much outsourcing could we do uh and and really uh achieve this level of of of mutual understanding. Uh so I'd be very interested in Senel's reaction to that. Uh and maybe just a just a a final thought, do we really want to move away uh from using the metaphor of a tool uh to to a partnership? We're on are we on safer ground? Really thinking of it as a tool uh to make sure that we don't uh we don't outsource too much. [Applause]"
  }
]